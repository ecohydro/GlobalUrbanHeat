{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS Calculations\n",
    "\n",
    "Notebook to crunch numbers for the MS.\n",
    "\n",
    "by Cascade Tuholske 2020.02.23 \n",
    "\n",
    "Updated 2020.08.27 - CPT\n",
    "Was run on ERA5 RH with CHIRTS-Daily Tmax from ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Depdencies \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Regressions, no intercept addition is needed because we're using SK LEARN HERE \n",
    "\n",
    "def lm_func(df, col):\n",
    "    \n",
    "    \"simple linear model of a time series data, returns coef\"\n",
    "    \n",
    "    # Get Data\n",
    "    X_year = np.array(df.groupby('year')['ID_HDC_G0'].mean().index).reshape((-1, 1))\n",
    "    Y_stats = np.array(df.groupby('year')[col].sum()).reshape((-1, 1))\n",
    "\n",
    "    # Add Intercept\n",
    "    X_year_2 = sm.add_constant(X_year)\n",
    "\n",
    "    # Regress\n",
    "    model = sm.OLS(Y_stats, X_year_2).fit() \n",
    "        \n",
    "    coef = int(model.params[1])\n",
    "    #coef = int(coef)\n",
    "            \n",
    "    # R2 and P\n",
    "    r2 = model.rsquared_adj\n",
    "    p = model.pvalues[0]\n",
    "    \n",
    "    return coef, round(r2, 2), round(p, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load Data\n",
    "DATA = 'WBGT32_1D' # UPDATE \n",
    "\n",
    "# file paths\n",
    "DATA_IN = \"/home/cascade/projects/UrbanHeat/data/\"  \n",
    "FIG_OUT = \"/home/cascade/projects/UrbanHeat/figures/\"\n",
    "FN_IN = 'processed/PNAS-DATA-v2/'+DATA+'_EXP.json'\n",
    "HI_STATS = pd.read_json(DATA_IN+FN_IN, orient = 'split')\n",
    "\n",
    "# Set scale\n",
    "scale = 10**9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322082\n",
      "321538\n"
     ]
    }
   ],
   "source": [
    "# Drop cites where 1983 had 1 day and none elsewhere\n",
    "\n",
    "print(len(HI_STATS))\n",
    "only83 = HI_STATS.groupby('ID_HDC_G0')['tot_days'].sum() == 1 # sum up total days and find those with 1 day\n",
    "only83 = list(only83[only83 == True].index) # make a list of IDs\n",
    "sub = HI_STATS[HI_STATS['ID_HDC_G0'].isin(only83)] # subset those IDs\n",
    "bad_ids = sub[(sub['year'] == 1983) & (sub['tot_days'] == 1)] # drop those from 1983 only\n",
    "drop_list = list(bad_ids['ID_HDC_G0']) # make a list\n",
    "HI_STATS= HI_STATS[~HI_STATS['ID_HDC_G0'].isin(drop_list)] # drop those from the list\n",
    "print(len(HI_STATS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Add In Meta Data (e.g. geographic data)\n",
    "meta_fn = DATA_IN+'interim/GHS-UCDB-IDS.csv'\n",
    "meta_data = pd.read_csv(meta_fn)\n",
    "\n",
    "#### Merge in meta\n",
    "HI_STATS = HI_STATS.merge(meta_data, on = 'ID_HDC_G0', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CTR_MN_NM</th>\n",
       "      <th>UC_NM_MN</th>\n",
       "      <th>year</th>\n",
       "      <th>tot_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>215647</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Pekan Baru [IDN]</td>\n",
       "      <td>2016</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67759</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Samitah [SAU]</td>\n",
       "      <td>2002</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67770</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Samitah [SAU]</td>\n",
       "      <td>2013</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217682</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Telukkepik [IDN]</td>\n",
       "      <td>2016</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67843</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>Harad [YEM]</td>\n",
       "      <td>2002</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215629</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Pekan Baru [IDN]</td>\n",
       "      <td>1998</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215749</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Dumai [IDN]</td>\n",
       "      <td>2016</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67764</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Samitah [SAU]</td>\n",
       "      <td>2007</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216623</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Pinangsebatang [IDN]</td>\n",
       "      <td>2016</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>Peru</td>\n",
       "      <td>Pucallpa [PER]</td>\n",
       "      <td>1998</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67385</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Sabya [SAU]</td>\n",
       "      <td>2002</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67760</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Samitah [SAU]</td>\n",
       "      <td>2003</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68100</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>Dayr al Muqazilah [YEM]</td>\n",
       "      <td>1998</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67399</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Sabya [SAU]</td>\n",
       "      <td>2016</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65918</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Al Qunfudhah [SAU]</td>\n",
       "      <td>2016</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67773</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Samitah [SAU]</td>\n",
       "      <td>2016</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67854</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>Harad [YEM]</td>\n",
       "      <td>2013</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68109</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>Dayr al Muqazilah [YEM]</td>\n",
       "      <td>2007</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67855</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>Harad [YEM]</td>\n",
       "      <td>2014</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68066</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>Al Misqa [YEM]</td>\n",
       "      <td>1998</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145441</td>\n",
       "      <td>India</td>\n",
       "      <td>Devakottai [IND]</td>\n",
       "      <td>2016</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67397</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Sabya [SAU]</td>\n",
       "      <td>2014</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67758</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Samitah [SAU]</td>\n",
       "      <td>2001</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145406</td>\n",
       "      <td>India</td>\n",
       "      <td>Kottaiyur [IND]</td>\n",
       "      <td>2015</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67755</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Samitah [SAU]</td>\n",
       "      <td>1998</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67771</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Samitah [SAU]</td>\n",
       "      <td>2014</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67844</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>Harad [YEM]</td>\n",
       "      <td>2003</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68168</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>Dayr Abkar [YEM]</td>\n",
       "      <td>1998</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67396</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Sabya [SAU]</td>\n",
       "      <td>2013</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145440</td>\n",
       "      <td>India</td>\n",
       "      <td>Devakottai [IND]</td>\n",
       "      <td>2015</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214865</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Duri [IDN]</td>\n",
       "      <td>2016</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67842</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>Harad [YEM]</td>\n",
       "      <td>2001</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67768</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Samitah [SAU]</td>\n",
       "      <td>2011</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67761</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Samitah [SAU]</td>\n",
       "      <td>2004</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146596</td>\n",
       "      <td>India</td>\n",
       "      <td>Arantangi [IND]</td>\n",
       "      <td>2015</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67757</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Samitah [SAU]</td>\n",
       "      <td>2000</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67836</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>Harad [YEM]</td>\n",
       "      <td>1995</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67848</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>Harad [YEM]</td>\n",
       "      <td>2007</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67762</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Samitah [SAU]</td>\n",
       "      <td>2005</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67566</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Abu 'Aris [SAU]</td>\n",
       "      <td>2002</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65901</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Al Qunfudhah [SAU]</td>\n",
       "      <td>1999</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67841</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>Harad [YEM]</td>\n",
       "      <td>2000</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67852</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>Harad [YEM]</td>\n",
       "      <td>2011</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68112</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>Dayr al Muqazilah [YEM]</td>\n",
       "      <td>2010</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67839</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>Harad [YEM]</td>\n",
       "      <td>1998</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67752</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Samitah [SAU]</td>\n",
       "      <td>1995</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Tahiriyah [SAU]</td>\n",
       "      <td>2002</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67387</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Sabya [SAU]</td>\n",
       "      <td>2004</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67857</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>Harad [YEM]</td>\n",
       "      <td>2016</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146590</td>\n",
       "      <td>India</td>\n",
       "      <td>Arantangi [IND]</td>\n",
       "      <td>2009</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CTR_MN_NM                 UC_NM_MN  year  tot_days\n",
       "215647     Indonesia         Pekan Baru [IDN]  2016       228\n",
       "67759   Saudi Arabia            Samitah [SAU]  2002       223\n",
       "67770   Saudi Arabia            Samitah [SAU]  2013       222\n",
       "217682     Indonesia         Telukkepik [IDN]  2016       221\n",
       "67843          Yemen              Harad [YEM]  2002       220\n",
       "215629     Indonesia         Pekan Baru [IDN]  1998       220\n",
       "215749     Indonesia              Dumai [IDN]  2016       219\n",
       "67764   Saudi Arabia            Samitah [SAU]  2007       218\n",
       "216623     Indonesia     Pinangsebatang [IDN]  2016       216\n",
       "7550            Peru           Pucallpa [PER]  1998       216\n",
       "67385   Saudi Arabia              Sabya [SAU]  2002       215\n",
       "67760   Saudi Arabia            Samitah [SAU]  2003       214\n",
       "68100          Yemen  Dayr al Muqazilah [YEM]  1998       214\n",
       "67399   Saudi Arabia              Sabya [SAU]  2016       214\n",
       "65918   Saudi Arabia       Al Qunfudhah [SAU]  2016       213\n",
       "67773   Saudi Arabia            Samitah [SAU]  2016       212\n",
       "67854          Yemen              Harad [YEM]  2013       212\n",
       "68109          Yemen  Dayr al Muqazilah [YEM]  2007       211\n",
       "67855          Yemen              Harad [YEM]  2014       211\n",
       "68066          Yemen           Al Misqa [YEM]  1998       211\n",
       "145441         India         Devakottai [IND]  2016       211\n",
       "67397   Saudi Arabia              Sabya [SAU]  2014       211\n",
       "67758   Saudi Arabia            Samitah [SAU]  2001       211\n",
       "145406         India          Kottaiyur [IND]  2015       211\n",
       "67755   Saudi Arabia            Samitah [SAU]  1998       210\n",
       "67771   Saudi Arabia            Samitah [SAU]  2014       210\n",
       "67844          Yemen              Harad [YEM]  2003       210\n",
       "68168          Yemen         Dayr Abkar [YEM]  1998       210\n",
       "67396   Saudi Arabia              Sabya [SAU]  2013       210\n",
       "145440         India         Devakottai [IND]  2015       209\n",
       "214865     Indonesia               Duri [IDN]  2016       209\n",
       "67842          Yemen              Harad [YEM]  2001       209\n",
       "67768   Saudi Arabia            Samitah [SAU]  2011       209\n",
       "67761   Saudi Arabia            Samitah [SAU]  2004       209\n",
       "146596         India          Arantangi [IND]  2015       208\n",
       "67757   Saudi Arabia            Samitah [SAU]  2000       208\n",
       "67836          Yemen              Harad [YEM]  1995       208\n",
       "67848          Yemen              Harad [YEM]  2007       208\n",
       "67762   Saudi Arabia            Samitah [SAU]  2005       208\n",
       "67566   Saudi Arabia          Abu 'Aris [SAU]  2002       207\n",
       "65901   Saudi Arabia       Al Qunfudhah [SAU]  1999       207\n",
       "67841          Yemen              Harad [YEM]  2000       207\n",
       "67852          Yemen              Harad [YEM]  2011       207\n",
       "68112          Yemen  Dayr al Muqazilah [YEM]  2010       207\n",
       "67839          Yemen              Harad [YEM]  1998       207\n",
       "67752   Saudi Arabia            Samitah [SAU]  1995       207\n",
       "67500   Saudi Arabia          Tahiriyah [SAU]  2002       206\n",
       "67387   Saudi Arabia              Sabya [SAU]  2004       206\n",
       "67857          Yemen              Harad [YEM]  2016       206\n",
       "146590         India          Arantangi [IND]  2009       206"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Check\n",
    "check = HI_STATS.sort_values('tot_days', ascending = False)\n",
    "check[['CTR_MN_NM', 'UC_NM_MN', 'year', 'tot_days']].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Total Change in people Days\n",
    "data = HI_STATS.groupby('year')['people_days'].sum()\n",
    "year = str(data.index[33])\n",
    "value = str(data.values[33]/10**9)\n",
    "print('person days in 2016 was '+value+' billion')\n",
    "\n",
    "year = str(data.index[0])\n",
    "value = str(data.values[0]/10**9)\n",
    "print('person days in 1983 was '+value+' billion')\n",
    "\n",
    "#### Pct Change in Poeple Days 1983 - 2016\n",
    "pdays16 = data.iloc[len(data) -1]\n",
    "pdays83 = data.iloc[0]\n",
    "out = (data.iloc[len(data) -1] - data.iloc[0]) / data.iloc[0] * 100\n",
    "print('pct increase in people days 83 - 16 is ', out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Rate of change\n",
    "data = HI_STATS\n",
    "coef, r2, p = lm_func(data, 'people_days')\n",
    "print('annual increase in people days ', 'was', coef/10**9, ' p=', p)\n",
    "coef1, r21, p1 = lm_func(data, 'people_days_heat')\n",
    "print('annual increase in people days heat ', 'was', coef1/10**9, ' p=', p)\n",
    "coef2, r22, p2 = lm_func(data, 'people_days_pop')\n",
    "print('annual increase in people days pop ', 'was', coef2/10**9, ' p=', p)\n",
    "print('attrib heat ', 'was', coef1 / coef *100, ' p=', p, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Pct Pday Annual Increase from Heat\n",
    "coef_pdays, r2_pdays, p_pdays = lm_func(HI_STATS, 'people_days') # regress pdays\n",
    "coef_heat, r2_heat, p_heat = lm_func(HI_STATS, 'people_days_heat') # regreas heat\n",
    "\n",
    "print('warming is what pct of total?', coef_heat/coef_pdays *100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Largest cities compared to global total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Top cities\n",
    "cities = pd.read_csv(DATA_PATH+'processed/PNAS-DATA-v2/WBGT32_1D_EXP-TOP50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = cities.sort_values('coef_pdays', ascending = False).head(25) # get the top ten cities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What pct of the global annual increase comes from the top ten cities?\n",
    "ans = top['coef_pdays'].sum() / coef\n",
    "print('Top 25 cities of total annual increase', ans * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_coefs = pd.read_json(DATA_IN+'processed/PNAS-DATA-v2/WBGT32_1D_TREND_EXP05.json', orient = 'split')\n",
    "GHS = gpd.read_file('/home/cascade/projects/UrbanHeat/data/raw/GHS_UCDB/GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_0.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(city_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Number of cities w/ sig increase in exposure?\n",
    "print('The pct of cities w/ increases in exposure: ', len(city_coefs)/len(GHS)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(city_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ans = len(city_coefs[(city_coefs['GCPNT_LAT'] < 23.5) & (city_coefs['GCPNT_LAT'] > -23.5)]) / len(city_coefs)\n",
    "print('what pct of pday cities are low lat?', ans*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('what pct of global pop are cities with sig pdays?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_search(country, data_set):\n",
    "    \"what pct of cities had a p-day increase?\"\n",
    "    print('Num of Cities in '+country+' ', len(data_set[data_set['CTR_MN_NM'] == country]) / len(GHS[GHS['CTR_MN_NM'] == country]) *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = city_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_search('Senegal', data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_search('Nigeria', data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_search('India', data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pct of global population exposured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_coefs = pd.read_json(DATA_IN+'processed/PNAS-DATA-v2/WBGT32_1D_TREND_EXP05.json', orient = 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pd.read_csv(DATA_IN+'interim/GHS-UCDB-Interp.csv')\n",
    "p16 = pop[['ID_HDC_G0', 'P2016']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdays_pop = pd.merge(city_coefs[['ID_HDC_G0']], p16, on = 'ID_HDC_G0', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pdays_pop['P2016'].sum() / p16['P2016'].sum() * 100 \n",
    "print('What is the global urban population in 2016', p16['P2016'].sum())\n",
    "print('How many people live in cities with increasing exp in 2016', pdays_pop['P2016'].sum())\n",
    "print('What pct of total urban pop has sig increase exp in 2015', ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From UN-DESA 2018 estimates for total global pop in 2015\n",
    "ans =  pdays_pop['P2016'].sum() / 7383009000 * 100\n",
    "print('What pct of total world pop has sig increase exp in 2016', ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UN-DESA Urban pop in 2015 was  3 981 498\n",
    "p16['P2016'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Heat Days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_totdays = pd.read_json(DATA_IN+'processed/PNAS-DATA-v2/WBGT32_1D_TREND_HEATP05.json', orient = 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('What pct of all cities had sig increase in days/yr > WBGT32 C ?')\n",
    "print(len(city_totdays)/len(GHS))\n",
    "print(len(city_totdays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('What pct of all cities >1 day / yr in days/yr > WBGT32 C ?')\n",
    "print(len(city_totdays[city_totdays['coef_totDays'] >= 1])/len(GHS))\n",
    "print(len(city_totdays[city_totdays['coef_totDays'] >= 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many cities day increase per year ... 1, 3\n",
    "top = len(city_totdays)\n",
    "bottom = len(city_totdays[city_totdays['coef_totDays'] >= 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(top)\n",
    "print(bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What are some big cities?\n",
    "hot1m = city_totdays[(city_totdays['coef_totDays'] >= 1.5) & (city_totdays['P2016'] >= 10**6)][['coef_totDays', 'UC_NM_MN', 'P2016']].sort_values('coef_totDays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hot1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Columbo & San Salvador & Conakry\n",
    "print('Conakry tot days:', city_totdays[city_totdays['ID_HDC_G0'] == 1502]['coef_totDays'].values)\n",
    "print('columbo tot days:', city_totdays[city_totdays['ID_HDC_G0'] == 8835]['coef_totDays'].values)\n",
    "print('San Sal tot days:', city_totdays[city_totdays['ID_HDC_G0'] == 321]['coef_totDays'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dehli & Kolkata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delhi 6955 & Kolkata 9691\n",
    "K = city_coefs[city_coefs['ID_HDC_G0']== 9691]\n",
    "D = city_coefs[city_coefs['ID_HDC_G0']== 6955]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Share of heat Kolkata', K.coef_heat / K.coef_pdays * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Share of heat Delhi', D.coef_heat / D.coef_pdays * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Populations of specific cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pd.read_csv(DATA_IN+'interim/GHS-UCDB-Interp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9691, Kolkata 1998\n",
    "# 2046, Paris 2003\n",
    "# 4417, Aleppo 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pop[pop['ID_HDC_G0'] == 9691]['P2015'] / 10**3\n",
    "print('Pop of Kolkata in 1998', ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WBGT 32 vs 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbgt32 = pd.read_json(DATA_IN+'processed/PNAS-DATA-v2/WBGT32_1D_TREND_EXP05.json', orient = 'split')\n",
    "wbgt28 = pd.read_json(DATA_IN+'processed/PNAS-DATA-v2/WBGT28_1D_TREND_EXP05.json', orient = 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('how many cities wbgt 32?', len(wbgt32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('how many cities wbgt 28?', len(wbgt28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('no trend', len(meta_data) - 8510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dif is', 8510 - 6022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regional Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Annual Rates\n",
    "\n",
    "scale = 10**6\n",
    "geog = 'sub-region'\n",
    "\n",
    "for label in np.unique(HI_STATS[geog]):\n",
    "    label = label\n",
    "    data = HI_STATS[HI_STATS[geog] == label]\n",
    "    \n",
    "    #### Rate of change\n",
    "    coef, r2, p = lm_func(data, 'people_days')\n",
    "    print('annual increase in people days '+label, 'was', coef/scale, ' p=', p)\n",
    "    coef1, r21, p1 = lm_func(data, 'people_days_heat')\n",
    "    print('annual increase in people days heat '+label, 'was', coef1/scale, ' p=', p)\n",
    "    coef2, r22, p2 = lm_func(data, 'people_days_pop')\n",
    "    print('annual increase in people days pop '+label, 'was', coef2/scale, ' p=', p)\n",
    "    print('attrib heat '+label, 'was', coef1 / coef *100, ' p=', p, '\\n')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Trends for Africa, N & SS\n",
    "geog = 'region'\n",
    "location = 'Africa'\n",
    "data = HI_STATS[HI_STATS[geog] == location]\n",
    "print(location)\n",
    "\n",
    "#### Total Change in people Days\n",
    "data = data.groupby('year')['people_days'].sum()\n",
    "year = str(data.index[33])\n",
    "value = str(data.values[33]/10**9)\n",
    "print('person days in 2016 was '+value+' billion')\n",
    "\n",
    "year = str(data.index[0])\n",
    "value = str(data.values[0]/10**9)\n",
    "print('person days in 1983 was '+value+' billion')\n",
    "\n",
    "#### Pct Change in Poeple Days 1983 - 2016\n",
    "pdays16 = data.iloc[len(data) -1]\n",
    "pdays83 = data.iloc[0]\n",
    "out = (data.iloc[len(data) -1] - data.iloc[0]) / data.iloc[0] * 100\n",
    "print('pct increase in people days 83 - 16 is ', out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### S Asia as pct of total  global = 5.245146271 B \n",
    "\n",
    "print('pct of total pdays from S Asia is ', 1899.70765 / 10**3 / 5.245146271 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Median Slope\n",
    "region = 'Europe'\n",
    "col = 'coef_heat'\n",
    "geog = 'region'\n",
    "scale = 10**3\n",
    "result = city_coefs[city_coefs[geog]== region][col].median()\n",
    "print(region, col, 'is ', result/scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat Waves\n",
    "\n",
    "- 9691 Kolkata 1998\n",
    "- 2046 Paris 2003\n",
    "- 4417, Aleppo 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(dir_in, geog, location):\n",
    "    \"\"\"Function makes data to plot daily city-level HI Max and average\n",
    "    Args:\n",
    "        dir_in = directory to get data\n",
    "        geog = column for geography, city-level = 'ID_HDC_G0'\n",
    "        location = usually a city id\n",
    "    \"\"\"\n",
    "    \n",
    "    fn_list = sorted(glob.glob(dir_in+'*.csv')) # get data\n",
    "    df_out = pd.DataFrame() # to write dataframe\n",
    "    \n",
    "     # get leap year cols from 2016\n",
    "    hi16 = pd.read_csv(fn_list[33]) \n",
    "    cols = list(hi16.iloc[:,3:].columns)\n",
    "    cols = [year[5:] for year in cols] # cols for data frame\n",
    "    \n",
    "    temp_list = [] # empty list for temps\n",
    "    \n",
    "    # loop through dir and get data\n",
    "    for i, fn in enumerate(fn_list):\n",
    "        df = pd.read_csv(fn) # open data frame\n",
    "        year_label = [(df.columns[3]).split('.')[0]] # get year\n",
    "        row = df[df[geog] == location]\n",
    "        temp = row.iloc[:,3:] # get only temp columns\n",
    "        \n",
    "        # add in col for leap years\n",
    "        if temp.shape[1] == 365:\n",
    "            temp.insert(loc = 59, column = year_label[0]+'.02.29', value = np.nan, allow_duplicates=False)\n",
    "\n",
    "        # Set Index & Columns\n",
    "        temp.index = year_label\n",
    "        temp.columns = cols # revalue to m.d\n",
    "    \n",
    "        # add to list\n",
    "        temp_list.append(temp)\n",
    "    \n",
    "    df_out = pd.concat(temp_list) # make one big dataframe\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df, year, start, end):#, start, end):\n",
    "    \"\"\" Make the data for a plot\n",
    "    Args: \n",
    "        df = df w/ daily HI max for a given city\n",
    "        year = year you want to plot against average\n",
    "        start = start of plot in julian days (e.g 1 - 365/366)\n",
    "        end = end of plot in julian days\n",
    "    \"\"\"\n",
    "\n",
    "    # Deal with leap year\n",
    "    if year % 4 !=0:\n",
    "        df.drop(columns ='02.29', inplace = True)\n",
    "    \n",
    "    # Subset data\n",
    "    start = start - 1 # zero indexing \n",
    "    subset = df.iloc[:,start:end]\n",
    "    \n",
    "    # HI Max for year\n",
    "    hi_year = subset.loc[str(year)]\n",
    "    \n",
    "    # make 34-avg daily hi\n",
    "    means = subset.mean(axis = 0)\n",
    "    \n",
    "    # make colums to date time\n",
    "    cols = pd.to_datetime([str(year)+'.'+date for date in hi_year.index])\n",
    "    \n",
    "    return hi_year, means, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Heat Wave From All DATA\n",
    "def select_city_year(df, city_id, year):\n",
    "    \"Quick search to find city and years within HI_STATS\"\n",
    "    df_out = df[(df['ID_HDC_G0'] == city_id) & (df['year'] == year)]\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "# open heat events data\n",
    "events_fn = '/home/cascade/projects/UrbanHeat/data/processed/PNAS-DATA-v2/HI461_1D_STATS.json'\n",
    "events = pd.read_json(events_fn, orient = 'split')\n",
    "\n",
    "# [4417, 'Aleppo'] [2046, 'Paris'] [9691, 'Kolkata'] \n",
    "city = select_city_year(events, 4417, 2010)\n",
    "city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select event and compare to long term means\n",
    "# Kolkata 1998 UID-2920905, Aleppo 2010 \n",
    "event_id = 'UID-2920905'\n",
    "event = city[city['UID'] == event_id]\n",
    "tmax = list(event['tmax'])[0]\n",
    "dates = list(event['event_dates'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Heat Index Data\n",
    "data = 'HI'\n",
    "DATA_IN = '/home/cascade/projects/UrbanHeat/data/interim/CHIRTS_DAILY/'+data+'/' # output from avg temp\n",
    "t = 46.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get long term means\n",
    "# Args\n",
    "#[4417, 'Aleppo'] 2010 [2046, 'Paris'] 2003 [9691, 'Kolkata'] 1998 ['6955, Dehli']\n",
    "\n",
    "# Args\n",
    "city_list = [4417, 'Aleppo'] \n",
    "year = 2010\n",
    "\n",
    "# April 1 to Sep 30 (Use Julian Days), or 1 - 182 lagos\n",
    "start = 91 \n",
    "end = 273\n",
    "\n",
    "# Make Data \n",
    "df = make_data(DATA_IN, 'ID_HDC_G0', city_list[0])\n",
    "years, means, cols = plot_data(df, year, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df\n",
    "df = pd.DataFrame()\n",
    "df['dates'] = cols\n",
    "df['max']  = years.to_list()\n",
    "df['avg'] = means.to_list()\n",
    "df['dif'] = df['max'] - df['avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['dates'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dates[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end of heat wave\n",
    "start = dates[0]\n",
    "end = dates[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = df[(df['dates'] >= start) & (df['dates'] <= end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out[out['dif'] == out['dif'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phoenix Caclulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def c_to_f(C):\n",
    "    \"Convert HI C to F\"\n",
    "    return 1.8 * C + 32 \n",
    "\n",
    "def hi_to_wbgt(HI):\n",
    "    \"\"\" Convert HI to WBGT using emprical relationship from Bernard and Iheanacho 2015\n",
    "    WBGT [◦C] = −0.0034 HI2 + 0.96 HI−34; for HI [◦F]\n",
    "    \"\"\"\n",
    "    \n",
    "    WBGT = -0.0034*HI**2 + 0.96*HI - 34\n",
    "    \n",
    "    return WBGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = c_to_f(49)\n",
    "wbgt = hi_to_wbgt(hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check out cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_city_year(df, city_id, year):\n",
    "    \"Quick search to find city and years within HI_STATS\"\n",
    "    df_out = df[(df['ID_HDC_G0'] == city_id) & (df['year'] == year)]\n",
    "    \n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = select_city_year(ALL_DATA, 4417, 2010)\n",
    "city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = '/home/cascade/projects/UrbanHeat/data/interim/ERA5_HI/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'GHS-ERA5-HI_2009.csv'\n",
    "data = pd.read_csv(dir_list+fn)\n",
    "city = data[data['ID_HDC_G0'] == 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps_list = []\n",
    "dates_list = []\n",
    "for fn in sorted(os.listdir(dir_list)):\n",
    "    data = pd.read_csv(dir_list+fn)\n",
    "    city = data[data['ID_HDC_G0'] == 14]\n",
    "    dates = list(city.iloc[:,3:])\n",
    "    temps = list(city.iloc[:,3:].values[0])\n",
    "    dates_list.extend(dates)\n",
    "    temps_list.extend(temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(dates_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Africa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dependencies\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(dir_in, geog, location):\n",
    "    \"\"\"Function makes data to plot daily city-level HI Max and average\n",
    "    Args:\n",
    "        dir_in = directory to get data\n",
    "        geog = column for geography, city-level = 'ID_HDC_G0'\n",
    "        location = usually a city id\n",
    "    \"\"\"\n",
    "    \n",
    "    fn_list = sorted(glob.glob(dir_in+'*.csv')) # get data\n",
    "    df_out = pd.DataFrame() # to write dataframe\n",
    "    \n",
    "     # get leap year cols from 2016\n",
    "    hi16 = pd.read_csv(fn_list[33]) \n",
    "    cols = list(hi16.iloc[:,3:].columns)\n",
    "    cols = [year[5:] for year in cols] # cols for data frame\n",
    "    \n",
    "    temp_list = [] # empty list for temps\n",
    "    \n",
    "    # loop through dir and get data\n",
    "    for i, fn in enumerate(fn_list):\n",
    "        df = pd.read_csv(fn) # open data frame\n",
    "        year_label = [(df.columns[3]).split('.')[0]] # get year\n",
    "        row = df[df[geog] == location]\n",
    "        temp = row.iloc[:,3:] # get only temp columns\n",
    "        \n",
    "        # add in col for leap years\n",
    "        if temp.shape[1] == 365:\n",
    "            temp.insert(loc = 59, column = year_label[0]+'.02.29', value = np.nan, allow_duplicates=False)\n",
    "\n",
    "        # Set Index & Columns\n",
    "        temp.index = year_label\n",
    "        temp.columns = cols # revalue to m.d\n",
    "    \n",
    "        # add to list\n",
    "        temp_list.append(temp)\n",
    "    \n",
    "    df_out = pd.concat(temp_list) # make one big dataframe\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df, year, start, end):#, start, end):\n",
    "    \"\"\" Make the data for a plot\n",
    "    Args: \n",
    "        df = df w/ daily HI max for a given city\n",
    "        year = year you want to plot against average\n",
    "        start = start of plot in julian days (e.g 1 - 365/366)\n",
    "        end = end of plot in julian days\n",
    "    \"\"\"\n",
    "\n",
    "    # Deal with leap year\n",
    "    if year % 4 !=0:\n",
    "        df.drop(columns ='02.29', inplace = True)\n",
    "    \n",
    "    # Subset data\n",
    "    start = start - 1 # zero indexing \n",
    "    subset = df.iloc[:,start:end]\n",
    "    \n",
    "    # HI Max for year\n",
    "    hi_year = subset.loc[str(year)]\n",
    "    \n",
    "    # make 34-avg daily hi\n",
    "    means = subset.mean(axis = 0)\n",
    "    \n",
    "    # make colums to date time\n",
    "    cols = pd.to_datetime([str(year)+'.'+date for date in hi_year.index])\n",
    "    \n",
    "    return hi_year, means, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args\n",
    "#[3342, 'Cairo'] #[1910, 'Accra'] 3268, Cape Town\n",
    "#[4417, 'Aleppo'] 2010 [2046, 'Paris'] 2003 [9691, 'Kolkata'] 1998 ['6955, Dehli']\n",
    "\n",
    "# Args\n",
    "city_list = [9691, 'Kolkata'] \n",
    "year = 1998\n",
    "font_size = 10\n",
    "\n",
    "\n",
    "# April 1 to Sep 30 (Use Julian Days), or 1 - 182 lagos\n",
    "start = 91 \n",
    "end = 273\n",
    "\n",
    "# Labels\n",
    "hi_label = str(year)+' '+data\n",
    "labels =  ['avg. '+data, hi_label, str(t)+''+DS+'C']  # <<<<<------------ Be sure to update! \n",
    "\n",
    "# Make Data \n",
    "df = make_data(DATA_IN, 'ID_HDC_G0', city_list[0])\n",
    "years, means, cols = plot_data(df, year, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend First vs. Second Half of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Share of exposure due to heat by 17 year split\n",
    "\n",
    "## 1983 - 1999\n",
    "data1 = HI_STATS[(HI_STATS['year'] >= 1983) & (HI_STATS['year'] < 2000)]\n",
    "coef1pop , r21pop, p1pop  = lm_func(data1 , 'people_days_pop')\n",
    "coef1heat , r21heat, p1heat = lm_func(data1 , 'people_days_heat')\n",
    "\n",
    "years = list(np.unique(data1['year']))\n",
    "plt.plot(years, data1.groupby('year')['people_days_heat'].sum())\n",
    "sns.regplot(years, data1.groupby('year')['people_days_heat'].sum(), \n",
    "            color = 'blue', scatter = False, truncate = True)\n",
    "\n",
    "## 2000 - 2016\n",
    "data2 = HI_STATS[(HI_STATS['year'] >= 2000) & (HI_STATS['year'] <= 2016)]\n",
    "coef2heat , r22heat, p2heat = lm_func(data2 , 'people_days_heat')\n",
    "coef2pop , r22pop, p1pop  = lm_func(data2 , 'people_days_pop')\n",
    "\n",
    "years = list(np.unique(data1['year']))\n",
    "plt.plot(years, data2.groupby('year')['people_days_heat'].sum())\n",
    "sns.regplot(years, data2.groupby('year')['people_days_heat'].sum(), \n",
    "            color = 'orange', scatter = False, truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2000 - 2016\n",
    "data2pop = HI_STATS[(HI_STATS['year'] >= 1983) & (HI_STATS['year'] < 2000)]\n",
    "coef2pop , r22pop, p1pop  = lm_func(data2pop , 'people_days_pop')\n",
    "\n",
    "data2heat = HI_STATS[(HI_STATS['year'] >= 2000) & (HI_STATS['year'] <= 2016)]\n",
    "coef2heat , r22heat, p2heat = lm_func(data2heat , 'people_days_heat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Estimates\n",
    "print('From 83 - 99, contribution from heat was', coef1heat/(coef1pop+coef1heat))\n",
    "print('From 00 - 16, contribution from heat was', coef2heat/(coef2pop+coef2heat))\n",
    "print('From 83 - 00, heat was', coef1heat/10**9, round(p1heat, 3))\n",
    "print('From 00 - 16, heat was', coef2heat/10**9)\n",
    "print('From 83 - 00, pop was', coef1pop/10**9)\n",
    "print('From 00 - 16, pop was', coef2pop/10**9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
