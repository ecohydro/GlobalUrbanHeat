{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS Calculations\n",
    "\n",
    "Notebook to crunch numbers for the MS.\n",
    "\n",
    "by Cascade Tuholske 2020.02.23 \n",
    "\n",
    "Updated 2020.08.27 - CPT\n",
    "Was run on ERA5 RH with CHIRTS-Daily Tmax from ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Depdencies \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Regressions, no intercept addition is needed because we're using SK LEARN HERE \n",
    "\n",
    "def lm_func(df, col):\n",
    "    \n",
    "    \"simple linear model of a time series data, returns coef\"\n",
    "    \n",
    "    # Get Data\n",
    "    X_year = np.array(df.groupby('year')['ID_HDC_G0'].mean().index).reshape((-1, 1))\n",
    "    Y_stats = np.array(df.groupby('year')[col].sum()).reshape((-1, 1))\n",
    "\n",
    "    # Add Intercept\n",
    "    X_year_2 = sm.add_constant(X_year)\n",
    "\n",
    "    # Regress\n",
    "    model = sm.OLS(Y_stats, X_year_2).fit() \n",
    "        \n",
    "    coef = int(model.params[1])\n",
    "    #coef = int(coef)\n",
    "            \n",
    "    # R2 and P\n",
    "    r2 = model.rsquared_adj\n",
    "    p = model.pvalues[0]\n",
    "    \n",
    "    return coef, round(r2, 2), round(p, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load Data\n",
    "DATA = 'WBGT32_1D' # UPDATE \n",
    "\n",
    "# file paths\n",
    "DATA_IN = \"/home/cascade/projects/UrbanHeat/data/\"  \n",
    "FIG_OUT = \"/home/cascade/projects/UrbanHeat/figures/\"\n",
    "FN_IN = 'processed/PNAS-DATA-v2/'+DATA+'_EXP.json'\n",
    "HI_STATS = pd.read_json(DATA_IN+FN_IN, orient = 'split')\n",
    "\n",
    "# Set scale\n",
    "scale = 10**9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drop cites where 1983 had 1 day and none elsewhere\n",
    "\n",
    "print(len(HI_STATS))\n",
    "only83 = HI_STATS.groupby('ID_HDC_G0')['tot_days'].sum() == 1 # sum up total days and find those with 1 day\n",
    "only83 = list(only83[only83 == True].index) # make a list of IDs\n",
    "sub = HI_STATS[HI_STATS['ID_HDC_G0'].isin(only83)] # subset those IDs\n",
    "bad_ids = sub[(sub['year'] == 1983) & (sub['tot_days'] == 1)] # drop those from 1983 only\n",
    "drop_list = list(bad_ids['ID_HDC_G0']) # make a list\n",
    "HI_STATS= HI_STATS[~HI_STATS['ID_HDC_G0'].isin(drop_list)] # drop those from the list\n",
    "print(len(HI_STATS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Add In Meta Data (e.g. geographic data)\n",
    "meta_fn = DATA_IN+'interim/GHS-UCDB-IDS.csv'\n",
    "meta_data = pd.read_csv(meta_fn)\n",
    "\n",
    "#### Merge in meta\n",
    "HI_STATS = HI_STATS.merge(meta_data, on = 'ID_HDC_G0', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Total Change in people Days\n",
    "data = HI_STATS.groupby('year')['people_days'].sum()\n",
    "year = str(data.index[33])\n",
    "value = str(data.values[33]/10**9)\n",
    "print('person days in 2016 was '+value+' billion')\n",
    "\n",
    "year = str(data.index[0])\n",
    "value = str(data.values[0]/10**9)\n",
    "print('person days in 1983 was '+value+' billion')\n",
    "\n",
    "#### Pct Change in Poeple Days 1983 - 2016\n",
    "pdays16 = data.iloc[len(data) -1]\n",
    "pdays83 = data.iloc[0]\n",
    "out = (data.iloc[len(data) -1] - data.iloc[0]) / data.iloc[0] * 100\n",
    "print('pct increase in people days 83 - 16 is ', out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Rate of change\n",
    "data = HI_STATS\n",
    "coef, r2, p = lm_func(data, 'people_days')\n",
    "print('annual increase in people days ', 'was', coef/10**9, ' p=', p)\n",
    "coef1, r21, p1 = lm_func(data, 'people_days_heat')\n",
    "print('annual increase in people days heat ', 'was', coef1/10**9, ' p=', p)\n",
    "coef2, r22, p2 = lm_func(data, 'people_days_pop')\n",
    "print('annual increase in people days pop ', 'was', coef2/10**9, ' p=', p)\n",
    "print('attrib heat ', 'was', coef1 / coef *100, ' p=', p, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Pct Pday Annual Increase from Heat\n",
    "coef_pdays, r2_pdays, p_pdays = lm_func(HI_STATS, 'people_days') # regress pdays\n",
    "coef_heat, r2_heat, p_heat = lm_func(HI_STATS, 'people_days_heat') # regreas heat\n",
    "\n",
    "print('warming is what pct of total?', coef_heat/coef_pdays *100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Largest cities compared to global total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Top cities\n",
    "cities = pd.read_csv(DATA_PATH+'processed/PNAS-DATA-v2/WBGT32_1D_EXP-TOP50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = cities.sort_values('coef_pdays', ascending = False).head(25) # get the top ten cities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What pct of the global annual increase comes from the top ten cities?\n",
    "ans = top['coef_pdays'].sum() / coef\n",
    "print('Top 25 cities of total annual increase', ans * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_coefs = pd.read_json(DATA_IN+'processed/PNAS-DATA-v2/WBGT32_1D_TREND_EXP05.json', orient = 'split')\n",
    "GHS = gpd.read_file('/home/cascade/projects/UrbanHeat/data/raw/GHS_UCDB/GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_0.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(city_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Number of cities w/ sig increase in exposure?\n",
    "print('The pct of cities w/ increases in exposure: ', len(city_coefs)/len(GHS)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(city_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ans = len(city_coefs[(city_coefs['GCPNT_LAT'] < 23.5) & (city_coefs['GCPNT_LAT'] > -23.5)]) / len(city_coefs)\n",
    "print('what pct of pday cities are low lat?', ans*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('what pct of global pop are cities with sig pdays?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_search(country, data_set):\n",
    "    \"what pct of cities had a p-day increase?\"\n",
    "    print('Num of Cities in '+country+' ', len(data_set[data_set['CTR_MN_NM'] == country]) / len(GHS[GHS['CTR_MN_NM'] == country]) *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = city_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_search('Senegal', data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_search('Nigeria', data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_search('India', data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pct of global population exposured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_coefs = pd.read_json(DATA_IN+'processed/PNAS-DATA-v2/WBGT32_1D_TREND_EXP05.json', orient = 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pd.read_csv(DATA_IN+'interim/GHS-UCDB-Interp.csv')\n",
    "p16 = pop[['ID_HDC_G0', 'P2016']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdays_pop = pd.merge(city_coefs[['ID_HDC_G0']], p16, on = 'ID_HDC_G0', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pdays_pop['P2016'].sum() / p16['P2016'].sum() * 100 \n",
    "print('What is the global urban population in 2016', p16['P2016'].sum())\n",
    "print('How many people live in cities with increasing exp in 2016', pdays_pop['P2016'].sum())\n",
    "print('What pct of total urban pop has sig increase exp in 2015', ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From UN-DESA 2018 estimates for total global pop in 2015\n",
    "ans =  pdays_pop['P2016'].sum() / 7383009000 * 100\n",
    "print('What pct of total world pop has sig increase exp in 2016', ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UN-DESA Urban pop in 2015 was  3 981 498\n",
    "p16['P2016'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Heat Days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_totdays = pd.read_json(DATA_IN+'processed/PNAS-DATA-v2/WBGT32_1D_TREND_HEATP05.json', orient = 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('What pct of all cities had sig increase in days/yr > WBGT32 C ?')\n",
    "print(len(city_totdays)/len(GHS))\n",
    "print(len(city_totdays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('What pct of all cities >1 day / yr in days/yr > WBGT32 C ?')\n",
    "print(len(city_totdays[city_totdays['coef_totDays'] >= 1])/len(GHS))\n",
    "print(len(city_totdays[city_totdays['coef_totDays'] >= 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many cities day increase per year ... 1, 3\n",
    "top = len(city_totdays)\n",
    "bottom = len(city_totdays[city_totdays['coef_totDays'] >= 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(top)\n",
    "print(bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What are some big cities?\n",
    "hot50 = city_totdays[(city_totdays['coef_totDays'] >= 1.5) * city_totdays['P2016'] >= 5*10**5][['coef_totDays', 'UC_NM_MN']].sort_values('coef_totDays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hot50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Columbo & San Salvador & Conakry\n",
    "print('Conakry tot days:', city_totdays[city_totdays['ID_HDC_G0'] == 1502]['coef_totDays'].values)\n",
    "print('columbo tot days:', city_totdays[city_totdays['ID_HDC_G0'] == 8835]['coef_totDays'].values)\n",
    "print('San Sal tot days:', city_totdays[city_totdays['ID_HDC_G0'] == 321]['coef_totDays'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dehli & Kolkata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delhi 6955 & Kolkata 9691\n",
    "K = city_coefs[city_coefs['ID_HDC_G0']== 9691]\n",
    "D = city_coefs[city_coefs['ID_HDC_G0']== 6955]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Share of heat Kolkata', K.coef_heat / K.coef_pdays * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Share of heat Delhi', D.coef_heat / D.coef_pdays * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Populations of specific cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pd.read_csv(DATA_IN+'interim/GHS-UCDB-Interp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9691, Kolkata 1998\n",
    "# 2046, Paris 2003\n",
    "# 4417, Aleppo 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pop[pop['ID_HDC_G0'] == 9691]['P2015'] / 10**3\n",
    "print('Pop of Kolkata in 1998', ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regional Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Annual Rates\n",
    "\n",
    "scale = 10**6\n",
    "geog = 'sub-region'\n",
    "\n",
    "for label in np.unique(HI_STATS[geog]):\n",
    "    label = label\n",
    "    data = HI_STATS[HI_STATS[geog] == label]\n",
    "    \n",
    "    #### Rate of change\n",
    "    coef, r2, p = lm_func(data, 'people_days')\n",
    "    print('annual increase in people days '+label, 'was', coef/scale, ' p=', p)\n",
    "    coef1, r21, p1 = lm_func(data, 'people_days_heat')\n",
    "    print('annual increase in people days heat '+label, 'was', coef1/scale, ' p=', p)\n",
    "    coef2, r22, p2 = lm_func(data, 'people_days_pop')\n",
    "    print('annual increase in people days pop '+label, 'was', coef2/scale, ' p=', p)\n",
    "    print('attrib heat '+label, 'was', coef1 / coef *100, ' p=', p, '\\n')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Trends for Africa, N & SS\n",
    "geog = 'region'\n",
    "location = 'Africa'\n",
    "data = HI_STATS[HI_STATS[geog] == location]\n",
    "print(location)\n",
    "\n",
    "#### Total Change in people Days\n",
    "data = data.groupby('year')['people_days'].sum()\n",
    "year = str(data.index[33])\n",
    "value = str(data.values[33]/10**9)\n",
    "print('person days in 2016 was '+value+' billion')\n",
    "\n",
    "year = str(data.index[0])\n",
    "value = str(data.values[0]/10**9)\n",
    "print('person days in 1983 was '+value+' billion')\n",
    "\n",
    "#### Pct Change in Poeple Days 1983 - 2016\n",
    "pdays16 = data.iloc[len(data) -1]\n",
    "pdays83 = data.iloc[0]\n",
    "out = (data.iloc[len(data) -1] - data.iloc[0]) / data.iloc[0] * 100\n",
    "print('pct increase in people days 83 - 16 is ', out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### S Asia as pct of total  global = 5.245146271 B \n",
    "\n",
    "print('pct of total pdays from S Asia is ', 1899.70765 / 10**3 / 5.245146271 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Median Slope\n",
    "region = 'Europe'\n",
    "col = 'coef_heat'\n",
    "geog = 'region'\n",
    "scale = 10**3\n",
    "result = city_coefs[city_coefs[geog]== region][col].median()\n",
    "print(region, col, 'is ', result/scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat Waves\n",
    "\n",
    "- 9691 Kolkata 1998\n",
    "- 2046 Paris 2003\n",
    "- 4417, Aleppo 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(dir_in, geog, location):\n",
    "    \"\"\"Function makes data to plot daily city-level HI Max and average\n",
    "    Args:\n",
    "        dir_in = directory to get data\n",
    "        geog = column for geography, city-level = 'ID_HDC_G0'\n",
    "        location = usually a city id\n",
    "    \"\"\"\n",
    "    \n",
    "    fn_list = sorted(glob.glob(dir_in+'*.csv')) # get data\n",
    "    df_out = pd.DataFrame() # to write dataframe\n",
    "    \n",
    "     # get leap year cols from 2016\n",
    "    hi16 = pd.read_csv(fn_list[33]) \n",
    "    cols = list(hi16.iloc[:,3:].columns)\n",
    "    cols = [year[5:] for year in cols] # cols for data frame\n",
    "    \n",
    "    temp_list = [] # empty list for temps\n",
    "    \n",
    "    # loop through dir and get data\n",
    "    for i, fn in enumerate(fn_list):\n",
    "        df = pd.read_csv(fn) # open data frame\n",
    "        year_label = [(df.columns[3]).split('.')[0]] # get year\n",
    "        row = df[df[geog] == location]\n",
    "        temp = row.iloc[:,3:] # get only temp columns\n",
    "        \n",
    "        # add in col for leap years\n",
    "        if temp.shape[1] == 365:\n",
    "            temp.insert(loc = 59, column = year_label[0]+'.02.29', value = np.nan, allow_duplicates=False)\n",
    "\n",
    "        # Set Index & Columns\n",
    "        temp.index = year_label\n",
    "        temp.columns = cols # revalue to m.d\n",
    "    \n",
    "        # add to list\n",
    "        temp_list.append(temp)\n",
    "    \n",
    "    df_out = pd.concat(temp_list) # make one big dataframe\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df, year, start, end):#, start, end):\n",
    "    \"\"\" Make the data for a plot\n",
    "    Args: \n",
    "        df = df w/ daily HI max for a given city\n",
    "        year = year you want to plot against average\n",
    "        start = start of plot in julian days (e.g 1 - 365/366)\n",
    "        end = end of plot in julian days\n",
    "    \"\"\"\n",
    "\n",
    "    # Deal with leap year\n",
    "    if year % 4 !=0:\n",
    "        df.drop(columns ='02.29', inplace = True)\n",
    "    \n",
    "    # Subset data\n",
    "    start = start - 1 # zero indexing \n",
    "    subset = df.iloc[:,start:end]\n",
    "    \n",
    "    # HI Max for year\n",
    "    hi_year = subset.loc[str(year)]\n",
    "    \n",
    "    # make 34-avg daily hi\n",
    "    means = subset.mean(axis = 0)\n",
    "    \n",
    "    # make colums to date time\n",
    "    cols = pd.to_datetime([str(year)+'.'+date for date in hi_year.index])\n",
    "    \n",
    "    return hi_year, means, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_HDC_G0</th>\n",
       "      <th>year</th>\n",
       "      <th>total_days</th>\n",
       "      <th>duration</th>\n",
       "      <th>avg_temp</th>\n",
       "      <th>avg_intensity</th>\n",
       "      <th>tot_intensity</th>\n",
       "      <th>event_dates</th>\n",
       "      <th>intensity</th>\n",
       "      <th>tmax</th>\n",
       "      <th>UID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>4417</td>\n",
       "      <td>2010</td>\n",
       "      <td>666</td>\n",
       "      <td>2</td>\n",
       "      <td>46.356890</td>\n",
       "      <td>0.256890</td>\n",
       "      <td>0.513780</td>\n",
       "      <td>[2010.07.19, 2010.07.20]</td>\n",
       "      <td>[0.3747319333, 0.13904817090000002]</td>\n",
       "      <td>[46.4747319333, 46.2390481709]</td>\n",
       "      <td>UID-1718681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>4417</td>\n",
       "      <td>2010</td>\n",
       "      <td>666</td>\n",
       "      <td>1</td>\n",
       "      <td>47.189391</td>\n",
       "      <td>1.089391</td>\n",
       "      <td>1.089391</td>\n",
       "      <td>[2010.07.22]</td>\n",
       "      <td>[1.089390514]</td>\n",
       "      <td>[47.189390514]</td>\n",
       "      <td>UID-1718682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4417</td>\n",
       "      <td>2010</td>\n",
       "      <td>666</td>\n",
       "      <td>1</td>\n",
       "      <td>46.610048</td>\n",
       "      <td>0.510048</td>\n",
       "      <td>0.510048</td>\n",
       "      <td>[2010.07.26]</td>\n",
       "      <td>[0.5100479559000001]</td>\n",
       "      <td>[46.6100479559]</td>\n",
       "      <td>UID-1718683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>4417</td>\n",
       "      <td>2010</td>\n",
       "      <td>666</td>\n",
       "      <td>9</td>\n",
       "      <td>51.839383</td>\n",
       "      <td>5.739383</td>\n",
       "      <td>51.654447</td>\n",
       "      <td>[2010.07.29, 2010.07.30, 2010.07.31, 2010.08.0...</td>\n",
       "      <td>[3.2814992021, 7.1096257613, 12.5753128385, 6....</td>\n",
       "      <td>[49.3814992021, 53.2096257613, 58.6753128385, ...</td>\n",
       "      <td>UID-1718684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>4417</td>\n",
       "      <td>2010</td>\n",
       "      <td>666</td>\n",
       "      <td>7</td>\n",
       "      <td>53.780789</td>\n",
       "      <td>7.680789</td>\n",
       "      <td>53.765526</td>\n",
       "      <td>[2010.08.10, 2010.08.11, 2010.08.12, 2010.08.1...</td>\n",
       "      <td>[4.7263495980000005, 9.0482525407, 8.917087211...</td>\n",
       "      <td>[50.826349598, 55.1482525407, 55.0170872119, 5...</td>\n",
       "      <td>UID-1718685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>4417</td>\n",
       "      <td>2010</td>\n",
       "      <td>666</td>\n",
       "      <td>3</td>\n",
       "      <td>49.940370</td>\n",
       "      <td>3.840370</td>\n",
       "      <td>11.521111</td>\n",
       "      <td>[2010.08.19, 2010.08.20, 2010.08.21]</td>\n",
       "      <td>[5.0217016262, 3.2292439878, 3.2701651029]</td>\n",
       "      <td>[51.1217016262, 49.3292439878, 49.3701651029]</td>\n",
       "      <td>UID-1718686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>4417</td>\n",
       "      <td>2010</td>\n",
       "      <td>666</td>\n",
       "      <td>3</td>\n",
       "      <td>47.528756</td>\n",
       "      <td>1.428756</td>\n",
       "      <td>4.286268</td>\n",
       "      <td>[2010.08.23, 2010.08.24, 2010.08.25]</td>\n",
       "      <td>[1.0340144974, 2.3068767788, 0.9453762832]</td>\n",
       "      <td>[47.1340144974, 48.4068767788, 47.0453762832]</td>\n",
       "      <td>UID-1718687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>4417</td>\n",
       "      <td>2010</td>\n",
       "      <td>666</td>\n",
       "      <td>1</td>\n",
       "      <td>47.976825</td>\n",
       "      <td>1.876825</td>\n",
       "      <td>1.876825</td>\n",
       "      <td>[2010.09.05]</td>\n",
       "      <td>[1.8768247615]</td>\n",
       "      <td>[47.9768247615]</td>\n",
       "      <td>UID-1718688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>4417</td>\n",
       "      <td>2010</td>\n",
       "      <td>666</td>\n",
       "      <td>3</td>\n",
       "      <td>46.524747</td>\n",
       "      <td>0.424747</td>\n",
       "      <td>1.274240</td>\n",
       "      <td>[2010.09.09, 2010.09.10, 2010.09.11]</td>\n",
       "      <td>[0.4653870438, 0.6443832787, 0.16447000450000002]</td>\n",
       "      <td>[46.5653870438, 46.7443832787, 46.2644700045]</td>\n",
       "      <td>UID-1718689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID_HDC_G0  year  total_days  duration   avg_temp  avg_intensity  \\\n",
       "148       4417  2010         666         2  46.356890       0.256890   \n",
       "149       4417  2010         666         1  47.189391       1.089391   \n",
       "150       4417  2010         666         1  46.610048       0.510048   \n",
       "151       4417  2010         666         9  51.839383       5.739383   \n",
       "152       4417  2010         666         7  53.780789       7.680789   \n",
       "153       4417  2010         666         3  49.940370       3.840370   \n",
       "154       4417  2010         666         3  47.528756       1.428756   \n",
       "155       4417  2010         666         1  47.976825       1.876825   \n",
       "156       4417  2010         666         3  46.524747       0.424747   \n",
       "\n",
       "     tot_intensity                                        event_dates  \\\n",
       "148       0.513780                           [2010.07.19, 2010.07.20]   \n",
       "149       1.089391                                       [2010.07.22]   \n",
       "150       0.510048                                       [2010.07.26]   \n",
       "151      51.654447  [2010.07.29, 2010.07.30, 2010.07.31, 2010.08.0...   \n",
       "152      53.765526  [2010.08.10, 2010.08.11, 2010.08.12, 2010.08.1...   \n",
       "153      11.521111               [2010.08.19, 2010.08.20, 2010.08.21]   \n",
       "154       4.286268               [2010.08.23, 2010.08.24, 2010.08.25]   \n",
       "155       1.876825                                       [2010.09.05]   \n",
       "156       1.274240               [2010.09.09, 2010.09.10, 2010.09.11]   \n",
       "\n",
       "                                             intensity  \\\n",
       "148                [0.3747319333, 0.13904817090000002]   \n",
       "149                                      [1.089390514]   \n",
       "150                               [0.5100479559000001]   \n",
       "151  [3.2814992021, 7.1096257613, 12.5753128385, 6....   \n",
       "152  [4.7263495980000005, 9.0482525407, 8.917087211...   \n",
       "153         [5.0217016262, 3.2292439878, 3.2701651029]   \n",
       "154         [1.0340144974, 2.3068767788, 0.9453762832]   \n",
       "155                                     [1.8768247615]   \n",
       "156  [0.4653870438, 0.6443832787, 0.16447000450000002]   \n",
       "\n",
       "                                                  tmax          UID  \n",
       "148                     [46.4747319333, 46.2390481709]  UID-1718681  \n",
       "149                                     [47.189390514]  UID-1718682  \n",
       "150                                    [46.6100479559]  UID-1718683  \n",
       "151  [49.3814992021, 53.2096257613, 58.6753128385, ...  UID-1718684  \n",
       "152  [50.826349598, 55.1482525407, 55.0170872119, 5...  UID-1718685  \n",
       "153      [51.1217016262, 49.3292439878, 49.3701651029]  UID-1718686  \n",
       "154      [47.1340144974, 48.4068767788, 47.0453762832]  UID-1718687  \n",
       "155                                    [47.9768247615]  UID-1718688  \n",
       "156      [46.5653870438, 46.7443832787, 46.2644700045]  UID-1718689  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find Heat Wave From All DATA\n",
    "def select_city_year(df, city_id, year):\n",
    "    \"Quick search to find city and years within HI_STATS\"\n",
    "    df_out = df[(df['ID_HDC_G0'] == city_id) & (df['year'] == year)]\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "# open heat events data\n",
    "events_fn = '/home/cascade/projects/UrbanHeat/data/processed/PNAS-DATA-v2/HI461_1D_STATS.json'\n",
    "events = pd.read_json(events_fn, orient = 'split')\n",
    "\n",
    "# [4417, 'Aleppo'] [2046, 'Paris'] [9691, 'Kolkata'] \n",
    "city = select_city_year(events, 4417, 2010)\n",
    "city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select event and compare to long term means\n",
    "# Kolkata 1998 UID-2920905, Aleppo 2010 \n",
    "event_id = 'UID-2920905'\n",
    "event = city[city['UID'] == event_id]\n",
    "tmax = list(event['tmax'])[0]\n",
    "dates = list(event['event_dates'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Heat Index Data\n",
    "data = 'HI'\n",
    "DATA_IN = '/home/cascade/projects/UrbanHeat/data/interim/CHIRTS_DAILY/'+data+'/' # output from avg temp\n",
    "t = 46.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get long term means\n",
    "# Args\n",
    "#[4417, 'Aleppo'] 2010 [2046, 'Paris'] 2003 [9691, 'Kolkata'] 1998 ['6955, Dehli']\n",
    "\n",
    "# Args\n",
    "city_list = [4417, 'Aleppo'] \n",
    "year = 2010\n",
    "\n",
    "# April 1 to Sep 30 (Use Julian Days), or 1 - 182 lagos\n",
    "start = 91 \n",
    "end = 273\n",
    "\n",
    "# Make Data \n",
    "df = make_data(DATA_IN, 'ID_HDC_G0', city_list[0])\n",
    "years, means, cols = plot_data(df, year, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2010-04-01', '2010-04-02', '2010-04-03', '2010-04-04',\n",
       "               '2010-04-05', '2010-04-06', '2010-04-07', '2010-04-08',\n",
       "               '2010-04-09', '2010-04-10',\n",
       "               ...\n",
       "               '2010-09-21', '2010-09-22', '2010-09-23', '2010-09-24',\n",
       "               '2010-09-25', '2010-09-26', '2010-09-27', '2010-09-28',\n",
       "               '2010-09-29', '2010-09-30'],\n",
       "              dtype='datetime64[ns]', length=183, freq=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df\n",
    "df = pd.DataFrame()\n",
    "df['dates'] = cols\n",
    "df['max']  = years.to_list()\n",
    "df['avg'] = means.to_list()\n",
    "df['dif'] = df['max'] - df['avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas._libs.tslibs.timestamps.Timestamp"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['dates'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dates[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end of heat wave\n",
    "start = dates[0]\n",
    "end = dates[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = df[(df['dates'] >= start) & (df['dates'] <= end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dates    1998-06-14 00:00:00\n",
       "max                  66.0351\n",
       "avg                  55.5479\n",
       "dif                  12.5468\n",
       "dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "      <th>max</th>\n",
       "      <th>avg</th>\n",
       "      <th>dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1998-06-08</td>\n",
       "      <td>65.644533</td>\n",
       "      <td>53.097732</td>\n",
       "      <td>12.546801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dates        max        avg        dif\n",
       "68 1998-06-08  65.644533  53.097732  12.546801"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[out['dif'] == out['dif'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check out cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_city_year(df, city_id, year):\n",
    "    \"Quick search to find city and years within HI_STATS\"\n",
    "    df_out = df[(df['ID_HDC_G0'] == city_id) & (df['year'] == year)]\n",
    "    \n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = select_city_year(ALL_DATA, 4417, 2010)\n",
    "city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = '/home/cascade/projects/UrbanHeat/data/interim/ERA5_HI/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'GHS-ERA5-HI_2009.csv'\n",
    "data = pd.read_csv(dir_list+fn)\n",
    "city = data[data['ID_HDC_G0'] == 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps_list = []\n",
    "dates_list = []\n",
    "for fn in sorted(os.listdir(dir_list)):\n",
    "    data = pd.read_csv(dir_list+fn)\n",
    "    city = data[data['ID_HDC_G0'] == 14]\n",
    "    dates = list(city.iloc[:,3:])\n",
    "    temps = list(city.iloc[:,3:].values[0])\n",
    "    dates_list.extend(dates)\n",
    "    temps_list.extend(temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(dates_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Africa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dependencies\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(dir_in, geog, location):\n",
    "    \"\"\"Function makes data to plot daily city-level HI Max and average\n",
    "    Args:\n",
    "        dir_in = directory to get data\n",
    "        geog = column for geography, city-level = 'ID_HDC_G0'\n",
    "        location = usually a city id\n",
    "    \"\"\"\n",
    "    \n",
    "    fn_list = sorted(glob.glob(dir_in+'*.csv')) # get data\n",
    "    df_out = pd.DataFrame() # to write dataframe\n",
    "    \n",
    "     # get leap year cols from 2016\n",
    "    hi16 = pd.read_csv(fn_list[33]) \n",
    "    cols = list(hi16.iloc[:,3:].columns)\n",
    "    cols = [year[5:] for year in cols] # cols for data frame\n",
    "    \n",
    "    temp_list = [] # empty list for temps\n",
    "    \n",
    "    # loop through dir and get data\n",
    "    for i, fn in enumerate(fn_list):\n",
    "        df = pd.read_csv(fn) # open data frame\n",
    "        year_label = [(df.columns[3]).split('.')[0]] # get year\n",
    "        row = df[df[geog] == location]\n",
    "        temp = row.iloc[:,3:] # get only temp columns\n",
    "        \n",
    "        # add in col for leap years\n",
    "        if temp.shape[1] == 365:\n",
    "            temp.insert(loc = 59, column = year_label[0]+'.02.29', value = np.nan, allow_duplicates=False)\n",
    "\n",
    "        # Set Index & Columns\n",
    "        temp.index = year_label\n",
    "        temp.columns = cols # revalue to m.d\n",
    "    \n",
    "        # add to list\n",
    "        temp_list.append(temp)\n",
    "    \n",
    "    df_out = pd.concat(temp_list) # make one big dataframe\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df, year, start, end):#, start, end):\n",
    "    \"\"\" Make the data for a plot\n",
    "    Args: \n",
    "        df = df w/ daily HI max for a given city\n",
    "        year = year you want to plot against average\n",
    "        start = start of plot in julian days (e.g 1 - 365/366)\n",
    "        end = end of plot in julian days\n",
    "    \"\"\"\n",
    "\n",
    "    # Deal with leap year\n",
    "    if year % 4 !=0:\n",
    "        df.drop(columns ='02.29', inplace = True)\n",
    "    \n",
    "    # Subset data\n",
    "    start = start - 1 # zero indexing \n",
    "    subset = df.iloc[:,start:end]\n",
    "    \n",
    "    # HI Max for year\n",
    "    hi_year = subset.loc[str(year)]\n",
    "    \n",
    "    # make 34-avg daily hi\n",
    "    means = subset.mean(axis = 0)\n",
    "    \n",
    "    # make colums to date time\n",
    "    cols = pd.to_datetime([str(year)+'.'+date for date in hi_year.index])\n",
    "    \n",
    "    return hi_year, means, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args\n",
    "#[3342, 'Cairo'] #[1910, 'Accra'] 3268, Cape Town\n",
    "#[4417, 'Aleppo'] 2010 [2046, 'Paris'] 2003 [9691, 'Kolkata'] 1998 ['6955, Dehli']\n",
    "\n",
    "# Args\n",
    "city_list = [9691, 'Kolkata'] \n",
    "year = 1998\n",
    "font_size = 10\n",
    "\n",
    "\n",
    "# April 1 to Sep 30 (Use Julian Days), or 1 - 182 lagos\n",
    "start = 91 \n",
    "end = 273\n",
    "\n",
    "# Labels\n",
    "hi_label = str(year)+' '+data\n",
    "labels =  ['avg. '+data, hi_label, str(t)+''+DS+'C']  # <<<<<------------ Be sure to update! \n",
    "\n",
    "# Make Data \n",
    "df = make_data(DATA_IN, 'ID_HDC_G0', city_list[0])\n",
    "years, means, cols = plot_data(df, year, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend First vs. Second Half of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Share of exposure due to heat by 17 year split\n",
    "\n",
    "## 1983 - 1999\n",
    "data1 = HI_STATS[(HI_STATS['year'] >= 1983) & (HI_STATS['year'] < 2000)]\n",
    "coef1pop , r21pop, p1pop  = lm_func(data1 , 'people_days_pop')\n",
    "coef1heat , r21heat, p1heat = lm_func(data1 , 'people_days_heat')\n",
    "\n",
    "years = list(np.unique(data1['year']))\n",
    "plt.plot(years, data1.groupby('year')['people_days_heat'].sum())\n",
    "sns.regplot(years, data1.groupby('year')['people_days_heat'].sum(), \n",
    "            color = 'blue', scatter = False, truncate = True)\n",
    "\n",
    "## 2000 - 2016\n",
    "data2 = HI_STATS[(HI_STATS['year'] >= 2000) & (HI_STATS['year'] <= 2016)]\n",
    "coef2heat , r22heat, p2heat = lm_func(data2 , 'people_days_heat')\n",
    "coef2pop , r22pop, p1pop  = lm_func(data2 , 'people_days_pop')\n",
    "\n",
    "years = list(np.unique(data1['year']))\n",
    "plt.plot(years, data2.groupby('year')['people_days_heat'].sum())\n",
    "sns.regplot(years, data2.groupby('year')['people_days_heat'].sum(), \n",
    "            color = 'orange', scatter = False, truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2000 - 2016\n",
    "data2pop = HI_STATS[(HI_STATS['year'] >= 1983) & (HI_STATS['year'] < 2000)]\n",
    "coef2pop , r22pop, p1pop  = lm_func(data2pop , 'people_days_pop')\n",
    "\n",
    "data2heat = HI_STATS[(HI_STATS['year'] >= 2000) & (HI_STATS['year'] <= 2016)]\n",
    "coef2heat , r22heat, p2heat = lm_func(data2heat , 'people_days_heat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Estimates\n",
    "print('From 83 - 99, contribution from heat was', coef1heat/(coef1pop+coef1heat))\n",
    "print('From 00 - 16, contribution from heat was', coef2heat/(coef2pop+coef2heat))\n",
    "print('From 83 - 00, heat was', coef1heat/10**9, round(p1heat, 3))\n",
    "print('From 00 - 16, heat was', coef2heat/10**9)\n",
    "print('From 83 - 00, pop was', coef1pop/10**9)\n",
    "print('From 00 - 16, pop was', coef2pop/10**9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
