{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to explore updating PNAS data\n",
    "- rewrite code to make json not csv <br>\n",
    "- what are we dropping city year combos in pdays routine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from random import random\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import geopandas as gpd \n",
    "from glob import glob\n",
    "from statistics import mean\n",
    "import julian\n",
    "import time \n",
    "import multiprocessing as mp \n",
    "from multiprocessing import Pool\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "DATA_PATH = '/home/cascade/projects/UrbanHeat/data/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID_HDC_G0</th>\n",
       "      <th>CTR_MN_NM</th>\n",
       "      <th>1983.01.01</th>\n",
       "      <th>1983.01.02</th>\n",
       "      <th>1983.01.03</th>\n",
       "      <th>1983.01.04</th>\n",
       "      <th>1983.01.05</th>\n",
       "      <th>1983.01.06</th>\n",
       "      <th>1983.01.07</th>\n",
       "      <th>...</th>\n",
       "      <th>1983.12.22</th>\n",
       "      <th>1983.12.23</th>\n",
       "      <th>1983.12.24</th>\n",
       "      <th>1983.12.25</th>\n",
       "      <th>1983.12.26</th>\n",
       "      <th>1983.12.27</th>\n",
       "      <th>1983.12.28</th>\n",
       "      <th>1983.12.29</th>\n",
       "      <th>1983.12.30</th>\n",
       "      <th>1983.12.31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5782</td>\n",
       "      <td>Russia</td>\n",
       "      <td>-49.590861</td>\n",
       "      <td>-38.042995</td>\n",
       "      <td>-36.750948</td>\n",
       "      <td>-28.374049</td>\n",
       "      <td>-26.626346</td>\n",
       "      <td>-21.826344</td>\n",
       "      <td>-21.446321</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.149449</td>\n",
       "      <td>-36.333571</td>\n",
       "      <td>-30.164864</td>\n",
       "      <td>-19.821702</td>\n",
       "      <td>-14.509835</td>\n",
       "      <td>-14.259680</td>\n",
       "      <td>-16.596240</td>\n",
       "      <td>-14.705007</td>\n",
       "      <td>-14.277887</td>\n",
       "      <td>-16.171796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3316</td>\n",
       "      <td>Russia</td>\n",
       "      <td>-4.088565</td>\n",
       "      <td>-2.252119</td>\n",
       "      <td>-6.184317</td>\n",
       "      <td>-13.705580</td>\n",
       "      <td>-7.631833</td>\n",
       "      <td>-7.392443</td>\n",
       "      <td>-5.487767</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.845948</td>\n",
       "      <td>-18.931796</td>\n",
       "      <td>-8.486376</td>\n",
       "      <td>-8.990228</td>\n",
       "      <td>-15.775358</td>\n",
       "      <td>-19.373392</td>\n",
       "      <td>-19.813643</td>\n",
       "      <td>-12.648720</td>\n",
       "      <td>-13.315901</td>\n",
       "      <td>-15.861366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5645</td>\n",
       "      <td>Russia</td>\n",
       "      <td>-38.693309</td>\n",
       "      <td>-23.243517</td>\n",
       "      <td>-13.361891</td>\n",
       "      <td>-11.548153</td>\n",
       "      <td>-16.044299</td>\n",
       "      <td>-18.756276</td>\n",
       "      <td>-24.426800</td>\n",
       "      <td>...</td>\n",
       "      <td>-20.344301</td>\n",
       "      <td>-26.453421</td>\n",
       "      <td>-16.921110</td>\n",
       "      <td>-2.669969</td>\n",
       "      <td>-4.402858</td>\n",
       "      <td>-9.637487</td>\n",
       "      <td>-11.979378</td>\n",
       "      <td>-15.932049</td>\n",
       "      <td>-15.186321</td>\n",
       "      <td>-12.247702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3185</td>\n",
       "      <td>Finland</td>\n",
       "      <td>-0.263081</td>\n",
       "      <td>0.372479</td>\n",
       "      <td>-5.046791</td>\n",
       "      <td>-2.224906</td>\n",
       "      <td>-1.583173</td>\n",
       "      <td>-0.187118</td>\n",
       "      <td>-0.431349</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.141768</td>\n",
       "      <td>-3.382719</td>\n",
       "      <td>-2.959680</td>\n",
       "      <td>-7.515815</td>\n",
       "      <td>-13.457510</td>\n",
       "      <td>-4.966569</td>\n",
       "      <td>-5.707407</td>\n",
       "      <td>-10.071544</td>\n",
       "      <td>-9.986938</td>\n",
       "      <td>-10.397102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3539</td>\n",
       "      <td>Russia</td>\n",
       "      <td>-10.146690</td>\n",
       "      <td>-9.364139</td>\n",
       "      <td>-20.047189</td>\n",
       "      <td>-16.052717</td>\n",
       "      <td>-12.344176</td>\n",
       "      <td>-5.424159</td>\n",
       "      <td>-0.371261</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.950157</td>\n",
       "      <td>-2.916705</td>\n",
       "      <td>-1.734350</td>\n",
       "      <td>-1.815293</td>\n",
       "      <td>-6.173747</td>\n",
       "      <td>-10.115407</td>\n",
       "      <td>-10.211072</td>\n",
       "      <td>-10.359057</td>\n",
       "      <td>-10.359030</td>\n",
       "      <td>-11.863613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 368 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ID_HDC_G0 CTR_MN_NM  1983.01.01  1983.01.02  1983.01.03  \\\n",
       "0           0       5782    Russia  -49.590861  -38.042995  -36.750948   \n",
       "1           1       3316    Russia   -4.088565   -2.252119   -6.184317   \n",
       "2           2       5645    Russia  -38.693309  -23.243517  -13.361891   \n",
       "3           3       3185   Finland   -0.263081    0.372479   -5.046791   \n",
       "4           4       3539    Russia  -10.146690   -9.364139  -20.047189   \n",
       "\n",
       "   1983.01.04  1983.01.05  1983.01.06  1983.01.07  ...  1983.12.22  \\\n",
       "0  -28.374049  -26.626346  -21.826344  -21.446321  ...  -27.149449   \n",
       "1  -13.705580   -7.631833   -7.392443   -5.487767  ...  -12.845948   \n",
       "2  -11.548153  -16.044299  -18.756276  -24.426800  ...  -20.344301   \n",
       "3   -2.224906   -1.583173   -0.187118   -0.431349  ...   -6.141768   \n",
       "4  -16.052717  -12.344176   -5.424159   -0.371261  ...   -5.950157   \n",
       "\n",
       "   1983.12.23  1983.12.24  1983.12.25  1983.12.26  1983.12.27  1983.12.28  \\\n",
       "0  -36.333571  -30.164864  -19.821702  -14.509835  -14.259680  -16.596240   \n",
       "1  -18.931796   -8.486376   -8.990228  -15.775358  -19.373392  -19.813643   \n",
       "2  -26.453421  -16.921110   -2.669969   -4.402858   -9.637487  -11.979378   \n",
       "3   -3.382719   -2.959680   -7.515815  -13.457510   -4.966569   -5.707407   \n",
       "4   -2.916705   -1.734350   -1.815293   -6.173747  -10.115407  -10.211072   \n",
       "\n",
       "   1983.12.29  1983.12.30  1983.12.31  \n",
       "0  -14.705007  -14.277887  -16.171796  \n",
       "1  -12.648720  -13.315901  -15.861366  \n",
       "2  -15.932049  -15.186321  -12.247702  \n",
       "3  -10.071544   -9.986938  -10.397102  \n",
       "4  -10.359057  -10.359030  -11.863613  \n",
       "\n",
       "[5 rows x 368 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore files - 1983\n",
    "content = os.listdir(DATA_PATH+'interim/')\n",
    "fns = sorted(glob(DATA_PATH+'interim/ERA5_HI/*csv'))\n",
    "fn = fns[0]\n",
    "df1983 = pd.read_csv(fn)\n",
    "df1983.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID_HDC_G0</th>\n",
       "      <th>CTR_MN_NM</th>\n",
       "      <th>1983.01.01</th>\n",
       "      <th>1983.01.02</th>\n",
       "      <th>1983.01.03</th>\n",
       "      <th>1983.01.04</th>\n",
       "      <th>1983.01.05</th>\n",
       "      <th>1983.01.06</th>\n",
       "      <th>1983.01.07</th>\n",
       "      <th>...</th>\n",
       "      <th>1983.12.22</th>\n",
       "      <th>1983.12.23</th>\n",
       "      <th>1983.12.24</th>\n",
       "      <th>1983.12.25</th>\n",
       "      <th>1983.12.26</th>\n",
       "      <th>1983.12.27</th>\n",
       "      <th>1983.12.28</th>\n",
       "      <th>1983.12.29</th>\n",
       "      <th>1983.12.30</th>\n",
       "      <th>1983.12.31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5782</td>\n",
       "      <td>Russia</td>\n",
       "      <td>-49.590861</td>\n",
       "      <td>-38.042995</td>\n",
       "      <td>-36.750948</td>\n",
       "      <td>-28.374049</td>\n",
       "      <td>-26.626346</td>\n",
       "      <td>-21.826344</td>\n",
       "      <td>-21.446321</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.149449</td>\n",
       "      <td>-36.333571</td>\n",
       "      <td>-30.164864</td>\n",
       "      <td>-19.821702</td>\n",
       "      <td>-14.509835</td>\n",
       "      <td>-14.259680</td>\n",
       "      <td>-16.596240</td>\n",
       "      <td>-14.705007</td>\n",
       "      <td>-14.277887</td>\n",
       "      <td>-16.171796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3316</td>\n",
       "      <td>Russia</td>\n",
       "      <td>-4.088565</td>\n",
       "      <td>-2.252119</td>\n",
       "      <td>-6.184317</td>\n",
       "      <td>-13.705580</td>\n",
       "      <td>-7.631833</td>\n",
       "      <td>-7.392443</td>\n",
       "      <td>-5.487767</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.845948</td>\n",
       "      <td>-18.931796</td>\n",
       "      <td>-8.486376</td>\n",
       "      <td>-8.990228</td>\n",
       "      <td>-15.775358</td>\n",
       "      <td>-19.373392</td>\n",
       "      <td>-19.813643</td>\n",
       "      <td>-12.648720</td>\n",
       "      <td>-13.315901</td>\n",
       "      <td>-15.861366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5645</td>\n",
       "      <td>Russia</td>\n",
       "      <td>-38.693309</td>\n",
       "      <td>-23.243517</td>\n",
       "      <td>-13.361891</td>\n",
       "      <td>-11.548153</td>\n",
       "      <td>-16.044299</td>\n",
       "      <td>-18.756276</td>\n",
       "      <td>-24.426800</td>\n",
       "      <td>...</td>\n",
       "      <td>-20.344301</td>\n",
       "      <td>-26.453421</td>\n",
       "      <td>-16.921110</td>\n",
       "      <td>-2.669969</td>\n",
       "      <td>-4.402858</td>\n",
       "      <td>-9.637487</td>\n",
       "      <td>-11.979378</td>\n",
       "      <td>-15.932049</td>\n",
       "      <td>-15.186321</td>\n",
       "      <td>-12.247702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3185</td>\n",
       "      <td>Finland</td>\n",
       "      <td>-0.263081</td>\n",
       "      <td>0.372479</td>\n",
       "      <td>-5.046791</td>\n",
       "      <td>-2.224906</td>\n",
       "      <td>-1.583173</td>\n",
       "      <td>-0.187118</td>\n",
       "      <td>-0.431349</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.141768</td>\n",
       "      <td>-3.382719</td>\n",
       "      <td>-2.959680</td>\n",
       "      <td>-7.515815</td>\n",
       "      <td>-13.457510</td>\n",
       "      <td>-4.966569</td>\n",
       "      <td>-5.707407</td>\n",
       "      <td>-10.071544</td>\n",
       "      <td>-9.986938</td>\n",
       "      <td>-10.397102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3539</td>\n",
       "      <td>Russia</td>\n",
       "      <td>-10.146690</td>\n",
       "      <td>-9.364139</td>\n",
       "      <td>-20.047189</td>\n",
       "      <td>-16.052717</td>\n",
       "      <td>-12.344176</td>\n",
       "      <td>-5.424159</td>\n",
       "      <td>-0.371261</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.950157</td>\n",
       "      <td>-2.916705</td>\n",
       "      <td>-1.734350</td>\n",
       "      <td>-1.815293</td>\n",
       "      <td>-6.173747</td>\n",
       "      <td>-10.115407</td>\n",
       "      <td>-10.211072</td>\n",
       "      <td>-10.359057</td>\n",
       "      <td>-10.359030</td>\n",
       "      <td>-11.863613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 368 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ID_HDC_G0 CTR_MN_NM  1983.01.01  1983.01.02  1983.01.03  \\\n",
       "0           0       5782    Russia  -49.590861  -38.042995  -36.750948   \n",
       "1           1       3316    Russia   -4.088565   -2.252119   -6.184317   \n",
       "2           2       5645    Russia  -38.693309  -23.243517  -13.361891   \n",
       "3           3       3185   Finland   -0.263081    0.372479   -5.046791   \n",
       "4           4       3539    Russia  -10.146690   -9.364139  -20.047189   \n",
       "\n",
       "   1983.01.04  1983.01.05  1983.01.06  1983.01.07  ...  1983.12.22  \\\n",
       "0  -28.374049  -26.626346  -21.826344  -21.446321  ...  -27.149449   \n",
       "1  -13.705580   -7.631833   -7.392443   -5.487767  ...  -12.845948   \n",
       "2  -11.548153  -16.044299  -18.756276  -24.426800  ...  -20.344301   \n",
       "3   -2.224906   -1.583173   -0.187118   -0.431349  ...   -6.141768   \n",
       "4  -16.052717  -12.344176   -5.424159   -0.371261  ...   -5.950157   \n",
       "\n",
       "   1983.12.23  1983.12.24  1983.12.25  1983.12.26  1983.12.27  1983.12.28  \\\n",
       "0  -36.333571  -30.164864  -19.821702  -14.509835  -14.259680  -16.596240   \n",
       "1  -18.931796   -8.486376   -8.990228  -15.775358  -19.373392  -19.813643   \n",
       "2  -26.453421  -16.921110   -2.669969   -4.402858   -9.637487  -11.979378   \n",
       "3   -3.382719   -2.959680   -7.515815  -13.457510   -4.966569   -5.707407   \n",
       "4   -2.916705   -1.734350   -1.815293   -6.173747  -10.115407  -10.211072   \n",
       "\n",
       "   1983.12.29  1983.12.30  1983.12.31  \n",
       "0  -14.705007  -14.277887  -16.171796  \n",
       "1  -12.648720  -13.315901  -15.861366  \n",
       "2  -15.932049  -15.186321  -12.247702  \n",
       "3  -10.071544   -9.986938  -10.397102  \n",
       "4  -10.359057  -10.359030  -11.863613  \n",
       "\n",
       "[5 rows x 368 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1983.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check out the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 1 - Function Loads all Tmax Data as an X-array\n",
    "def read_data(dir_path, space_dim, time_dim):\n",
    "    \"\"\" Function reads in all Tmax .csv files, joins them by date along the x-axis\n",
    "    and returns the whole record as a x-array data array\n",
    "    \n",
    "    Args:   \n",
    "        dir_path = path to .csv files \n",
    "        time_dim = name for time dim as a str ... use date :-)\n",
    "        space_dim = col name for GHS-UCDB IDs as an str (ID_HDC_G0)\n",
    "    \"\"\"\n",
    "    fn_list = sorted(glob.glob(dir_path+'*.csv'))\n",
    "    df_out = pd.DataFrame()\n",
    "    date_list = []\n",
    "\n",
    "    # Open all Tmax files and concat into a df\n",
    "    for i, fn in enumerate(fn_list):    \n",
    "        # Open the CSV\n",
    "        df = pd.read_csv(fn)\n",
    "\n",
    "        # Get the city ids \n",
    "        if i == 1:\n",
    "            df_id = df[space_dim]\n",
    "\n",
    "        # get only the Tmax columns and concate date list \n",
    "        df_temp = df.iloc[:,3:] # get only temp columns\n",
    "        date_list = date_list+list(df_temp.columns)\n",
    "\n",
    "        # Drop cities w/ no temp record \n",
    "        df_temp_drop = df_temp.dropna()\n",
    "\n",
    "        # Merge\n",
    "        df_out = pd.concat([df_out, df_temp_drop], axis=1)\n",
    "        print(df_out.shape)\n",
    "    \n",
    "    # make date into an array\n",
    "    tmax_arr = df_out.to_numpy()\n",
    "\n",
    "    # Make data into an xr.DataArray\n",
    "    tmax_xr_da = xr.DataArray(tmax_arr, coords=[df_id, date_list], \n",
    "                             dims=[space_dim, time_dim])\n",
    "    return tmax_xr_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmax_days(xarray, Tthresh):\n",
    "    \"\"\" Function finds all the tmax days in a year and sums total days per year \n",
    "    greater than a threshold within a year where Tmax > Tthresh for each city. Returns the total number of days,\n",
    "    the dates, the tempatures, and the intensity (daily Tmax - Tthresh)\n",
    "    \n",
    "    Args: \n",
    "        xarray = an xarray object with dims = (space, times)\n",
    "        Tthresh = int of temp threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty lists & df\n",
    "    id_list = []\n",
    "    date_list = []\n",
    "    tmax_list = []\n",
    "    intensity_list = []\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    # subset xarray\n",
    "    out = xarray.where(xarray > Tthresh, drop = True)\n",
    "\n",
    "    # start loop \n",
    "    for index, loc in enumerate(out.ID_HDC_G0):\n",
    "        id_list.append(out.ID_HDC_G0.values[index]) # get IDS\n",
    "        date_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values) # get event dates\n",
    "        \n",
    "        # #CPT 2020.02.23 \n",
    "        # dayTot_list.append(len(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values)) # get event totals\n",
    "        \n",
    "        tmax_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').values) # get temp values\n",
    "        intensity_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').values - Tthresh) # get severity\n",
    "\n",
    "    # write to a data frame\n",
    "    df_out['ID_HDC_G0'] = id_list\n",
    "    # df_out['total_days'] = dayTot_list #CPT 2020.02.23\n",
    "    df_out['dates'] = date_list\n",
    "    df_out['tmax'] = tmax_list\n",
    "    df_out['tmax_tntensity'] = intensity_list\n",
    "\n",
    "    # return df_out\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_split(dates, ID_HDC_G0, intensity, tmax): #, total_days): #CPT 2020.02.23\n",
    "    \n",
    "    \"\"\" Searchs a list of dates and isolates sequential dates as a list, then calculates event stats.\n",
    "    See comments in code for more details. \n",
    "    \n",
    "    Args:\n",
    "        dates: pandas.core.index as julian dates\n",
    "        ID_HDC_G0: city ID as string\n",
    "        intensity: numpy.ndarray of intensities values\n",
    "        tmax: numpy.ndarray of intensities values of tmax values\n",
    "        total_days: total number of tmax days in a year for a given city\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # city id\n",
    "    city_id = ID_HDC_G0\n",
    "    # tot_days = total_days #CPT 2020.02.23\n",
    "    \n",
    "    # lists to fill\n",
    "    city_id_list = []\n",
    "    # tot_days_list = [] #CPT 2020.02.23\n",
    "    event_dates_list = []\n",
    "    dur_list = []\n",
    "    intensity_list = []\n",
    "    tmax_list = []\n",
    "    avg_temp_list = []\n",
    "    avg_int_list = []\n",
    "    tot_int_list = []\n",
    "    year_list = []\n",
    "    \n",
    "    # data frame out\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    # turn days into julian days\n",
    "    jul_days = jul_convert(dates)\n",
    "    \n",
    "    # Counters to make sure we write the correct event dates to a list, don't want julian days in output\n",
    "    counter = 0\n",
    "    start = 0\n",
    "    end = 0\n",
    "    \n",
    "    # Loop through dur list and isolate seq days, temps, and intensities\n",
    "    for k, g in groupby(enumerate(jul_days.values), lambda x: x[1]-x[0]):\n",
    "        \n",
    "        seq = list(map(itemgetter(1), g)) # isolate seq. days\n",
    "        dur = len(seq) # duration of each event\n",
    "        \n",
    "        counter = counter + dur # add duration to counter\n",
    "        end = counter # end of current event\n",
    "        \n",
    "        event_dates = dates[start:end] # dates of tmax days during each event\n",
    "        intense = intensity[start:end] # intensity of each day during event\n",
    "        temp = tmax[start:end] # temp of each day during event\n",
    "        avg_temp = mean(temp) # avg. temp during event\n",
    "        avg_int = mean(intense) # avg. intensity during event\n",
    "        tot_int = intense.sum() # total intensity during event\n",
    "        \n",
    "        start = counter # reset start to current end (e.g. counter)\n",
    "        year = event_dates[0].split('.')[0]\n",
    "        \n",
    "        # fill lists\n",
    "        city_id_list.append(city_id)\n",
    "        year_list.append(year)\n",
    "        # tot_days_list.append(tot_days) #CPT 2020.02.23\n",
    "        dur_list.append(dur)\n",
    "        event_dates_list.append(event_dates)\n",
    "        intensity_list.append(intense)\n",
    "        tmax_list.append(temp)\n",
    "        avg_temp_list.append(avg_temp)\n",
    "        avg_int_list.append(avg_int)\n",
    "        tot_int_list.append(tot_int)\n",
    "\n",
    "    # write out as a dateframe\n",
    "    df_out['ID_HDC_G0'] = city_id_list\n",
    "    df_out['year'] = year_list\n",
    "    # df_out['total_days'] = tot_days_list #CPT 2020.02.23\n",
    "    df_out['duration'] = dur_list\n",
    "    df_out['avg_temp'] = avg_temp_list\n",
    "    df_out['avg_intensity'] = avg_int_list\n",
    "    df_out['tot_intensity'] = tot_int_list\n",
    "    df_out['event_dates'] = event_dates_list\n",
    "    df_out['duration'] = dur_list\n",
    "    df_out['intensity'] = intensity_list\n",
    "    df_out['tmax'] = tmax_list\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 4 function feeds output from function 3 into function 4\n",
    "def tmax_stats(df_in):\n",
    "    \"\"\" runs event_split functionon a dataframe to produce desired tmax stats\n",
    "\n",
    "        NOTE - If you add arguments to event_split to make more states,\n",
    "        be sure to update this function\n",
    "\n",
    "        args:\n",
    "            df: input dataframe\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "\n",
    "    # NOTE - If you add arguments to event_split to make more stats,\n",
    "    # be sure to update this function\n",
    "\n",
    "    for index, row in df_in.iterrows():\n",
    "        dates = row['dates'] # Get event dates\n",
    "        intensity = row['tmax_tntensity'] # Get intensity for each day\n",
    "        tmax = row['tmax'] # Get tmax for each day\n",
    "        ID_HDC_G0 = row['ID_HDC_G0'] # get city id\n",
    "        # total_days = row['total_days'] # get total number of tmax days -- CPT 2020.02.23\n",
    "\n",
    "        df = event_split(dates, ID_HDC_G0, intensity, tmax)# , total_days) #CPT 2020.02.23\n",
    "\n",
    "        df_out = df_out.append(df)\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jul_convert(dates):\n",
    "    \"Function turn days into julian datetime\"\n",
    "    jul_days = pd.to_datetime(dates).to_julian_date()\n",
    "    \n",
    "    return jul_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok let's see how this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data into an xr.DataArray\n",
    "df_id = list(df1983['ID_HDC_G0'])\n",
    "date_list = list(df1983.columns[3:])\n",
    "df_temp = df1983.iloc[:,3:] # get only temp columns\n",
    "df_temp_drop = df_temp.dropna() # drop cities with no temp record\n",
    "space_dim = 'ID_HDC_G0'\n",
    "time_dim = 'date'\n",
    "\n",
    "tmax_arr = df_temp_drop.to_numpy()\n",
    "\n",
    "tmax_xr_da = xr.DataArray(tmax_arr, coords=[df_id, date_list], \n",
    "                     dims=[space_dim, time_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_HDC_G0</th>\n",
       "      <th>dates</th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmax_tntensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3091</td>\n",
       "      <td>[1983.05.16]</td>\n",
       "      <td>[42.49354861557129]</td>\n",
       "      <td>[1.8935486155712908]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_HDC_G0         dates                 tmax        tmax_tntensity\n",
       "0       3091  [1983.05.16]  [42.49354861557129]  [1.8935486155712908]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1983_406 = tmax_days(tmax_xr_da, 40.6)\n",
    "df1983_406.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1983_406_STATS = tmax_stats(df1983_406)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID_HDC_G0          int64\n",
       "year              object\n",
       "duration           int64\n",
       "avg_temp         float64\n",
       "avg_intensity    float64\n",
       "tot_intensity    float64\n",
       "event_dates       object\n",
       "intensity         object\n",
       "tmax              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1983_406_STATS.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n",
      "<class 'str'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for item in df1983_406_STATS.iloc[0,:].values:\n",
    "    print(type(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try json for 2 day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_nm = DATA_PATH+'interim/jsontest.json'\n",
    "df1983_406_STATS.to_json(fn_nm, orient = 'split', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(fn_nm) as json_data:\n",
    "    data = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(fn_nm, orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['event_dates'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_two(df, fn_out):\n",
    "    \n",
    "    \"find two day events buddy\"\n",
    "    print(len(df))\n",
    "    df_out = df[df['duration'] > 1]\n",
    "    print(len(df_out))\n",
    "    #df_out.to_json(fn_out, orient = 'split', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157540\n",
      "102552\n"
     ]
    }
   ],
   "source": [
    "fn_out = DATA_PATH+'interim/jsontest2day.json'\n",
    "find_two(df, fn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GHS-ERA5-Stats_1983-2d406.json\n",
      "157540\n",
      "102552\n",
      "GHS-ERA5-Stats_1984-2d406.json\n",
      "167920\n",
      "109274\n",
      "GHS-ERA5-Stats_1985-2d406.json\n",
      "163842\n",
      "105745\n",
      "GHS-ERA5-Stats_1986-2d406.json\n",
      "165723\n",
      "107579\n",
      "GHS-ERA5-Stats_1987-2d406.json\n",
      "164761\n",
      "108021\n",
      "GHS-ERA5-Stats_1988-2d406.json\n",
      "171581\n",
      "112544\n",
      "GHS-ERA5-Stats_1989-2d406.json\n",
      "161433\n",
      "104770\n",
      "GHS-ERA5-Stats_1990-2d406.json\n",
      "173922\n",
      "114781\n",
      "GHS-ERA5-Stats_1991-2d406.json\n",
      "162674\n",
      "104662\n",
      "GHS-ERA5-Stats_1992-2d406.json\n",
      "157817\n",
      "103797\n",
      "GHS-ERA5-Stats_1993-2d406.json\n",
      "165024\n",
      "107132\n",
      "GHS-ERA5-Stats_1994-2d406.json\n",
      "174729\n",
      "114889\n",
      "GHS-ERA5-Stats_1995-2d406.json\n",
      "169289\n",
      "110326\n",
      "GHS-ERA5-Stats_1996-2d406.json\n",
      "167882\n",
      "110057\n",
      "GHS-ERA5-Stats_1997-2d406.json\n",
      "171199\n",
      "111914\n",
      "GHS-ERA5-Stats_1998-2d406.json\n",
      "171928\n",
      "114703\n",
      "GHS-ERA5-Stats_1999-2d406.json\n",
      "177208\n",
      "116065\n",
      "GHS-ERA5-Stats_2000-2d406.json\n",
      "164259\n",
      "109016\n",
      "GHS-ERA5-Stats_2001-2d406.json\n",
      "164007\n",
      "108755\n",
      "GHS-ERA5-Stats_2002-2d406.json\n",
      "168388\n",
      "110843\n",
      "GHS-ERA5-Stats_2003-2d406.json\n",
      "173438\n",
      "114242\n",
      "GHS-ERA5-Stats_2004-2d406.json\n",
      "166382\n",
      "110353\n",
      "GHS-ERA5-Stats_2005-2d406.json\n",
      "170251\n",
      "114766\n",
      "GHS-ERA5-Stats_2006-2d406.json\n",
      "165348\n",
      "110655\n",
      "GHS-ERA5-Stats_2007-2d406.json\n",
      "172395\n",
      "114743\n",
      "GHS-ERA5-Stats_2008-2d406.json\n",
      "174385\n",
      "113832\n",
      "GHS-ERA5-Stats_2009-2d406.json\n",
      "171468\n",
      "114842\n",
      "GHS-ERA5-Stats_2010-2d406.json\n",
      "176089\n",
      "119888\n",
      "GHS-ERA5-Stats_2011-2d406.json\n",
      "171668\n",
      "115825\n",
      "GHS-ERA5-Stats_2012-2d406.json\n",
      "170840\n",
      "114109\n",
      "GHS-ERA5-Stats_2013-2d406.json\n",
      "178242\n",
      "119090\n",
      "GHS-ERA5-Stats_2014-2d406.json\n",
      "165131\n",
      "109573\n",
      "GHS-ERA5-Stats_2015-2d406.json\n",
      "170990\n",
      "113837\n",
      "GHS-ERA5-Stats_2016-2d406.json\n",
      "178723\n",
      "120682\n"
     ]
    }
   ],
   "source": [
    "# as a loop\n",
    "fns = sorted(glob(DATA_PATH+'interim/ERA5_STATS/*csv'))\n",
    "\n",
    "\n",
    "for fn in fns:\n",
    "    \n",
    "    fn_out = fn.split('STATS/')[1].split('.csv')[0]\n",
    "    fn_out = fn_out+'-2d406.json'\n",
    "    print(fn_out)\n",
    "    \n",
    "    df = pd.read_csv(fn)\n",
    "    print(len(df))\n",
    "    df_out = df[df['duration'] > 1]\n",
    "    print(len(df_out))\n",
    "#     df_out = find_two(fn, fn_out)\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok now find 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tmax_float(tmax):\n",
    "    tmaxfloat = float(tmax.split('[')[1].split(']')[0])\n",
    "    return tmaxfloat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(value):\n",
    "    out = value\n",
    "    out = float(out.split('[')[1].split(']')[0])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "(\"could not convert string to float: '41.90553678 40.79086727'\", 'occurred at index 24')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-1972745cc7f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# facts['pop2050'] = facts.apply(lambda row: final_pop(row['population'],row['population_growth']),axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tmax'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/geo/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6911\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6912\u001b[0m         )\n\u001b[0;32m-> 6913\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6915\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/geo/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/geo/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;31m# compute the result using the series generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/geo/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-1972745cc7f6>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# facts['pop2050'] = facts.apply(lambda row: final_pop(row['population'],row['population_growth']),axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tmax'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-118-989bb3dc7d35>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'['\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m']'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: (\"could not convert string to float: '41.90553678 40.79086727'\", 'occurred at index 24')"
     ]
    }
   ],
   "source": [
    "# facts['pop2050'] = facts.apply(lambda row: final_pop(row['population'],row['population_growth']),axis=1)\n",
    "df.apply(lambda row: test(row['tmax']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('factbook.db')\n",
    "query = \"select * from facts where area_land =0;\"\n",
    "facts = pd.read_sql_query(query,conn)\n",
    "print(list(facts.columns.values))\n",
    "\n",
    "def final_pop(initial_pop,growth_rate):\n",
    "    final = initial_pop*math.e**(growth_rate*35)\n",
    "    return(final)\n",
    "\n",
    "facts['pop2050'] = facts['population','population_growth'].apply(final_pop,axis=1)\n",
    "facts['pop2050'] = facts.apply(lambda row: final_pop(row['population'],row['population_growth']),axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
