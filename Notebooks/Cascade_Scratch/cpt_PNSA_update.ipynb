{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to explore updating PNAS data\n",
    "- rewrite code to make json not csv <br>\n",
    "- what are we dropping city year combos in pdays routine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from random import random\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import geopandas as gpd \n",
    "from glob import glob\n",
    "from statistics import mean\n",
    "import julian\n",
    "import time \n",
    "import multiprocessing as mp \n",
    "from multiprocessing import Pool\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "DATA_PATH = '/home/cascade/projects/UrbanHeat/data/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's write a script to make meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_in = DATA_PATH+'interim/GHS-UCDB-IDS.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fn_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_HDC_G0</th>\n",
       "      <th>CTR_MN_NM</th>\n",
       "      <th>UC_NM_MN</th>\n",
       "      <th>GCPNT_LAT</th>\n",
       "      <th>GCPNT_LON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5782</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Norilsk [RUS]</td>\n",
       "      <td>69.333682</td>\n",
       "      <td>88.205172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3316</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Murmansk [RUS]</td>\n",
       "      <td>68.955354</td>\n",
       "      <td>33.078645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5645</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Novy Urengoy [RUS]</td>\n",
       "      <td>66.083799</td>\n",
       "      <td>76.646580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3185</td>\n",
       "      <td>Finland</td>\n",
       "      <td>Oulu [FIN]</td>\n",
       "      <td>65.019378</td>\n",
       "      <td>25.482396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3539</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Severodvinsk [RUS]</td>\n",
       "      <td>64.572142</td>\n",
       "      <td>39.831477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_HDC_G0 CTR_MN_NM            UC_NM_MN  GCPNT_LAT  GCPNT_LON\n",
       "0       5782    Russia       Norilsk [RUS]  69.333682  88.205172\n",
       "1       3316    Russia      Murmansk [RUS]  68.955354  33.078645\n",
       "2       5645    Russia  Novy Urengoy [RUS]  66.083799  76.646580\n",
       "3       3185   Finland          Oulu [FIN]  65.019378  25.482396\n",
       "4       3539    Russia  Severodvinsk [RUS]  64.572142  39.831477"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghs_fn = DATA_PATH+'raw/GHS_UCDB/GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_0.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghs = gpd.read_file(ghs_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID_HDC_G0\n",
      "QA2_1V\n",
      "AREA\n",
      "BBX_LATMN\n",
      "BBX_LONMN\n",
      "BBX_LATMX\n",
      "BBX_LONMX\n",
      "GCPNT_LAT\n",
      "GCPNT_LON\n",
      "CTR_MN_NM\n",
      "CTR_MN_ISO\n",
      "XBRDR\n",
      "XCTR_NBR\n",
      "XC_NM_LST\n",
      "XC_ISO_LST\n",
      "GRGN_L1\n",
      "GRGN_L2\n",
      "UC_NM_MN\n",
      "UC_NM_LST\n",
      "UC_NM_SRC\n",
      "H75_NBR\n",
      "H90_NBR\n",
      "H00_NBR\n",
      "H75_AREA\n",
      "H90_AREA\n",
      "H00_AREA\n",
      "E_BM_NM_LS\n",
      "E_SL_LST\n",
      "EL_AV_ALS\n",
      "E_KG_NM_LS\n",
      "E_RB_NM_LS\n",
      "E_WR_P_90\n",
      "E_WR_P_00\n",
      "E_WR_P_14\n",
      "E_WR_T_90\n",
      "E_WR_T_00\n",
      "E_WR_T_14\n",
      "B75\n",
      "B90\n",
      "B00\n",
      "B15\n",
      "P75\n",
      "P90\n",
      "P00\n",
      "P15\n",
      "BUCAP75\n",
      "BUCAP90\n",
      "BUCAP00\n",
      "BUCAP15\n",
      "NTL_AV\n",
      "GDP90_SM\n",
      "GDP00_SM\n",
      "GDP15_SM\n",
      "INCM_CMI\n",
      "DEV_CMI\n",
      "TT2CC\n",
      "E_GR_AV90\n",
      "E_GR_AV00\n",
      "E_GR_AV14\n",
      "E_GR_AH90\n",
      "E_GR_AM90\n",
      "E_GR_AL90\n",
      "E_GR_AT90\n",
      "E_GR_AH00\n",
      "E_GR_AM00\n",
      "E_GR_AL00\n",
      "E_GR_AT00\n",
      "E_GR_AH14\n",
      "E_GR_AM14\n",
      "E_GR_AL14\n",
      "E_GR_AT14\n",
      "E_EC2E_E75\n",
      "E_EC2E_E90\n",
      "E_EC2E_E00\n",
      "E_EC2E_E12\n",
      "E_EC2E_R75\n",
      "E_EC2E_R90\n",
      "E_EC2E_R00\n",
      "E_EC2E_R12\n",
      "E_EC2E_I75\n",
      "E_EC2E_I90\n",
      "E_EC2E_I00\n",
      "E_EC2E_I12\n",
      "E_EC2E_T75\n",
      "E_EC2E_T90\n",
      "E_EC2E_T00\n",
      "E_EC2E_T12\n",
      "E_EC2E_A75\n",
      "E_EC2E_A90\n",
      "E_EC2E_A00\n",
      "E_EC2E_A12\n",
      "E_EC2O_E75\n",
      "E_EC2O_E90\n",
      "E_EC2O_E00\n",
      "E_EC2O_E12\n",
      "E_EC2O_R75\n",
      "E_EC2O_R90\n",
      "E_EC2O_R00\n",
      "E_EC2O_R12\n",
      "E_EC2O_I75\n",
      "E_EC2O_I90\n",
      "E_EC2O_I00\n",
      "E_EC2O_I12\n",
      "E_EC2O_T75\n",
      "E_EC2O_T90\n",
      "E_EC2O_T00\n",
      "E_EC2O_T12\n",
      "E_EC2O_A75\n",
      "E_EC2O_A90\n",
      "E_EC2O_A00\n",
      "E_EC2O_A12\n",
      "E_EPM2_E75\n",
      "E_EPM2_E90\n",
      "E_EPM2_E00\n",
      "E_EPM2_E12\n",
      "E_EPM2_R75\n",
      "E_EPM2_R90\n",
      "E_EPM2_R00\n",
      "E_EPM2_R12\n",
      "E_EPM2_I75\n",
      "E_EPM2_I90\n",
      "E_EPM2_I00\n",
      "E_EPM2_I12\n",
      "E_EPM2_T75\n",
      "E_EPM2_T90\n",
      "E_EPM2_T00\n",
      "E_EPM2_T12\n",
      "E_EPM2_A75\n",
      "E_EPM2_A90\n",
      "E_EPM2_A00\n",
      "E_EPM2_A12\n",
      "E_CPM2_T00\n",
      "E_CPM2_T05\n",
      "E_CPM2_T10\n",
      "E_CPM2_T14\n",
      "EX_FD_AREA\n",
      "EX_FD_B75\n",
      "EX_FD_B90\n",
      "EX_FD_B00\n",
      "EX_FD_B15\n",
      "EX_FD_P75\n",
      "EX_FD_P90\n",
      "EX_FD_P00\n",
      "EX_FD_P15\n",
      "EX_SS_AREA\n",
      "EX_SS_B75\n",
      "EX_SS_B90\n",
      "EX_SS_B00\n",
      "EX_SS_B15\n",
      "EX_SS_P75\n",
      "EX_SS_P90\n",
      "EX_SS_P00\n",
      "EX_SS_P15\n",
      "EX_EQ19PGA\n",
      "EX_EQ19MMI\n",
      "EX_EQ19_Q\n",
      "EX_HW_IDX\n",
      "SDG_LUE901\n",
      "SDG_A2G14\n",
      "SDG_OS15MX\n",
      "geometry\n"
     ]
    }
   ],
   "source": [
    "for col in ghs.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns we want \n",
    "cols = ['ID_HDC_G0','CTR_MN_NM', 'UC_NM_MN','GCPNT_LAT','GCPNT_LON']\n",
    "out = ghs[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out \n",
    "fn_out = DATA_PATH+'interim/GHS-UCDB-IDS.csv'\n",
    "out.to_csv(fn_out, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afghanistan\n",
      "Albania\n",
      "Algeria\n",
      "Angola\n",
      "Argentina\n",
      "Armenia\n",
      "Australia\n",
      "Austria\n",
      "Azerbaijan\n",
      "Bahamas\n",
      "Bahrain\n",
      "Bangladesh\n",
      "Barbados\n",
      "Belarus\n",
      "Belgium\n",
      "Belize\n",
      "Benin\n",
      "Bhutan\n",
      "Bolivia\n",
      "Bosnia and Herzegovina\n",
      "Botswana\n",
      "Brazil\n",
      "Brunei\n",
      "Bulgaria\n",
      "Burkina Faso\n",
      "Burundi\n",
      "Cambodia\n",
      "Cameroon\n",
      "Canada\n",
      "Cape Verde\n",
      "Central African Republic\n",
      "Chad\n",
      "Chile\n",
      "China\n",
      "Colombia\n",
      "Comoros\n",
      "Costa Rica\n",
      "Croatia\n",
      "Cuba\n",
      "Curaçao\n",
      "Cyprus\n",
      "Czech Republic\n",
      "Côte d'Ivoire\n",
      "Democratic Republic of the Congo\n",
      "Denmark\n",
      "Djibouti\n",
      "Dominican Republic\n",
      "Ecuador\n",
      "Egypt\n",
      "El Salvador\n",
      "Equatorial Guinea\n",
      "Eritrea\n",
      "Estonia\n",
      "Ethiopia\n",
      "Fiji\n",
      "Finland\n",
      "France\n",
      "French Polynesia\n",
      "Gabon\n",
      "Gambia\n",
      "Georgia\n",
      "Germany\n",
      "Ghana\n",
      "Greece\n",
      "Guatemala\n",
      "Guinea\n",
      "Guinea-Bissau\n",
      "Guyana\n",
      "Haiti\n",
      "Honduras\n",
      "Hungary\n",
      "Iceland\n",
      "India\n",
      "Indonesia\n",
      "Iran\n",
      "Iraq\n",
      "Ireland\n",
      "Israel\n",
      "Italy\n",
      "Jamaica\n",
      "Japan\n",
      "Jersey\n",
      "Jordan\n",
      "Kazakhstan\n",
      "Kenya\n",
      "Kosovo\n",
      "Kuwait\n",
      "Kyrgyzstan\n",
      "Laos\n",
      "Latvia\n",
      "Lebanon\n",
      "Lesotho\n",
      "Liberia\n",
      "Libya\n",
      "Lithuania\n",
      "Luxembourg\n",
      "Macedonia\n",
      "Madagascar\n",
      "Malawi\n",
      "Malaysia\n",
      "Maldives\n",
      "Mali\n",
      "Malta\n",
      "Mauritania\n",
      "Mauritius\n",
      "Mexico\n",
      "Moldova\n",
      "Mongolia\n",
      "Montenegro\n",
      "Morocco\n",
      "Mozambique\n",
      "Myanmar\n",
      "Namibia\n",
      "Nepal\n",
      "Netherlands\n",
      "New Caledonia\n",
      "New Zealand\n",
      "Nicaragua\n",
      "Niger\n",
      "Nigeria\n",
      "North Korea\n",
      "Norway\n",
      "Oman\n",
      "Pakistan\n",
      "Palestina\n",
      "Panama\n",
      "Papua New Guinea\n",
      "Paraguay\n",
      "Peru\n",
      "Philippines\n",
      "Poland\n",
      "Portugal\n",
      "Puerto Rico\n",
      "Qatar\n",
      "Republic of Congo\n",
      "Romania\n",
      "Russia\n",
      "Rwanda\n",
      "Saudi Arabia\n",
      "Senegal\n",
      "Serbia\n",
      "Sierra Leone\n",
      "Singapore\n",
      "Slovakia\n",
      "Slovenia\n",
      "Solomon Islands\n",
      "Somalia\n",
      "South Africa\n",
      "South Korea\n",
      "South Sudan\n",
      "Spain\n",
      "Sri Lanka\n",
      "Sudan\n",
      "Suriname\n",
      "Swaziland\n",
      "Sweden\n",
      "Switzerland\n",
      "Syria\n",
      "São Tomé and Príncipe\n",
      "Taiwan\n",
      "Tajikistan\n",
      "Tanzania\n",
      "Thailand\n",
      "Timor-Leste\n",
      "Togo\n",
      "Trinidad and Tobago\n",
      "Tunisia\n",
      "Turkey\n",
      "Turkmenistan\n",
      "Uganda\n",
      "Ukraine\n",
      "United Arab Emirates\n",
      "United Kingdom\n",
      "United States\n",
      "Uruguay\n",
      "Uzbekistan\n",
      "Venezuela\n",
      "Vietnam\n",
      "Western Sahara\n",
      "Yemen\n",
      "Zambia\n",
      "Zimbabwe\n"
     ]
    }
   ],
   "source": [
    "for ctn in np.unique(out.CTR_MN_NM):\n",
    "    print(ctn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_fn = DATA_PATH+'raw/countrylist.csv'\n",
    "regions = pd.read_csv(regions_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>alpha-2</th>\n",
       "      <th>alpha-3</th>\n",
       "      <th>country-code</th>\n",
       "      <th>iso_3166-2</th>\n",
       "      <th>region</th>\n",
       "      <th>sub-region</th>\n",
       "      <th>intermediate-region</th>\n",
       "      <th>region-code</th>\n",
       "      <th>sub-region-code</th>\n",
       "      <th>intermediate-region-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>4.0</td>\n",
       "      <td>ISO 3166-2:AF</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Southern Asia</td>\n",
       "      <td>Southern Asia</td>\n",
       "      <td>142.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Åland Islands</td>\n",
       "      <td>AX</td>\n",
       "      <td>ALA</td>\n",
       "      <td>248.0</td>\n",
       "      <td>ISO 3166-2:AX</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Northern Europe</td>\n",
       "      <td>Northern Europe</td>\n",
       "      <td>150.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Albania</td>\n",
       "      <td>AL</td>\n",
       "      <td>ALB</td>\n",
       "      <td>8.0</td>\n",
       "      <td>ISO 3166-2:AL</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Southern Europe</td>\n",
       "      <td>Southern Europe</td>\n",
       "      <td>150.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZ</td>\n",
       "      <td>DZA</td>\n",
       "      <td>12.0</td>\n",
       "      <td>ISO 3166-2:DZ</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Northern Africa</td>\n",
       "      <td>Northern Africa</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>American Samoa</td>\n",
       "      <td>AS</td>\n",
       "      <td>ASM</td>\n",
       "      <td>16.0</td>\n",
       "      <td>ISO 3166-2:AS</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>Polynesia</td>\n",
       "      <td>Polynesia</td>\n",
       "      <td>9.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name alpha-2 alpha-3  country-code     iso_3166-2   region  \\\n",
       "0     Afghanistan      AF     AFG           4.0  ISO 3166-2:AF     Asia   \n",
       "1   Åland Islands      AX     ALA         248.0  ISO 3166-2:AX   Europe   \n",
       "2         Albania      AL     ALB           8.0  ISO 3166-2:AL   Europe   \n",
       "3         Algeria      DZ     DZA          12.0  ISO 3166-2:DZ   Africa   \n",
       "4  American Samoa      AS     ASM          16.0  ISO 3166-2:AS  Oceania   \n",
       "\n",
       "        sub-region intermediate-region  region-code  sub-region-code  \\\n",
       "0    Southern Asia       Southern Asia        142.0             34.0   \n",
       "1  Northern Europe     Northern Europe        150.0            154.0   \n",
       "2  Southern Europe     Southern Europe        150.0             39.0   \n",
       "3  Northern Africa     Northern Africa          2.0             15.0   \n",
       "4        Polynesia           Polynesia          9.0             61.0   \n",
       "\n",
       "   intermediate-region-code  \n",
       "0                       NaN  \n",
       "1                       NaN  \n",
       "2                       NaN  \n",
       "3                       NaN  \n",
       "4                       NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CTR_MN_NM</th>\n",
       "      <th>region</th>\n",
       "      <th>sub-region</th>\n",
       "      <th>intermediate-region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Southern Asia</td>\n",
       "      <td>Southern Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Åland Islands</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Northern Europe</td>\n",
       "      <td>Northern Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Albania</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Southern Europe</td>\n",
       "      <td>Southern Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Northern Africa</td>\n",
       "      <td>Northern Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>American Samoa</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>Polynesia</td>\n",
       "      <td>Polynesia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Western Asia</td>\n",
       "      <td>Western Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>Zambia</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>Eastern Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>Eastern Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>Swaziland</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>Southern Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>Kosovo</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Southern Europe</td>\n",
       "      <td>Southern Europe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CTR_MN_NM   region          sub-region intermediate-region\n",
       "0       Afghanistan     Asia       Southern Asia       Southern Asia\n",
       "1     Åland Islands   Europe     Northern Europe     Northern Europe\n",
       "2           Albania   Europe     Southern Europe     Southern Europe\n",
       "3           Algeria   Africa     Northern Africa     Northern Africa\n",
       "4    American Samoa  Oceania           Polynesia           Polynesia\n",
       "..              ...      ...                 ...                 ...\n",
       "246           Yemen     Asia        Western Asia        Western Asia\n",
       "247          Zambia   Africa  Sub-Saharan Africa      Eastern Africa\n",
       "248        Zimbabwe   Africa  Sub-Saharan Africa      Eastern Africa\n",
       "249       Swaziland   Africa  Sub-Saharan Africa     Southern Africa\n",
       "250          Kosovo   Europe     Southern Europe     Southern Europe\n",
       "\n",
       "[251 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions_fn = DATA_PATH+'raw/countrylist.csv'\n",
    "regions = pd.read_csv(regions_fn)\n",
    "\n",
    "cols = ['name','region','sub-region','intermediate-region']\n",
    "regions = regions[cols]\n",
    "regions.rename(columns={'name': 'CTR_MN_NM'}, inplace = True)\n",
    "regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore files - 1983\n",
    "content = os.listdir(DATA_PATH+'interim/')\n",
    "fns = sorted(glob(DATA_PATH+'interim/ERA5_HI/*csv'))\n",
    "fn = fns[0]\n",
    "df1983 = pd.read_csv(fn)\n",
    "df1983.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1983.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check out the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 1 - Function Loads all Tmax Data as an X-array\n",
    "def read_data(dir_path, space_dim, time_dim):\n",
    "    \"\"\" Function reads in all Tmax .csv files, joins them by date along the x-axis\n",
    "    and returns the whole record as a x-array data array\n",
    "    \n",
    "    Args:   \n",
    "        dir_path = path to .csv files \n",
    "        time_dim = name for time dim as a str ... use date :-)\n",
    "        space_dim = col name for GHS-UCDB IDs as an str (ID_HDC_G0)\n",
    "    \"\"\"\n",
    "    fn_list = sorted(glob.glob(dir_path+'*.csv'))\n",
    "    df_out = pd.DataFrame()\n",
    "    date_list = []\n",
    "\n",
    "    # Open all Tmax files and concat into a df\n",
    "    for i, fn in enumerate(fn_list):    \n",
    "        # Open the CSV\n",
    "        df = pd.read_csv(fn)\n",
    "\n",
    "        # Get the city ids \n",
    "        if i == 1:\n",
    "            df_id = df[space_dim]\n",
    "\n",
    "        # get only the Tmax columns and concate date list \n",
    "        df_temp = df.iloc[:,3:] # get only temp columns\n",
    "        date_list = date_list+list(df_temp.columns)\n",
    "\n",
    "        # Drop cities w/ no temp record \n",
    "        df_temp_drop = df_temp.dropna()\n",
    "\n",
    "        # Merge\n",
    "        df_out = pd.concat([df_out, df_temp_drop], axis=1)\n",
    "        print(df_out.shape)\n",
    "    \n",
    "    # make date into an array\n",
    "    tmax_arr = df_out.to_numpy()\n",
    "\n",
    "    # Make data into an xr.DataArray\n",
    "    tmax_xr_da = xr.DataArray(tmax_arr, coords=[df_id, date_list], \n",
    "                             dims=[space_dim, time_dim])\n",
    "    return tmax_xr_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmax_days(xarray, Tthresh):\n",
    "    \"\"\" Function finds all the tmax days in a year and sums total days per year \n",
    "    greater than a threshold within a year where Tmax > Tthresh for each city. Returns the total number of days,\n",
    "    the dates, the tempatures, and the intensity (daily Tmax - Tthresh)\n",
    "    \n",
    "    Args: \n",
    "        xarray = an xarray object with dims = (space, times)\n",
    "        Tthresh = int of temp threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty lists & df\n",
    "    id_list = []\n",
    "    date_list = []\n",
    "    tmax_list = []\n",
    "    intensity_list = []\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    # subset xarray\n",
    "    out = xarray.where(xarray > Tthresh, drop = True)\n",
    "\n",
    "    # start loop \n",
    "    for index, loc in enumerate(out.ID_HDC_G0):\n",
    "        id_list.append(out.ID_HDC_G0.values[index]) # get IDS\n",
    "        date_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values) # get event dates\n",
    "        \n",
    "        # #CPT 2020.02.23 \n",
    "        # dayTot_list.append(len(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values)) # get event totals\n",
    "        \n",
    "        tmax_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').values) # get temp values\n",
    "        intensity_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').values - Tthresh) # get severity\n",
    "\n",
    "    # write to a data frame\n",
    "    df_out['ID_HDC_G0'] = id_list\n",
    "    # df_out['total_days'] = dayTot_list #CPT 2020.02.23\n",
    "    df_out['dates'] = date_list\n",
    "    df_out['tmax'] = tmax_list\n",
    "    df_out['tmax_tntensity'] = intensity_list\n",
    "\n",
    "    # return df_out\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_split(dates, ID_HDC_G0, intensity, tmax): #, total_days): #CPT 2020.02.23\n",
    "    \n",
    "    \"\"\" Searchs a list of dates and isolates sequential dates as a list, then calculates event stats.\n",
    "    See comments in code for more details. \n",
    "    \n",
    "    Args:\n",
    "        dates: pandas.core.index as julian dates\n",
    "        ID_HDC_G0: city ID as string\n",
    "        intensity: numpy.ndarray of intensities values\n",
    "        tmax: numpy.ndarray of intensities values of tmax values\n",
    "        total_days: total number of tmax days in a year for a given city\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # city id\n",
    "    city_id = ID_HDC_G0\n",
    "    # tot_days = total_days #CPT 2020.02.23\n",
    "    \n",
    "    # lists to fill\n",
    "    city_id_list = []\n",
    "    # tot_days_list = [] #CPT 2020.02.23\n",
    "    event_dates_list = []\n",
    "    dur_list = []\n",
    "    intensity_list = []\n",
    "    tmax_list = []\n",
    "    avg_temp_list = []\n",
    "    avg_int_list = []\n",
    "    tot_int_list = []\n",
    "    year_list = []\n",
    "    \n",
    "    # data frame out\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    # turn days into julian days\n",
    "    jul_days = jul_convert(dates)\n",
    "    \n",
    "    # Counters to make sure we write the correct event dates to a list, don't want julian days in output\n",
    "    counter = 0\n",
    "    start = 0\n",
    "    end = 0\n",
    "    \n",
    "    # Loop through dur list and isolate seq days, temps, and intensities\n",
    "    for k, g in groupby(enumerate(jul_days.values), lambda x: x[1]-x[0]):\n",
    "        \n",
    "        seq = list(map(itemgetter(1), g)) # isolate seq. days\n",
    "        dur = len(seq) # duration of each event\n",
    "        \n",
    "        counter = counter + dur # add duration to counter\n",
    "        end = counter # end of current event\n",
    "        \n",
    "        event_dates = dates[start:end] # dates of tmax days during each event\n",
    "        intense = intensity[start:end] # intensity of each day during event\n",
    "        temp = tmax[start:end] # temp of each day during event\n",
    "        avg_temp = mean(temp) # avg. temp during event\n",
    "        avg_int = mean(intense) # avg. intensity during event\n",
    "        tot_int = intense.sum() # total intensity during event\n",
    "        \n",
    "        start = counter # reset start to current end (e.g. counter)\n",
    "        year = event_dates[0].split('.')[0]\n",
    "        \n",
    "        # fill lists\n",
    "        city_id_list.append(city_id)\n",
    "        year_list.append(year)\n",
    "        # tot_days_list.append(tot_days) #CPT 2020.02.23\n",
    "        dur_list.append(dur)\n",
    "        event_dates_list.append(event_dates)\n",
    "        intensity_list.append(intense)\n",
    "        tmax_list.append(temp)\n",
    "        avg_temp_list.append(avg_temp)\n",
    "        avg_int_list.append(avg_int)\n",
    "        tot_int_list.append(tot_int)\n",
    "\n",
    "    # write out as a dateframe\n",
    "    df_out['ID_HDC_G0'] = city_id_list\n",
    "    df_out['year'] = year_list\n",
    "    # df_out['total_days'] = tot_days_list #CPT 2020.02.23\n",
    "    df_out['duration'] = dur_list\n",
    "    df_out['avg_temp'] = avg_temp_list\n",
    "    df_out['avg_intensity'] = avg_int_list\n",
    "    df_out['tot_intensity'] = tot_int_list\n",
    "    df_out['event_dates'] = event_dates_list\n",
    "    df_out['duration'] = dur_list\n",
    "    df_out['intensity'] = intensity_list\n",
    "    df_out['tmax'] = tmax_list\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 4 function feeds output from function 3 into function 4\n",
    "def tmax_stats(df_in):\n",
    "    \"\"\" runs event_split functionon a dataframe to produce desired tmax stats\n",
    "\n",
    "        NOTE - If you add arguments to event_split to make more states,\n",
    "        be sure to update this function\n",
    "\n",
    "        args:\n",
    "            df: input dataframe\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "\n",
    "    # NOTE - If you add arguments to event_split to make more stats,\n",
    "    # be sure to update this function\n",
    "\n",
    "    for index, row in df_in.iterrows():\n",
    "        dates = row['dates'] # Get event dates\n",
    "        intensity = row['tmax_tntensity'] # Get intensity for each day\n",
    "        tmax = row['tmax'] # Get tmax for each day\n",
    "        ID_HDC_G0 = row['ID_HDC_G0'] # get city id\n",
    "        # total_days = row['total_days'] # get total number of tmax days -- CPT 2020.02.23\n",
    "\n",
    "        df = event_split(dates, ID_HDC_G0, intensity, tmax)# , total_days) #CPT 2020.02.23\n",
    "\n",
    "        df_out = df_out.append(df)\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jul_convert(dates):\n",
    "    \"Function turn days into julian datetime\"\n",
    "    jul_days = pd.to_datetime(dates).to_julian_date()\n",
    "    \n",
    "    return jul_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok let's see how this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data into an xr.DataArray\n",
    "df_id = list(df1983['ID_HDC_G0'])\n",
    "date_list = list(df1983.columns[3:])\n",
    "df_temp = df1983.iloc[:,3:] # get only temp columns\n",
    "df_temp_drop = df_temp.dropna() # drop cities with no temp record\n",
    "space_dim = 'ID_HDC_G0'\n",
    "time_dim = 'date'\n",
    "\n",
    "tmax_arr = df_temp_drop.to_numpy()\n",
    "\n",
    "tmax_xr_da = xr.DataArray(tmax_arr, coords=[df_id, date_list], \n",
    "                     dims=[space_dim, time_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1983_406 = tmax_days(tmax_xr_da, 40.6)\n",
    "df1983_406.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1983_406_STATS = tmax_stats(df1983_406)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1983_406_STATS.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df1983_406_STATS.iloc[0,:].values:\n",
    "    print(type(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try json for 2 day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_nm = DATA_PATH+'interim/jsontest.json'\n",
    "df1983_406_STATS.to_json(fn_nm, orient = 'split', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(fn_nm) as json_data:\n",
    "    data = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(fn_nm, orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['event_dates'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_two(df, fn_out):\n",
    "    \n",
    "    \"find two day events buddy\"\n",
    "    print(len(df))\n",
    "    df_out = df[df['duration'] > 1]\n",
    "    print(len(df_out))\n",
    "    #df_out.to_json(fn_out, orient = 'split', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_out = DATA_PATH+'interim/jsontest2day.json'\n",
    "find_two(df, fn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a loop\n",
    "fns = sorted(glob(DATA_PATH+'interim/ERA5_STATS/*csv'))\n",
    "\n",
    "\n",
    "for fn in fns:\n",
    "    \n",
    "    fn_out = fn.split('STATS/')[1].split('.csv')[0]\n",
    "    fn_out = fn_out+'-2d406.json'\n",
    "    print(fn_out)\n",
    "    \n",
    "    df = pd.read_csv(fn)\n",
    "    print(len(df))\n",
    "    df_out = df[df['duration'] > 1]\n",
    "    print(len(df_out))\n",
    "#     df_out = find_two(fn, fn_out)\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok now find 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tmax_float(tmax):\n",
    "    tmaxfloat = float(tmax.split('[')[1].split(']')[0])\n",
    "    return tmaxfloat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(value):\n",
    "    out = value\n",
    "    out = float(out.split('[')[1].split(']')[0])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facts['pop2050'] = facts.apply(lambda row: final_pop(row['population'],row['population_growth']),axis=1)\n",
    "df.apply(lambda row: test(row['tmax']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('factbook.db')\n",
    "query = \"select * from facts where area_land =0;\"\n",
    "facts = pd.read_sql_query(query,conn)\n",
    "print(list(facts.columns.values))\n",
    "\n",
    "def final_pop(initial_pop,growth_rate):\n",
    "    final = initial_pop*math.e**(growth_rate*35)\n",
    "    return(final)\n",
    "\n",
    "facts['pop2050'] = facts['population','population_growth'].apply(final_pop,axis=1)\n",
    "facts['pop2050'] = facts.apply(lambda row: final_pop(row['population'],row['population_growth']),axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
