{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHIRTMax Poly\n",
    "\n",
    "This is the cleaned version of cpt_UHI (see it for old code and more details). The goals are to build a routine to cal avg. temp for CHIRTMax for each city using the GHS-UCDB. I will compare these with the urban polygons I built for the ERL paper. \n",
    "\n",
    "by Cascade Tuholske 2019-08-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xarray \n",
    "\n",
    "import rasterio \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from rasterstats import zonal_stats\n",
    "from rasterio import features\n",
    "import os\n",
    "from ftplib import FTP\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths <---- For testing use the chirts in the pop raster \n",
    "DATA_IN_GHS = '/Users/cascade/Github/PopRaster/data/raw/JRC/ghs-ucdb/'\n",
    "DATA_IN_CHIRT = '/Users/cascade/Github/PopRaster/data/raw/CHIRT/'\n",
    "DATA_OUT = '/Users/cascade/Github/PopRaster/data/interim/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOOK AT CHIRTS RASTERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIRTSmax.2016.07.tif\n",
      "CHIRTSmax.1983.01.tif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import re\n",
    "\n",
    "for fn in os.listdir(DATA_IN_CHIRT):\n",
    "    # find all the tif files\n",
    "    if fn.endswith('.tif'):\n",
    "        print(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn Polygons into rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Names In\n",
    "shp_fn = 'GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_0.shp'\n",
    "rst_fn = 'CHIRTSmax.1983.01.tif'\n",
    "\n",
    "# File Names Out ---> made with 1983 CHIRT as TEMPLATE\n",
    "out_fn_ut = 'GHS_UCDB_Raster_untouched.tif'\n",
    "out_fn_t = 'GHS_UCDB_Raster_Raster_touched.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file with GeoPANDAS read_file\n",
    "polys = gpd.read_file(DATA_IN_GHS+shp_fn)\n",
    "rst = rasterio.open(DATA_IN_CHIRT+rst_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_to_raster (rst, polys, value, touched, out_fn, fill_value):\n",
    "    \"\"\"Function makes a raster from a list of polygons\n",
    "    \n",
    "    Args:   rst = input raster already read in as a rasterio object to act as a template\n",
    "            polys = input polygons already read in as a gpd dataframe\n",
    "            value = col with value to burn into raster\n",
    "            touched = bool, if True all pixels touched (not centers) are burned into raster\n",
    "            out_fn = out file name \n",
    "            fill_value = value to revalue input raster before burning in polygons \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    meta = rst.meta.copy() # copy meta data from rst\n",
    "    out_arr = rst.read(1) # get an array to burn shapes\n",
    "    out_arr.fill(fill_value) # revalue rst to an Nan Value before burning in polygons\n",
    "    \n",
    "    # extract geom and values to burn\n",
    "    shapes = ((geom,value) for geom, value in zip(polys['geometry'], polys[value])) \n",
    "    \n",
    "    # burn shapes intp an array\n",
    "    burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=rst.transform, all_touched=touched)\n",
    "    \n",
    "    # write our raster to disk\n",
    "    with rasterio.open(out_fn, 'w', **meta) as out:\n",
    "        out.write_band(1, burned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out function\n",
    "# Function works, though we need to deal with pixels that are used in more than one city\n",
    "\n",
    "out = DATA_OUT+out_fn_ut\n",
    "poly_to_raster(rst, polys, 'ID_HDC_G0', False, out, -9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Burn Results\n",
    "\n",
    "Function works, though we need to deal with pixels that overlap with more than once city\n",
    "Big difference between touched and untouched rasterized outputs\n",
    "Ask Chris and Kelly what they think\n",
    "\n",
    "<img src=\"../../Screenshots/GHS-UCDB-Raster-TouchVUn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Avg Temp w/ Xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a ChirtMax File & GHS Raster\n",
    "chirt_fn = 'CHIRTSmax.1983.01.tif'\n",
    "ghs_fn = 'GHS_UCDB_Raster_Raster_touched.tif' # <<<---- NOTE TOUCHED v UN_TOUCHED\n",
    "\n",
    "chirt = rasterio.open(DATA_IN_CHIRT+chirt_fn)\n",
    "ghs = rasterio.open(DATA_OUT+ghs_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check meta data\n",
    "print('chirt meta')\n",
    "print(chirt.meta)\n",
    "print('ghs meta')\n",
    "print(ghs.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get arrays\n",
    "\n",
    "chirt_arr = chirt.read(1)\n",
    "ghs_arr = ghs.read(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make arrays into xarray DataArray\n",
    "\n",
    "chirt_da = xr.DataArray(chirt_arr, dims = ['y', 'x']) # y and x are our 2-d labels\n",
    "ghs_da = xr.DataArray(ghs_arr, dims = ['y', 'x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make xarray dataset\n",
    "\n",
    "ds = xr.Dataset(data_vars = \n",
    "                    {'ghs' : (['y', 'x'], ghs_da),\n",
    "                    'chirt' : (['y', 'x'], chirt_da),})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask values from chirt that are ocean -9999\n",
    "\n",
    "ds_mask = ds.where(ds.chirt != -9999, drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask values that are not GHS polys\n",
    "\n",
    "ds_mask = ds_mask.where(ds_mask.ghs > 0, drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the CHIRT w/ the non-GHS pixels masked to a .tif file\n",
    "\n",
    "out_arr = np.array(ds_mask.chirt) # get masked chirt array\n",
    "meta = chirt.meta\n",
    "\n",
    "def raster_write(meta, array, file_out):\n",
    "    \"\"\" function to write out a raster file with an np array\n",
    "    requires meta data for raster, np array & file out path and name\n",
    "    \"\"\"\n",
    "    \n",
    "    kwargs = meta\n",
    "\n",
    "    # Update kwargs (change in data type)\n",
    "    kwargs.update(dtype=rasterio.float32, count = 1)\n",
    "\n",
    "    with rasterio.open(file_out, 'w', **kwargs) as dst:\n",
    "        dst.write_band(1, array.astype(rasterio.float32))\n",
    "\n",
    "raster_write(meta, out_arr, DATA_OUT+'CHIRTSmax.1983.01_GHSMaskv4.tif') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gut check\n",
    "\n",
    "From Chirt_GHS_Mask_Test.tif, it looks like I am isolating the correct pixels in the chirt dataset to exlcude ocean and include the GHS cities.\n",
    "\n",
    "Recall this first group is using the 'touched' raster\n",
    "\n",
    "Update 2019-08-19 **BE SURE TO FIRST MASK -9999 from the CHIRT raster, then from the GHS RASTER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mean CHIRT max for each GHS ID\n",
    "\n",
    "avg = ds_mask.groupby('ghs').mean(xr.ALL_DIMS) # <--------------- double check this is the correct notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cords are the GHS side here, data variable are the chirt avg, type is a xarray dataset\n",
    "\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn GHS IDS and avg. CHIRTMax values into 1-D numpy arrays of equal length\n",
    "\n",
    "avg_ID = np.array(avg.ghs)\n",
    "avg_chirt = np.array(avg.chirt)\n",
    "\n",
    "print(len(avg_ID))\n",
    "print(len(avg_chirt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn chirt max and IDS into a DF\n",
    "\n",
    "df_avg = pd.DataFrame()\n",
    "df_avg['chirtMax'] = avg_chirt\n",
    "df_avg['ID_HDC_G0'] = avg_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge avg temp into back into orgional ghs-UCDB polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open org. ghs-ucdb polys\n",
    "shp_fn = 'GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_0.shp'\n",
    "ghs_polys = gpd.read_file(DATA_IN_GHS+shp_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that 68 GHS polys don't have a chirtsMAX value likely because as poly ids are burned in pixels can only take\n",
    "# on one value thus if a pixel contains more than one polygon only one value will be burned\n",
    "\n",
    "len(ghs_polys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate col needed from original ghs-ucdb polygons\n",
    "\n",
    "df_ghs = gpd.GeoDataFrame()\n",
    "df_ghs['geometry'] = ghs_polys.geometry\n",
    "df_ghs['ID_HDC_G0'] = ghs_polys.ID_HDC_G0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate col needed from original ghs-ucdb polygons\n",
    "\n",
    "df_avg = pd.DataFrame()\n",
    "df_avg['chirtMax'] = avg_chirt\n",
    "df_avg['ID_HDC_G0'] = avg_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the df\n",
    "\n",
    "df_merge = pd.merge(df_ghs, df_avg, on='ID_HDC_G0', how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the merged tmax polys and look in QGIS \n",
    "out_fn = 'GHS-CHIRTS-Poly-Test.shp'\n",
    "df_merge.to_file(DATA_OUT+out_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make polygons of dropped GHS w/o temp\n",
    "\n",
    "- I need to figure out why 68 polygons are not getting captured in the raster.\n",
    "- I think what happens is that when the GHS-ID values get burned into a pixel, if more than one GHS-UCDB poly is in a pixel, the first or second GHS-ID value gets burned in, but not both obv. because a pixel can only take on one value. \n",
    "- I am not sure what to solution is ...\n",
    "\n",
    "UPDATE 2019-08-19 Not to worry about this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df_avg.ID_HDC_G0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_drop = df_merge[np.isnan(df_merge.chirtMax)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_drop.to_file(DATA_OUT+'GHS-CHIRTS-Poly-Test-MissingTMax.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Step is to build this out as a full script\n",
    "\n",
    "Build this as a .py file ... eg a freakin' program. Write a damn computer program\n",
    "\n",
    "CHECK W/ 1983.01 POLYGONS have so few Avgs could be with the ds_mask > 0 and missing val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open files before loop\n",
    "\n",
    "# Directories \n",
    "CHIRT_DIR = '/Users/cascade/Github/PopRaster/data/raw/CHIRT/' # <<--- path to loop through\n",
    "SHP_DIR = '/Users/cascade/Github/PopRaster/data/raw/JRC/ghs-ucdb/'\n",
    "POLY_RST_DIR = '/Users/cascade/Github/PopRaster/data/interim/'\n",
    "DATA_OUT = '/Users/cascade/Github/PopRaster/data/interim/'\n",
    "\n",
    "# Open Polygon Raster\n",
    "polyRst_fn = 'GHS_UCDB_Raster_Raster_touched.tif'\n",
    "polyRst = rasterio.open(POLY_RST_DIR+polyRst_fn)\n",
    "\n",
    "# Open the file with GeoPANDAS read_file\n",
    "shp_fn = 'GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_0.shp'\n",
    "shps = gpd.read_file(SHP_DIR+shp_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isloate SHP Poly Col to merge back in later \n",
    "\n",
    "df_ghs = gpd.GeoDataFrame()\n",
    "df_ghs['geometry'] = shps.geometry\n",
    "df_ghs['ID_HDC_G0'] = shps.ID_HDC_G0\n",
    "df_ghs['CTR_MN_NM'] = shps.CTR_MN_NM\n",
    "df_ghs['P75'] = shps.P75\n",
    "df_ghs['P90'] = shps.P90\n",
    "df_ghs['P00'] = shps.P00\n",
    "df_ghs['P15'] = shps.P15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get polyRst data as Xarray, \n",
    "polyRst_da = xr.DataArray(polyRst.read(1), dims = ['y', 'x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = rasterio.open(CHIRT_DIR+'CHIRTSmax.2016.07.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import re\n",
    "\n",
    "# open urban polys\n",
    "\n",
    "# open urban poly raster \n",
    "\n",
    "df_merge = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOUBLE CHECK THIS DUDE AND WRITE OUT SOME RASTERS TO TEST IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import re\n",
    "\n",
    "# make a copy of the ghs polys\n",
    "df_merge = df_ghs.copy()\n",
    "\n",
    "for fn in os.listdir(CHIRT_DIR):\n",
    "    # find all the tif files\n",
    "    if fn.endswith('.tif'):\n",
    "        \n",
    "        # NEED TO BUILD META DATA CHECK INTO ROUTINE and throw an error<<<<---------\n",
    "\n",
    "        # Get the date of each chirt file\n",
    "        date = (fn.split('CHIRTSmax.')[1].split('.tif')[0])\n",
    "        print(date)\n",
    "        \n",
    "        # Open CHIRT Data and turn data into array\n",
    "        tempRst = rasterio.open(CHIRT_DIR+fn)\n",
    "        \n",
    "        # Make arrays into xarray DataArray\n",
    "        tempRst_da = xr.DataArray(tempRst.read(1), dims = ['y', 'x']) # y and x are our 2-d labels\n",
    "        \n",
    "        # Make xarray dataset\n",
    "        ds = xr.Dataset(data_vars = \n",
    "                    {'ghs' : (['y', 'x'], polyRst_da),\n",
    "                    'temp' : (['y', 'x'], tempRst_da),})\n",
    "        \n",
    "        # UPDATED 2019-08-19 Mask the CHIRTS PIXELS FIRST, THEN GHS\n",
    "        # Mask values from chirt that are ocean in ghs and chirt in our ds \n",
    "        ds_mask = ds.where(ds.chirt != -9999, drop = False) #<<<<------ need to double check this\n",
    "        \n",
    "        # Mask pixels for both ghs and chirts where ghs cities are not present\n",
    "        ds_mask = ds_mask.where(ds_mask.ghs > 0, drop = False)\n",
    "        \n",
    "        # Group poly_IDs find temp\n",
    "        avg = ds_mask.groupby('ghs').mean(xr.ALL_DIMS)\n",
    "        \n",
    "        # turn GHS IDS and avg. CHIRTMax values into 1-D numpy arrays of equal length\n",
    "        avg_ID = np.array(avg.ghs)\n",
    "        avg_temp = np.array(avg.temp)\n",
    "        \n",
    "        print(len(avg_ID))\n",
    "        print(len(avg_temp))\n",
    "        \n",
    "        ###### CHECK W/ 1983.01 POLYGONS have so few Avgs could be with the ds_mask! ! ! ! !\n",
    "        \n",
    "        # turn chirt max and IDS into a DF\n",
    "        df_avg = pd.DataFrame()\n",
    "        df_avg[date] = avg_temp\n",
    "        df_avg['ID_HDC_G0'] = avg_ID\n",
    "        \n",
    "        # merge the df\n",
    "        df_merge = df_merge.merge(df_avg, on='ID_HDC_G0', how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.to_file(DATA_OUT+'GHS-CHIRTS-Poly-Loop-Test.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1983 = df_merge[np.isnan(df_merge['1983.01'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1983.to_file(DATA_OUT+'GHS-CHIRTS-198701-Drop.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XARRAY TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = xr.DataArray([[0,0,0], [0,19,19,], [19,0,0]], dims=('x', 'y'))\n",
    "# b = xr.DataArray(np.ones(25).reshape(5, 5), dims=('x', 'y'))\n",
    "\n",
    "b = xr.DataArray([[0,0,0], [1,1,1,], [0,0,0]], dims=('x', 'y'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make xarray dataset\n",
    "\n",
    "ds = xr.Dataset(data_vars = \n",
    "                    {'a' : (['y', 'x'], a),\n",
    "                    'b' : (['y', 'x'], b),})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mask = ds.where(ds.a > 0, drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mask.b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
