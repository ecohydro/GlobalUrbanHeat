{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def my_func(x):\n",
    "  print(mp.current_process())\n",
    "  return x**x\n",
    "\n",
    "def main():\n",
    "  pool = mp.Pool(mp.cpu_count())\n",
    "  result = pool.map(my_func, [4,2,3,5,3,2,1,2])\n",
    "  result_set_2 = pool.map(my_func, [4,6,5,4,6,3,23,4,6])\n",
    "\n",
    "  print(result)\n",
    "  print(result_set_2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program finds the TMax from CHIRTSMax Data and a raster with polygon IDS burned\n",
    "# By Cascade Tuholske 2019-08-20\n",
    "\n",
    "# Dependencies\n",
    "import rasterio \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from rasterstats import zonal_stats\n",
    "from rasterio import features\n",
    "import os\n",
    "import xarray as xr\n",
    "import fnmatch\n",
    "import time\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL TEST \n",
    "CHIRT_DIR = '/Users/cascade/Github/UrbanHeat/data/test_in/' # <<--- path to loop through\n",
    "SHP_DIR = '/Users/cascade/Github/PopRaster/data/raw/JRC/ghs-ucdb/'\n",
    "POLY_RST_DIR = '/Users/cascade/Github/PopRaster/data/interim/'\n",
    "DATA_OUT = '/Users/cascade/Github/UrbanHeat/data/test_out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Polygon Raster\n",
    "polyRst_fn = 'GHS_UCDB_Raster_Raster_touched.tif'\n",
    "polyRst = rasterio.open(POLY_RST_DIR+polyRst_fn)\n",
    "\n",
    "# Open the file with GeoPANDAS read_file\n",
    "shp_fn = 'GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_0.shp'\n",
    "shps = gpd.read_file(SHP_DIR+shp_fn)\n",
    "\n",
    "# Set fn out, change as needed \n",
    "fn_out = 'GHS-CHIRT-MONTHLY'  \n",
    "\n",
    "# Isloate SHP Poly Col to merge back in later \n",
    "df_ghs = gpd.GeoDataFrame()\n",
    "\n",
    "df_ghs['ID_HDC_G0'] = shps.ID_HDC_G0\n",
    "df_ghs['CTR_MN_NM'] = shps.CTR_MN_NM\n",
    "\n",
    "# Turn polyRst data as Xarray, \n",
    "polyRst_da = xr.DataArray(polyRst.read(1), dims = ['y', 'x'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Loop through dirs \n",
    "for dirpath, dirnames, files in os.walk(CHIRT_DIR):\n",
    "        # Set dir name for writing files\n",
    "        dir_year = dirpath.split(CHIRT_DIR)[1]\n",
    "\n",
    "        # make a copy of the ghs polys, reset for each dir\n",
    "        df_merge = df_ghs.copy()\n",
    "        \n",
    "        for fn in files:\n",
    "\n",
    "                # find all the tif files\n",
    "                if fn.endswith('.tif'):\n",
    "                \n",
    "                        # NEED TO BUILD META DATA CHECK INTO ROUTINE and throw an error<<<<---------\n",
    "\n",
    "                        # Get the date of each chirt file\n",
    "                        date = (fn.split('CHIRTSmax.')[1].split('.tif')[0])\n",
    "                        print(dir_year)\n",
    "                        print(date)\n",
    "                        \n",
    "                        # Open CHIRT Data and turn data into array\n",
    "                        tempRst = rasterio.open(dirpath+'/'+fn)\n",
    "                        \n",
    "                        # Make arrays into x    array DataArray\n",
    "                        tempRst_da = xr.DataArray(tempRst.read(1), dims = ['y', 'x']) # y and x are our 2-d labels\n",
    "                        \n",
    "                        # Make xarray dataset\n",
    "                        ds = xr.Dataset(data_vars = \n",
    "                                {'ghs' : (['y', 'x'], polyRst_da),\n",
    "                                'temp' : (['y', 'x'], tempRst_da),})\n",
    "                        \n",
    "                        # UPDATED 2019-08-19 Mask the CHIRTS PIXELS FIRST, THEN GHS\n",
    "                        # Mask values from chirt that are ocean in ghs and chirt in our ds \n",
    "                        ds_mask = ds.where(ds.temp != -9999, drop = False) #<<<<------ need to double check this\n",
    "                        \n",
    "                        # Mask pixels for both ghs and chirts where ghs cities are not present\n",
    "                        ds_mask = ds_mask.where(ds_mask.ghs > 0, drop = False)\n",
    "                        \n",
    "                        # Group poly_IDs find temp\n",
    "                        avg = ds_mask.groupby('ghs').mean(xr.ALL_DIMS)\n",
    "                        \n",
    "                        # turn GHS IDS and avg. CHIRTMax values into 1-D numpy arrays of equal length\n",
    "                        avg_ID = np.array(avg.ghs)\n",
    "                        avg_temp = np.array(avg.temp)\n",
    "                        \n",
    "                        print(len(avg_ID))\n",
    "                        print(len(avg_temp))\n",
    "                                \n",
    "                        # turn chirt max and IDS into a DF\n",
    "                        df_avg = pd.DataFrame()\n",
    "                        df_avg[date] = avg_temp\n",
    "                        df_avg['ID_HDC_G0'] = avg_ID\n",
    "                        \n",
    "                        # merge the df\n",
    "                        df_merge = df_merge.merge(df_avg, on='ID_HDC_G0', how = 'outer')\n",
    "\n",
    "                        # add to count and write out\n",
    "                        # count = count +1\n",
    "                        # print(count)\n",
    "                        # count = 0\n",
    "                        #if count == 3: #<<<<<<< ------ SET COUNT\n",
    "\n",
    "        # write files out for each dir        \n",
    "        # df_merge.to_file(DATA_OUT+fn_out+'_'+dir_year+'.shp') # shp out\n",
    "        df_merge.to_csv(DATA_OUT+fn_out+'_'+dir_year+'.csv') # csv out\n",
    "\n",
    "# Write out as a .shp file\n",
    "# df_merge.to_file(DATA_OUT+shp_fn_out)\n",
    "# df_merge.to_csv(DATA_OUT+csv_fn_out)\n",
    "\n",
    "print('DONE ! ! !')\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through dirs \n",
    "def chirt_ghs (dir_name):\n",
    "    for dirpath, dirnames, files in os.walk(dir_name):\n",
    "            # Set dir name for writing files\n",
    "            dir_year = dirpath.split(dir_name)[1]\n",
    "\n",
    "            # make a copy of the ghs polys, reset for each dir\n",
    "            df_merge = df_ghs.copy()\n",
    "\n",
    "            for fn in files:\n",
    "\n",
    "                    # find all the tif files\n",
    "                    if fn.endswith('.tif'):\n",
    "\n",
    "                            # NEED TO BUILD META DATA CHECK INTO ROUTINE and throw an error<<<<---------\n",
    "\n",
    "                            # Get the date of each chirt file\n",
    "                            date = (fn.split('CHIRTSmax.')[1].split('.tif')[0])\n",
    "                            print(dir_year)\n",
    "                            print(date)\n",
    "\n",
    "                            # Open CHIRT Data and turn data into array\n",
    "                            tempRst = rasterio.open(dirpath+'/'+fn)\n",
    "\n",
    "                            # Make arrays into x    array DataArray\n",
    "                            tempRst_da = xr.DataArray(tempRst.read(1), dims = ['y', 'x']) # y and x are our 2-d labels\n",
    "\n",
    "                            # Make xarray dataset\n",
    "                            ds = xr.Dataset(data_vars = \n",
    "                                    {'ghs' : (['y', 'x'], polyRst_da),\n",
    "                                    'temp' : (['y', 'x'], tempRst_da),})\n",
    "\n",
    "                            # UPDATED 2019-08-19 Mask the CHIRTS PIXELS FIRST, THEN GHS\n",
    "                            # Mask values from chirt that are ocean in ghs and chirt in our ds \n",
    "                            ds_mask = ds.where(ds.temp != -9999, drop = False) #<<<<------ need to double check this\n",
    "\n",
    "                            # Mask pixels for both ghs and chirts where ghs cities are not present\n",
    "                            ds_mask = ds_mask.where(ds_mask.ghs > 0, drop = False)\n",
    "\n",
    "                            # Group poly_IDs find temp\n",
    "                            avg = ds_mask.groupby('ghs').mean(xr.ALL_DIMS)\n",
    "\n",
    "                            # turn GHS IDS and avg. CHIRTMax values into 1-D numpy arrays of equal length\n",
    "                            avg_ID = np.array(avg.ghs)\n",
    "                            avg_temp = np.array(avg.temp)\n",
    "\n",
    "                            print(len(avg_ID))\n",
    "                            print(len(avg_temp))\n",
    "\n",
    "                            # turn chirt max and IDS into a DF\n",
    "                            df_avg = pd.DataFrame()\n",
    "                            df_avg[date] = avg_temp\n",
    "                            df_avg['ID_HDC_G0'] = avg_ID\n",
    "\n",
    "                            # merge the df\n",
    "                            df_merge = df_merge.merge(df_avg, on='ID_HDC_G0', how = 'outer')\n",
    "\n",
    "                            # add to count and write out\n",
    "                            # count = count +1\n",
    "                            # print(count)\n",
    "                            # count = 0\n",
    "                            #if count == 3: #<<<<<<< ------ SET COUNT\n",
    "\n",
    "            # write files out for each dir        \n",
    "            # df_merge.to_file(DATA_OUT+fn_out+'_'+dir_year+'.shp') # shp out\n",
    "            df_merge.to_csv(DATA_OUT+fn_out+'_'+dir_year+'ParPro.csv') # csv out\n",
    "\n",
    "    # Write out as a .shp file\n",
    "    # df_merge.to_file(DATA_OUT+shp_fn_out)\n",
    "    # df_merge.to_csv(DATA_OUT+csv_fn_out)\n",
    "\n",
    "    print('DONE ! ! !')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple (dir_name):\n",
    "    for dirpath, dirnames, files in os.walk(dir_name):\n",
    "            # Set dir name for writing files\n",
    "            dir_year = dirpath.split(dir_name)[1]\n",
    "\n",
    "            for fn in files:\n",
    "\n",
    "                    # find all the tif files\n",
    "                    if fn.endswith('.tif'):\n",
    "                        print(dir_year)\n",
    "                        print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "def main():\n",
    "  pool = mp.Pool(3) # mp.cpu_count() using 3 CPUS\n",
    "  result = pool.map(simple, CHIRT_DIR)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import rasterio\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "CHIRT_DIR = '/Users/cascade/Github/UrbanHeat/data/test_in/' # <<--- path to loop through\n",
    "\n",
    "def process(fn):\n",
    "    date = (fn.split('CHIRTSmax.')[1].split('.tif')[0])\n",
    "    print(date)\n",
    "\n",
    "    # Open CHIRT Data and turn data into array\n",
    "    with rasterio.open(dirpath+'/'+fn, 'r') as dst:\n",
    "        print(dst.meta)\n",
    "\n",
    "\n",
    "queue = Queue()\n",
    "processes = [Process(target=process, args=()) for x in range(4)]\n",
    "\n",
    "for p in processes:\n",
    "    p.start()\n",
    "\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "# # p = multiprocessing.Pool()\n",
    "\n",
    "# pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "# # for dirpath, dirnames, files in os.walk(CHIRT_DIR):\n",
    "# #         # Set dir name for writing files\n",
    "# #         dir_year = dirpath.split(CHIRT_DIR)[1]\n",
    "\n",
    "# #         # make a copy of the ghs polys, reset for each dir\n",
    "# #         df_merge = df_ghs.copy()\n",
    "        \n",
    "# for dirpath, dirnames, files in os.walk(CHIRT_DIR):\n",
    "#     for fn in files:\n",
    "#         if fn.endswith('.tif'):\n",
    "#             print(type([fn]))\n",
    "# #             fn_in = files+fn\n",
    "# #             pool.map(process, [fn_in])\n",
    "\n",
    "\n",
    "# # for dirpath, dirnames, files in os.listdir(CHIRT_DIR):\n",
    "# #         for fn in files:\n",
    "# #             if fn.endswith('.tif'):\n",
    "# #                 pool.map(process, [fn])\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "def print_func(continent='Asia'):\n",
    "    print('The name of continent is : ', continent)\n",
    "\n",
    "if __name__ == \"__main__\":  # confirms that the code is under main function\n",
    "    names = ['America', 'Europe', 'Africa']\n",
    "    procs = []\n",
    "    proc = Process(target=print_func)  # instantiating without any argument\n",
    "    procs.append(proc)\n",
    "    proc.start()\n",
    "\n",
    "    # instantiating process with arguments\n",
    "    for name in names:\n",
    "        # print(name)\n",
    "        proc = Process(target=print_func, args=(name,))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "\n",
    "    # complete the processes\n",
    "    for proc in procs:\n",
    "        proc.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "import time\n",
    "\n",
    "work = ([\"A\", 5], [\"B\", 2], [\"C\", 1], [\"D\", 3])\n",
    "\n",
    "\n",
    "def work_log(work_data):\n",
    "    print(\" Process %s waiting %s seconds\" % (work_data[0], work_data[1]))\n",
    "    time.sleep(int(work_data[1]))\n",
    "    print(\" Process %s Finished.\" % work_data[0])\n",
    "\n",
    "\n",
    "def pool_handler():\n",
    "    p = Pool(2)\n",
    "    p.map(work_log, work)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool_handler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "from multiprocessing import Process\n",
    " \n",
    "def doubler(number):\n",
    "    \"\"\"\n",
    "    A doubling function that can be used by a process\n",
    "    \"\"\"\n",
    "    result = number * 2\n",
    "    proc = os.getpid()\n",
    "    print('{0} doubled to {1} by process id: {2}'.format(\n",
    "        number, result, proc))\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    numbers = [5, 10, 15, 20, 25]\n",
    "    procs = []\n",
    " \n",
    "    for index, number in enumerate(numbers):\n",
    "        proc = Process(target=doubler, args=(number,))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    " \n",
    "    for proc in procs:\n",
    "        proc.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 20, 40]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    " \n",
    "def doubler(number):\n",
    "    return number * 2\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    numbers = [5, 10, 20]\n",
    "    pool = Pool(processes=3)\n",
    "    print(pool.map(doubler, numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHIRT_DIR = '/Users/cascade/Github/UrbanHeat/data/test_in/year1/' # <<--- path to loop through\n",
    "\n",
    "for fn in os.listdir(CHIRT_DIR):\n",
    "    print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "def print_func(dir_list): \n",
    "    for fn in os.listdir(dir_list):\n",
    "        print(fn)\n",
    "\n",
    "proc = Process(target=print_func, args=CHIRT_DIR)  # instantiating without any argument)\n",
    "proc.start()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### HERE START HERE ! ! ! ####\n",
    "# https://stackoverflow.com/questions/41992810/python-multiprocessing-across-different-files\n",
    "\n",
    "CHIRT_DIR = '/Users/cascade/Github/UrbanHeat/data/test_in/year1' # <<--- path to loop through\n",
    "\n",
    "from multiprocessing import Pool, Queue\n",
    "import time \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def crunch_file(queue):\n",
    "    while not queue.empty():\n",
    "        fn = queue.get()\n",
    "        date = (fn.split('CHIRTSmax.')[1].split('.tif')[0])\n",
    "        print(date)\n",
    "        \n",
    "for root, dirnames, filenames in os.walk(CHIRT_DIR):\n",
    "    for fn in filenames:\n",
    "        queue.put(fn)\n",
    "        pool = Pool(None, crunch_file, (queue,))\n",
    "\n",
    "        pool.close() # signal that we won't submit any more tasks to pool\n",
    "pool.join() # wait until all processes are done\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHIRT_DIR = '/Users/cascade/Github/UrbanHeat/data/test_in/' # <<--- path to loop through\n",
    "for root, dirnames, filenames in os.walk(CHIRT_DIR):\n",
    "    print(root)\n",
    "#    print(dirnames)\n",
    "#     print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.walk(CHIRT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "glob(CHIRT_DIR+'/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import Pool\n",
    "from multiprocessing import JoinableQueue as Queue\n",
    "import os\n",
    "\n",
    "def explore_path(path):\n",
    "    directories = []\n",
    "    nondirectories = []\n",
    "    for filename in os.listdir(path):\n",
    "        fullname = os.path.join(path, filename)\n",
    "        if os.path.isdir(fullname):\n",
    "            directories.append(fullname)\n",
    "        else:\n",
    "            nondirectories.append(filename)\n",
    "    outputfile = path.replace(os.sep, '_') + '.txt'\n",
    "    with open(outputfile, 'w') as f:\n",
    "        for filename in nondirectories:\n",
    "            print >> f, filename\n",
    "    return directories\n",
    "\n",
    "def parallel_worker():\n",
    "    while True:\n",
    "        path = unsearched.get()\n",
    "        dirs = explore_path(path)\n",
    "        for newdir in dirs:\n",
    "            unsearched.put(newdir)\n",
    "        unsearched.task_done()\n",
    "\n",
    "# acquire the list of paths\n",
    "with open('paths.txt') as f:\n",
    "    paths = f.split()\n",
    "\n",
    "unsearched = Queue()\n",
    "for path in paths:\n",
    "    unsearched.put(path)\n",
    "\n",
    "pool = Pool(5)\n",
    "for i in range(5):\n",
    "    pool.apply_async(parallel_worker)\n",
    "\n",
    "unsearched.join()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirnames, filenames in os.walk(CHIRT_DIR):\n",
    "    data = filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through dirs\n",
    "\n",
    "# helper function \n",
    "def temp_ghs(dir_nm):\n",
    "#     print(type(dir_nm))\n",
    "    for fn in os.listdir(dir_nm):\n",
    "        \n",
    "        # find all the tif files\n",
    "        if fn.endswith('.tif'):\n",
    "\n",
    "                # NEED TO BUILD META DATA CHECK INTO ROUTINE and throw an error<<<<---------\n",
    "\n",
    "                # Get the date of each chirt file\n",
    "                date = (fn.split('CHIRTSmax.')[1].split('.tif')[0])\n",
    "                print(dir_year)\n",
    "                print(date)\n",
    "\n",
    "                # Open CHIRT Data and turn data into array\n",
    "                tempRst = rasterio.open(dirpath+'/'+fn)\n",
    "\n",
    "                # Make arrays into x    array DataArray\n",
    "                tempRst_da = xr.DataArray(tempRst.read(1), dims = ['y', 'x']) # y and x are our 2-d labels\n",
    "\n",
    "                # Make xarray dataset\n",
    "                ds = xr.Dataset(data_vars = \n",
    "                        {'ghs' : (['y', 'x'], polyRst_da),\n",
    "                        'temp' : (['y', 'x'], tempRst_da),})\n",
    "\n",
    "                # UPDATED 2019-08-19 Mask the CHIRTS PIXELS FIRST, THEN GHS\n",
    "                # Mask values from chirt that are ocean in ghs and chirt in our ds \n",
    "                ds_mask = ds.where(ds.temp != -9999, drop = False) #<<<<------ need to double check this\n",
    "\n",
    "                # Mask pixels for both ghs and chirts where ghs cities are not present\n",
    "                ds_mask = ds_mask.where(ds_mask.ghs > 0, drop = False)\n",
    "\n",
    "                # Group poly_IDs find temp\n",
    "                avg = ds_mask.groupby('ghs').mean(xr.ALL_DIMS)\n",
    "\n",
    "                # turn GHS IDS and avg. CHIRTMax values into 1-D numpy arrays of equal length\n",
    "                avg_ID = np.array(avg.ghs)\n",
    "                avg_temp = np.array(avg.temp)\n",
    "\n",
    "                print(len(avg_ID))\n",
    "                print(len(avg_temp))\n",
    "\n",
    "                # turn chirt max and IDS into a DF\n",
    "                df_avg = pd.DataFrame()\n",
    "                df_avg[date] = avg_temp\n",
    "                df_avg['ID_HDC_G0'] = avg_ID\n",
    "\n",
    "                # merge the df\n",
    "                df_merge = df_merge.merge(df_avg, on='ID_HDC_G0', how = 'outer')\n",
    "\n",
    "                # add to count and write out\n",
    "                # count = count +1\n",
    "                # print(count)\n",
    "                # count = 0\n",
    "                #if count == 3: #<<<<<<< ------ SET COUNT\n",
    "\n",
    "# write files out for each dir        \n",
    "# df_merge.to_file(DATA_OUT+fn_out+'_'+dir_year+'.shp') # shp out\n",
    "df_merge.to_csv(DATA_OUT+fn_out+'_'+dir_year+'ParPro.csv') # csv out\n",
    "\n",
    "# Write out as a .shp file\n",
    "# df_merge.to_file(DATA_OUT+shp_fn_out)\n",
    "# df_merge.to_csv(DATA_OUT+csv_fn_out)\n",
    "\n",
    "print('DONE ! ! !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1983.01\n",
      "1983.03\n",
      "1983.02\n",
      "1983.06\n",
      "1983.05\n",
      "1983.04\n",
      "1983.09\n",
      "1983.08\n",
      "1983.12\n",
      "1983.07\n",
      "1983.11\n",
      "1983.10\n",
      "0.07575726509094238\n"
     ]
    }
   ],
   "source": [
    "#### HERE START HERE ! ! ! ####\n",
    "# https://stackoverflow.com/questions/41992810/python-multiprocessing-across-different-files\n",
    "\n",
    "CHIRT_DIR = '/Users/cascade/Github/UrbanHeat/data/test_in/' # <<--- path to loop through\n",
    "\n",
    "from multiprocessing import Pool, Queue, Process\n",
    "import time \n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# helper function \n",
    "def open_file(dir_nm):\n",
    "#     print(type(dir_nm))\n",
    "    for fn in os.listdir(dir_nm):\n",
    "        date = (fn.split('CHIRTSmax.')[1].split('.tif')[0])\n",
    "        print(date)\n",
    "\n",
    "dir_list = glob(CHIRT_DIR+\"/*\")\n",
    "\n",
    "for dir_nm in (dir_list):\n",
    "    proc = Process(target=open_file, args=(dir_nm,))\n",
    "    proc.start()\n",
    "    proc.join()\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1983.01\n",
      "1983.03\n",
      "1983.02\n",
      "1983.06\n",
      "1983.05\n",
      "1983.04\n",
      "1983.09\n",
      "1983.08\n",
      "1983.12\n",
      "1983.07\n",
      "1983.11\n",
      "1983.10\n",
      "0.0008687973022460938\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for dir_nm in (dir_list):\n",
    "    open_file(dir_nm)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "from multiprocessing import Process\n",
    " \n",
    "def doubler(number):\n",
    "    \"\"\"\n",
    "    A doubling function that can be used by a process\n",
    "    \"\"\"\n",
    "    result = number * 2\n",
    "    proc = os.getpid()\n",
    "    print('{0} doubled to {1} by process id: {2}'.format(\n",
    "        number, result, proc))\n",
    " \n",
    "\n",
    "numbers = [5, 10, 15, 20, 25]\n",
    "procs = []\n",
    "\n",
    "for index, number in enumerate(numbers):\n",
    "    proc = Process(target=doubler, args=(number,))\n",
    "    procs.append(proc)\n",
    "    proc.start()\n",
    "\n",
    "for proc in procs:\n",
    "    proc.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
