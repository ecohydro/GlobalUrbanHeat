{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tmax Subset\n",
    "\n",
    "A notebook to subset Tmax daily for the 13000 GHS urban areas to identify dates >40c, consecuritve days >40 c etc.\n",
    "\n",
    "**Need to subset**\n",
    "- Days per year (done)\n",
    "- Duration of each event (done)\n",
    "- Intensity of each day during each event (>40.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depdencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from random import random\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import geopandas as gpd \n",
    "import glob\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_xr(file_in, time_dim, space_dim):\n",
    "    \n",
    "    \"\"\" Function reads in a csv w/ GHS-UCDB IDs and temp, isolates the temp\n",
    "    and returns a xarray data array with dims set to city ids and dates\n",
    "    \n",
    "    Args:\n",
    "        file_in = file name and path\n",
    "        time_dim = name for time dim as a str ... use date :-)\n",
    "        space_dim = col name for GHS-UCDB IDs as an str (ID_HDC_G0)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(file_in) # read the file in as a df\n",
    "    print(df.shape)\n",
    "    \n",
    "    df_id = df[space_dim] # get IDs\n",
    "    df_temp = df.iloc[:,3:] # get only temp columns\n",
    "    df_temp.index = df_id # set index values\n",
    "    df_temp_drop = df_temp.dropna() # Drop cities w/ no temp record \n",
    "    print(len(df_temp_drop))\n",
    "    \n",
    "    temp_np = df_temp_drop.to_numpy() # turn temp cols into an np array\n",
    "    \n",
    "    # make xr Data Array w/ data as temp and dims as spece (e.g. id)\n",
    "    \n",
    "    # Note 2019 09 17 changed to xr.Dataset from xr.Dataarray\n",
    "    temp_xr_da = xr.DataArray(temp_np, coords=[df_temp_drop.index, df_temp_drop.columns], \n",
    "                            dims=[space_dim, time_dim])\n",
    "    \n",
    "    return temp_xr_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_eventTot(xarray, Tthresh, year):\n",
    "    \"\"\" Function returns the number of days within a year where Tmax > Tthresh for each city.\n",
    "    \n",
    "    Args: \n",
    "        xarray = an xarray object with dims = (space, times)\n",
    "        Tthresh = int of temp threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    ## NOTE FOR SOME REASON out.ID_HDC_G0 cannot be fed a string ... note sure why so be careful with col names\n",
    "    out = xarray.where(xarray > Tthresh, drop = True)\n",
    "    id_list = []\n",
    "    event_tot = []\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    for index, loc in enumerate(out.ID_HDC_G0):\n",
    "        id_list.append(out.ID_HDC_G0.values[index])\n",
    "        event_tot.append(len(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values))\n",
    "    \n",
    "    df_out['ID_HDC_G0'] = id_list\n",
    "    df_out[year] = event_tot\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eventTot_loop(dir_nm, time_dim, space_dim, Tthresh):\n",
    "    \n",
    "    \"\"\" Loop through a dir with csvs to calc the total number of events greater than a threshold.\n",
    "        Leap years explain the difference in shapes 368 vs 369\n",
    "    \n",
    "    Args:\n",
    "        dir_nm = dir path to loop through\n",
    "        time_dim = name for time dim as a str ... use date :-) for csv_to_xr function\n",
    "        space_dim = col name for GHS-UCDB IDs as an str (ID_HDC_G0) for csv_to_xr function\n",
    "        Tthresh = int of temp threshold for temp_event function -- 40.6 is\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the GHS-ID List with GeoPANDAS read_file\n",
    "    ghs_ids_fn = 'GHS-UCSB-IDS.csv'\n",
    "    ghs_ids_df = pd.read_csv(DATA_INTERIM+ghs_ids_fn)\n",
    "    \n",
    "    # Git File list\n",
    "    fn_list = glob.glob(DAILY_PATH+'*.csv')\n",
    "    \n",
    "    for fn in sorted(fn_list):\n",
    "        \n",
    "        # Get year for arg for temp_event function\n",
    "        year = fn.split('GHS-Tmax-DAILY_')[1].split('.csv')[0]\n",
    "        print(year)\n",
    "        \n",
    "        temp_xr_da = csv_to_xr(fn, time_dim, space_dim)\n",
    "        \n",
    "        df_out = temp_eventTot(temp_xr_da, Tthresh, year)\n",
    "        \n",
    "        ghs_ids_df = ghs_ids_df.merge(df_out, on='ID_HDC_G0', how = 'outer') #<<<<----- NEED TO FIX THIS\n",
    "    \n",
    "    # build in later drop all NA GHS-IDs\n",
    "    \n",
    "    return ghs_ids_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_eventL_a(xarray, Tthresh, year): #<---------------- # NEED TO RENAME or\n",
    "    \"\"\" Function calculates the length of each Tmax threshold event as the number of days in a row\n",
    "    greater than a threshold within a year where Tmax > Tthresh for each city.\n",
    "    \n",
    "    Args: \n",
    "        xarray = an xarray object with dims = (space, times)\n",
    "        Tthresh = int of temp threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    ## NOTE FOR SOME REASON out.ID_HDC_G0 cannot be fed a string ... note sure why so be careful with col names\n",
    "    out = xarray.where(xarray > Tthresh, drop = True)\n",
    "    id_list = []\n",
    "    event_L = []\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    for index, loc in enumerate(out.ID_HDC_G0):\n",
    "        id_list.append(out.ID_HDC_G0.values[index])\n",
    "        event_tot.append(len(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values))\n",
    "    \n",
    "    df_out['ID_HDC_G0'] = id_list\n",
    "    df_out[year] = event_tot\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_eventL(xarray, Tthresh):\n",
    "    \"\"\" Function calculates the length of each Tmax threshold event as the number of days in a row\n",
    "    greater than a threshold within a year where Tmax > Tthresh for each city. Returns the length,\n",
    "    the dates, the tempatures, and the severity (daily Tmax - Tthresh)\n",
    "    \n",
    "    Args: \n",
    "        xarray = an xarray object with dims = (space, times)\n",
    "        Tthresh = int of temp threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty lists & df\n",
    "    id_list = []\n",
    "    date_list = []\n",
    "    dayTot_list = []\n",
    "    temp_list = []\n",
    "    severity_list = []\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    # subset xarry\n",
    "    out = xarray.where(xarray > Tthresh, drop = True)\n",
    "\n",
    "    # start loop \n",
    "    for index, loc in enumerate(out.ID_HDC_G0):\n",
    "        id_list.append(out.ID_HDC_G0.values[index]) # get IDS\n",
    "        date_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values) # get event dates\n",
    "        \n",
    "        # this is actually getting the total events of all 2019-09-22\n",
    "        dayTot_list.append(len(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values)) # get event totals\n",
    "        \n",
    "        temp_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').values) # get temp values\n",
    "        severity_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').values - Tthresh) # get severity\n",
    "\n",
    "    # write to a data frame\n",
    "    df_out['ID_HDC_G0'] = id_list\n",
    "    df_out['Days_Total'] = dayTot_list\n",
    "    df_out['Event_Dates'] = date_list\n",
    "    df_out['Event_Temps'] = temp_list\n",
    "    df_out['Event_Severity'] = severity_list\n",
    "\n",
    "    # return df_out\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eventL_loop(dir_nm, fn_out, time_dim, space_dim, Tthresh):\n",
    "    \n",
    "    \"\"\" Loop through a dir with csvs to apply temp_eventL function and save out a .csv for each year\n",
    "    \n",
    "    Args:\n",
    "        dir_nm = dir path to loop through\n",
    "        fn_out = string to label out files\n",
    "        time_dim = name for time dim as a str ... use date :-) for csv_to_xr function\n",
    "        space_dim = col name for GHS-UCDB IDs as an str (ID_HDC_G0) for csv_to_xr function\n",
    "        Tthresh = int of temp threshold for temp_event function -- 40.6 is\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the GHS-ID List with GeoPANDAS read_file\n",
    "    ghs_ids_fn = 'GHS-UCSB-IDS.csv'\n",
    "    ghs_ids_df = pd.read_csv(DATA_INTERIM+ghs_ids_fn)\n",
    "        \n",
    "    # Git File list\n",
    "    fn_list = glob.glob(DAILY_PATH+'*.csv')\n",
    "    \n",
    "    for fn in sorted(fn_list):\n",
    "        \n",
    "        # Get year for arg for temp_event function\n",
    "        year = fn.split('GHS-Tmax-DAILY_')[1].split('.csv')[0]\n",
    "        print(year)\n",
    "        \n",
    "        temp_xr_da = csv_to_xr(fn, time_dim, space_dim)\n",
    "        \n",
    "        df_out = temp_eventL(temp_xr_da, Tthresh)\n",
    "                \n",
    "        ghs_ids_df_out = ghs_ids_df.merge(df_out, on='ID_HDC_G0', how = 'inner') #<<<<----- NEED TO FIX THIS\n",
    "\n",
    "        ghs_ids_df_out.to_csv(DATA_OUT+fn_out+year+'.csv')\n",
    "\n",
    "        print(year, 'SAVED!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths\n",
    "DAILY_PATH = '/home/cascade/projects/data_out_urbanheat/CHIRTS-GHS-DAILY/'\n",
    "DATA_INTERIM = '/home/cascade/projects/UrbanHeat/data/interim/'\n",
    "DATA_OUT = '/home/cascade/projects/data_out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name to test\n",
    "fn_in = 'GHS-Tmax-DAILY_1983.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13135, 368)\n",
      "13067\n"
     ]
    }
   ],
   "source": [
    "xr1993 = csv_to_xr(DAILY_PATH+fn_in, 'date', 'ID_HDC_G0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (ID_HDC_G0: 13067, date: 365)>\n",
       "array([[-43.921947, -33.71345 , -33.054974, ..., -12.416152, -13.232986,\n",
       "        -15.403823],\n",
       "       [ -4.804248,  -3.914425,  -7.533999, ...,  -5.186461, -10.945722,\n",
       "        -16.29516 ],\n",
       "       [-23.904118, -17.422953, -13.182008, ..., -12.788978, -11.337886,\n",
       "        -10.00939 ],\n",
       "       ...,\n",
       "       [ 16.028023,  17.73603 ,  20.493294, ...,  14.559421,  15.160739,\n",
       "         15.184024],\n",
       "       [ 16.420553,  17.87142 ,  22.519674, ...,  15.680964,  16.169733,\n",
       "         16.039179],\n",
       "       [ 16.6943  ,  17.559229,  21.480919, ...,  14.446052,  15.235602,\n",
       "         14.005591]])\n",
       "Coordinates:\n",
       "  * ID_HDC_G0  (ID_HDC_G0) int64 5782 3316 5645 3185 ... 1116 1114 1161 1169\n",
       "  * date       (date) object '1983.01.01' '1983.01.02' ... '1983.12.31'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr1993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "event1993 = temp_eventL(xr1993, 40.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_HDC_G0</th>\n",
       "      <th>Days_Total</th>\n",
       "      <th>Event_Dates</th>\n",
       "      <th>Event_Temps</th>\n",
       "      <th>Event_Severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>6279</td>\n",
       "      <td>14</td>\n",
       "      <td>[1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...</td>\n",
       "      <td>[40.637226, 42.146217, 43.225623999999996, 40....</td>\n",
       "      <td>[0.03722599999999687, 1.5462169999999986, 2.62...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401</td>\n",
       "      <td>6295</td>\n",
       "      <td>10</td>\n",
       "      <td>[1983.06.23, 1983.06.24, 1983.06.26, 1983.06.2...</td>\n",
       "      <td>[41.303112, 42.403027, 41.33662, 42.872417, 41...</td>\n",
       "      <td>[0.7031119999999973, 1.8030270000000002, 0.736...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>6229</td>\n",
       "      <td>14</td>\n",
       "      <td>[1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...</td>\n",
       "      <td>[42.190994, 42.450027, 43.509415000000004, 40....</td>\n",
       "      <td>[1.590994000000002, 1.8500269999999972, 2.9094...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403</td>\n",
       "      <td>6249</td>\n",
       "      <td>15</td>\n",
       "      <td>[1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...</td>\n",
       "      <td>[41.742354999999996, 43.00139, 44.029526000000...</td>\n",
       "      <td>[1.142354999999995, 2.4013899999999992, 3.4295...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>404</td>\n",
       "      <td>6236</td>\n",
       "      <td>16</td>\n",
       "      <td>[1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...</td>\n",
       "      <td>[41.7475, 42.906532, 43.937798, 41.069748, 42....</td>\n",
       "      <td>[1.1475000000000009, 2.306531999999997, 3.3377...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>1664</td>\n",
       "      <td>1</td>\n",
       "      <td>[1983.08.05]</td>\n",
       "      <td>[41.430878]</td>\n",
       "      <td>[0.8308779999999985]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406</td>\n",
       "      <td>6263</td>\n",
       "      <td>15</td>\n",
       "      <td>[1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...</td>\n",
       "      <td>[41.94552, 43.20455, 44.232690000000005, 41.33...</td>\n",
       "      <td>[1.3455200000000005, 2.604549999999996, 3.6326...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407</td>\n",
       "      <td>6309</td>\n",
       "      <td>5</td>\n",
       "      <td>[1983.06.24, 1983.06.27, 1983.06.28, 1983.06.2...</td>\n",
       "      <td>[41.467690000000005, 41.93708, 40.816628, 41.0...</td>\n",
       "      <td>[0.8676900000000032, 1.3370800000000003, 0.216...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>12850</td>\n",
       "      <td>1</td>\n",
       "      <td>[1983.08.06]</td>\n",
       "      <td>[41.18087]</td>\n",
       "      <td>[0.5808699999999973]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409</td>\n",
       "      <td>1661</td>\n",
       "      <td>2</td>\n",
       "      <td>[1983.06.15, 1983.08.05]</td>\n",
       "      <td>[40.63337, 40.745804]</td>\n",
       "      <td>[0.0333699999999979, 0.14580399999999827]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>12841</td>\n",
       "      <td>1</td>\n",
       "      <td>[1983.08.06]</td>\n",
       "      <td>[42.226093]</td>\n",
       "      <td>[1.6260929999999973]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411</td>\n",
       "      <td>6312</td>\n",
       "      <td>8</td>\n",
       "      <td>[1983.06.23, 1983.06.24, 1983.06.26, 1983.06.2...</td>\n",
       "      <td>[40.835654999999996, 41.93557, 40.869164000000...</td>\n",
       "      <td>[0.23565499999999417, 1.335569999999997, 0.269...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412</td>\n",
       "      <td>6205</td>\n",
       "      <td>2</td>\n",
       "      <td>[1983.06.29, 1983.06.30]</td>\n",
       "      <td>[40.96825, 40.976826]</td>\n",
       "      <td>[0.3682499999999962, 0.3768260000000012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>6203</td>\n",
       "      <td>13</td>\n",
       "      <td>[1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...</td>\n",
       "      <td>[41.656876000000004, 41.91591, 42.975296, 42.1...</td>\n",
       "      <td>[1.0568760000000026, 1.3159099999999953, 2.375...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>1613</td>\n",
       "      <td>1</td>\n",
       "      <td>[1983.06.15]</td>\n",
       "      <td>[41.2614]</td>\n",
       "      <td>[0.6614000000000004]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>2761</td>\n",
       "      <td>9</td>\n",
       "      <td>[1983.06.27, 1983.07.07, 1983.07.23, 1983.07.2...</td>\n",
       "      <td>[42.827420000000004, 42.640809999999995, 41.29...</td>\n",
       "      <td>[2.227420000000002, 2.0408099999999934, 0.6940...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>2790</td>\n",
       "      <td>3</td>\n",
       "      <td>[1983.07.07, 1983.07.24, 1983.08.29]</td>\n",
       "      <td>[42.469685, 40.748492999999996, 41.240513]</td>\n",
       "      <td>[1.869684999999997, 0.14849299999999488, 0.640...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417</td>\n",
       "      <td>5349</td>\n",
       "      <td>88</td>\n",
       "      <td>[1983.06.01, 1983.06.02, 1983.06.03, 1983.06.0...</td>\n",
       "      <td>[41.509215999999995, 44.25772, 42.885985999999...</td>\n",
       "      <td>[0.9092159999999936, 3.6577199999999976, 2.285...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418</td>\n",
       "      <td>6280</td>\n",
       "      <td>13</td>\n",
       "      <td>[1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...</td>\n",
       "      <td>[41.118237, 42.552242, 43.61627, 41.074978, 42...</td>\n",
       "      <td>[0.5182369999999992, 1.9522419999999983, 3.016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419</td>\n",
       "      <td>1668</td>\n",
       "      <td>1</td>\n",
       "      <td>[1983.08.05]</td>\n",
       "      <td>[40.858604]</td>\n",
       "      <td>[0.2586039999999983]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1623</td>\n",
       "      <td>3</td>\n",
       "      <td>[1983.06.15, 1983.06.16, 1983.09.06]</td>\n",
       "      <td>[42.449690000000004, 40.707535, 40.890144]</td>\n",
       "      <td>[1.8496900000000025, 0.1075349999999986, 0.290...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>6302</td>\n",
       "      <td>9</td>\n",
       "      <td>[1983.06.23, 1983.06.24, 1983.06.26, 1983.06.2...</td>\n",
       "      <td>[41.131367, 42.23128, 41.164875, 42.700676, 41...</td>\n",
       "      <td>[0.5313669999999959, 1.6312799999999967, 0.564...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>2097</td>\n",
       "      <td>3</td>\n",
       "      <td>[1983.07.10, 1983.08.06, 1983.08.20]</td>\n",
       "      <td>[40.84176, 40.677690000000005, 40.69319]</td>\n",
       "      <td>[0.2417599999999993, 0.07769000000000403, 0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>6277</td>\n",
       "      <td>13</td>\n",
       "      <td>[1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...</td>\n",
       "      <td>[41.702248, 42.96128, 43.989418, 41.0907969999...</td>\n",
       "      <td>[1.102247999999996, 2.3612800000000007, 3.3894...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>5358</td>\n",
       "      <td>91</td>\n",
       "      <td>[1983.05.21, 1983.06.01, 1983.06.02, 1983.06.0...</td>\n",
       "      <td>[40.734726, 41.691296, 44.4398, 43.06806599999...</td>\n",
       "      <td>[0.13472600000000057, 1.0912959999999998, 3.83...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>5927</td>\n",
       "      <td>2</td>\n",
       "      <td>[1983.07.28, 1983.07.29]</td>\n",
       "      <td>[40.822975, 41.583168]</td>\n",
       "      <td>[0.22297499999999815, 0.9831679999999992]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426</td>\n",
       "      <td>6341</td>\n",
       "      <td>1</td>\n",
       "      <td>[1983.06.24]</td>\n",
       "      <td>[40.963584999999995]</td>\n",
       "      <td>[0.3635849999999934]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>5305</td>\n",
       "      <td>81</td>\n",
       "      <td>[1983.06.01, 1983.06.02, 1983.06.03, 1983.06.0...</td>\n",
       "      <td>[41.29473, 43.808859999999996, 42.0465, 42.808...</td>\n",
       "      <td>[0.6947299999999998, 3.2088599999999943, 1.446...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>5331</td>\n",
       "      <td>91</td>\n",
       "      <td>[1983.06.01, 1983.06.02, 1983.06.03, 1983.06.0...</td>\n",
       "      <td>[41.91895, 44.287247, 42.493637, 43.15344, 43....</td>\n",
       "      <td>[1.318950000000001, 3.6872469999999993, 1.8936...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>56</td>\n",
       "      <td>12</td>\n",
       "      <td>[1983.05.27, 1983.07.04, 1983.07.10, 1983.07.1...</td>\n",
       "      <td>[40.62904, 41.171574, 41.279720000000005, 42.7...</td>\n",
       "      <td>[0.029040000000001953, 0.5715739999999983, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>5111</td>\n",
       "      <td>35</td>\n",
       "      <td>[1983.06.02, 1983.06.03, 1983.06.05, 1983.06.1...</td>\n",
       "      <td>[41.448966999999996, 41.207115, 41.631126, 41....</td>\n",
       "      <td>[0.8489669999999947, 0.6071150000000003, 1.031...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>5320</td>\n",
       "      <td>89</td>\n",
       "      <td>[1983.06.01, 1983.06.02, 1983.06.03, 1983.06.0...</td>\n",
       "      <td>[42.363937, 44.440567, 42.584457, 43.040665000...</td>\n",
       "      <td>[1.7639369999999985, 3.840567, 1.9844569999999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>5488</td>\n",
       "      <td>27</td>\n",
       "      <td>[1983.07.02, 1983.07.03, 1983.07.04, 1983.07.1...</td>\n",
       "      <td>[41.693073, 40.66814, 41.684345, 40.658558, 42...</td>\n",
       "      <td>[1.0930729999999969, 0.06813999999999965, 1.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>5337</td>\n",
       "      <td>92</td>\n",
       "      <td>[1983.06.01, 1983.06.02, 1983.06.03, 1983.06.0...</td>\n",
       "      <td>[42.792477000000005, 44.869106, 43.53383, 43.6...</td>\n",
       "      <td>[2.192477000000004, 4.269106000000001, 2.93383...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>5900</td>\n",
       "      <td>60</td>\n",
       "      <td>[1983.06.08, 1983.06.19, 1983.06.20, 1983.06.2...</td>\n",
       "      <td>[40.624615000000006, 41.47041, 43.84187, 43.37...</td>\n",
       "      <td>[0.024615000000004272, 0.8704099999999997, 3.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>5322</td>\n",
       "      <td>90</td>\n",
       "      <td>[1983.06.01, 1983.06.02, 1983.06.03, 1983.06.0...</td>\n",
       "      <td>[42.420578000000006, 44.497208, 42.641098, 43....</td>\n",
       "      <td>[1.8205780000000047, 3.897207999999999, 2.0410...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>6329</td>\n",
       "      <td>3</td>\n",
       "      <td>[1983.06.23, 1983.06.24, 1983.06.27]</td>\n",
       "      <td>[40.695625, 41.521846999999994, 41.83386199999...</td>\n",
       "      <td>[0.0956249999999983, 0.9218469999999925, 1.233...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437</td>\n",
       "      <td>6217</td>\n",
       "      <td>10</td>\n",
       "      <td>[1983.06.23, 1983.06.24, 1983.06.26, 1983.06.2...</td>\n",
       "      <td>[41.557495, 42.34857, 40.813416, 41.521088, 41...</td>\n",
       "      <td>[0.9574950000000015, 1.7485700000000008, 0.213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>5361</td>\n",
       "      <td>94</td>\n",
       "      <td>[1983.05.21, 1983.06.01, 1983.06.02, 1983.06.0...</td>\n",
       "      <td>[40.685944, 42.964386, 45.041016, 43.966156, 4...</td>\n",
       "      <td>[0.0859439999999978, 2.364385999999996, 4.4410...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>6310</td>\n",
       "      <td>8</td>\n",
       "      <td>[1983.06.23, 1983.06.24, 1983.06.26, 1983.06.2...</td>\n",
       "      <td>[41.01079, 41.970079999999996, 41.30199, 42.83...</td>\n",
       "      <td>[0.41078999999999866, 1.3700799999999944, 0.70...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>12873</td>\n",
       "      <td>1</td>\n",
       "      <td>[1983.07.19]</td>\n",
       "      <td>[40.680664]</td>\n",
       "      <td>[0.08066399999999874]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>5975</td>\n",
       "      <td>1</td>\n",
       "      <td>[1983.07.30]</td>\n",
       "      <td>[40.78395]</td>\n",
       "      <td>[0.18394999999999584]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>5345</td>\n",
       "      <td>96</td>\n",
       "      <td>[1983.05.21, 1983.06.01, 1983.06.02, 1983.06.0...</td>\n",
       "      <td>[40.762733000000004, 43.044563000000004, 45.12...</td>\n",
       "      <td>[0.1627330000000029, 2.4445630000000023, 4.521...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>6231</td>\n",
       "      <td>12</td>\n",
       "      <td>[1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...</td>\n",
       "      <td>[40.610287, 42.27557, 43.066646999999996, 41.5...</td>\n",
       "      <td>[0.010286999999998159, 1.6755700000000004, 2.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>5569</td>\n",
       "      <td>25</td>\n",
       "      <td>[1983.07.03, 1983.07.04, 1983.07.05, 1983.07.1...</td>\n",
       "      <td>[40.871365000000004, 41.281063, 40.7879, 40.66...</td>\n",
       "      <td>[0.27136500000000296, 0.6810630000000018, 0.18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>12877</td>\n",
       "      <td>1</td>\n",
       "      <td>[1983.07.19]</td>\n",
       "      <td>[40.814323]</td>\n",
       "      <td>[0.21432300000000026]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446</td>\n",
       "      <td>5692</td>\n",
       "      <td>1</td>\n",
       "      <td>[1983.08.12]</td>\n",
       "      <td>[40.793816]</td>\n",
       "      <td>[0.1938159999999982]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>5313</td>\n",
       "      <td>90</td>\n",
       "      <td>[1983.06.01, 1983.06.02, 1983.06.03, 1983.06.0...</td>\n",
       "      <td>[42.379875, 44.456505, 42.600395, 43.056602000...</td>\n",
       "      <td>[1.779874999999997, 3.8565049999999985, 2.0003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>5326</td>\n",
       "      <td>94</td>\n",
       "      <td>[1983.06.01, 1983.06.02, 1983.06.03, 1983.06.0...</td>\n",
       "      <td>[42.701477000000004, 44.778107, 42.921997, 43....</td>\n",
       "      <td>[2.1014770000000027, 4.178106999999997, 2.3219...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>6265</td>\n",
       "      <td>18</td>\n",
       "      <td>[1983.06.21, 1983.06.22, 1983.06.23, 1983.06.2...</td>\n",
       "      <td>[40.98955, 41.406147, 42.965717, 43.708942, 41...</td>\n",
       "      <td>[0.38954999999999984, 0.8061469999999957, 2.36...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID_HDC_G0  Days_Total                                        Event_Dates  \\\n",
       "400       6279          14  [1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...   \n",
       "401       6295          10  [1983.06.23, 1983.06.24, 1983.06.26, 1983.06.2...   \n",
       "402       6229          14  [1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...   \n",
       "403       6249          15  [1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...   \n",
       "404       6236          16  [1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...   \n",
       "405       1664           1                                       [1983.08.05]   \n",
       "406       6263          15  [1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...   \n",
       "407       6309           5  [1983.06.24, 1983.06.27, 1983.06.28, 1983.06.2...   \n",
       "408      12850           1                                       [1983.08.06]   \n",
       "409       1661           2                           [1983.06.15, 1983.08.05]   \n",
       "410      12841           1                                       [1983.08.06]   \n",
       "411       6312           8  [1983.06.23, 1983.06.24, 1983.06.26, 1983.06.2...   \n",
       "412       6205           2                           [1983.06.29, 1983.06.30]   \n",
       "413       6203          13  [1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...   \n",
       "414       1613           1                                       [1983.06.15]   \n",
       "415       2761           9  [1983.06.27, 1983.07.07, 1983.07.23, 1983.07.2...   \n",
       "416       2790           3               [1983.07.07, 1983.07.24, 1983.08.29]   \n",
       "417       5349          88  [1983.06.01, 1983.06.02, 1983.06.03, 1983.06.0...   \n",
       "418       6280          13  [1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...   \n",
       "419       1668           1                                       [1983.08.05]   \n",
       "420       1623           3               [1983.06.15, 1983.06.16, 1983.09.06]   \n",
       "421       6302           9  [1983.06.23, 1983.06.24, 1983.06.26, 1983.06.2...   \n",
       "422       2097           3               [1983.07.10, 1983.08.06, 1983.08.20]   \n",
       "423       6277          13  [1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...   \n",
       "424       5358          91  [1983.05.21, 1983.06.01, 1983.06.02, 1983.06.0...   \n",
       "425       5927           2                           [1983.07.28, 1983.07.29]   \n",
       "426       6341           1                                       [1983.06.24]   \n",
       "427       5305          81  [1983.06.01, 1983.06.02, 1983.06.03, 1983.06.0...   \n",
       "428       5331          91  [1983.06.01, 1983.06.02, 1983.06.03, 1983.06.0...   \n",
       "429         56          12  [1983.05.27, 1983.07.04, 1983.07.10, 1983.07.1...   \n",
       "430       5111          35  [1983.06.02, 1983.06.03, 1983.06.05, 1983.06.1...   \n",
       "431       5320          89  [1983.06.01, 1983.06.02, 1983.06.03, 1983.06.0...   \n",
       "432       5488          27  [1983.07.02, 1983.07.03, 1983.07.04, 1983.07.1...   \n",
       "433       5337          92  [1983.06.01, 1983.06.02, 1983.06.03, 1983.06.0...   \n",
       "434       5900          60  [1983.06.08, 1983.06.19, 1983.06.20, 1983.06.2...   \n",
       "435       5322          90  [1983.06.01, 1983.06.02, 1983.06.03, 1983.06.0...   \n",
       "436       6329           3               [1983.06.23, 1983.06.24, 1983.06.27]   \n",
       "437       6217          10  [1983.06.23, 1983.06.24, 1983.06.26, 1983.06.2...   \n",
       "438       5361          94  [1983.05.21, 1983.06.01, 1983.06.02, 1983.06.0...   \n",
       "439       6310           8  [1983.06.23, 1983.06.24, 1983.06.26, 1983.06.2...   \n",
       "440      12873           1                                       [1983.07.19]   \n",
       "441       5975           1                                       [1983.07.30]   \n",
       "442       5345          96  [1983.05.21, 1983.06.01, 1983.06.02, 1983.06.0...   \n",
       "443       6231          12  [1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...   \n",
       "444       5569          25  [1983.07.03, 1983.07.04, 1983.07.05, 1983.07.1...   \n",
       "445      12877           1                                       [1983.07.19]   \n",
       "446       5692           1                                       [1983.08.12]   \n",
       "447       5313          90  [1983.06.01, 1983.06.02, 1983.06.03, 1983.06.0...   \n",
       "448       5326          94  [1983.06.01, 1983.06.02, 1983.06.03, 1983.06.0...   \n",
       "449       6265          18  [1983.06.21, 1983.06.22, 1983.06.23, 1983.06.2...   \n",
       "\n",
       "                                           Event_Temps  \\\n",
       "400  [40.637226, 42.146217, 43.225623999999996, 40....   \n",
       "401  [41.303112, 42.403027, 41.33662, 42.872417, 41...   \n",
       "402  [42.190994, 42.450027, 43.509415000000004, 40....   \n",
       "403  [41.742354999999996, 43.00139, 44.029526000000...   \n",
       "404  [41.7475, 42.906532, 43.937798, 41.069748, 42....   \n",
       "405                                        [41.430878]   \n",
       "406  [41.94552, 43.20455, 44.232690000000005, 41.33...   \n",
       "407  [41.467690000000005, 41.93708, 40.816628, 41.0...   \n",
       "408                                         [41.18087]   \n",
       "409                              [40.63337, 40.745804]   \n",
       "410                                        [42.226093]   \n",
       "411  [40.835654999999996, 41.93557, 40.869164000000...   \n",
       "412                              [40.96825, 40.976826]   \n",
       "413  [41.656876000000004, 41.91591, 42.975296, 42.1...   \n",
       "414                                          [41.2614]   \n",
       "415  [42.827420000000004, 42.640809999999995, 41.29...   \n",
       "416         [42.469685, 40.748492999999996, 41.240513]   \n",
       "417  [41.509215999999995, 44.25772, 42.885985999999...   \n",
       "418  [41.118237, 42.552242, 43.61627, 41.074978, 42...   \n",
       "419                                        [40.858604]   \n",
       "420         [42.449690000000004, 40.707535, 40.890144]   \n",
       "421  [41.131367, 42.23128, 41.164875, 42.700676, 41...   \n",
       "422           [40.84176, 40.677690000000005, 40.69319]   \n",
       "423  [41.702248, 42.96128, 43.989418, 41.0907969999...   \n",
       "424  [40.734726, 41.691296, 44.4398, 43.06806599999...   \n",
       "425                             [40.822975, 41.583168]   \n",
       "426                               [40.963584999999995]   \n",
       "427  [41.29473, 43.808859999999996, 42.0465, 42.808...   \n",
       "428  [41.91895, 44.287247, 42.493637, 43.15344, 43....   \n",
       "429  [40.62904, 41.171574, 41.279720000000005, 42.7...   \n",
       "430  [41.448966999999996, 41.207115, 41.631126, 41....   \n",
       "431  [42.363937, 44.440567, 42.584457, 43.040665000...   \n",
       "432  [41.693073, 40.66814, 41.684345, 40.658558, 42...   \n",
       "433  [42.792477000000005, 44.869106, 43.53383, 43.6...   \n",
       "434  [40.624615000000006, 41.47041, 43.84187, 43.37...   \n",
       "435  [42.420578000000006, 44.497208, 42.641098, 43....   \n",
       "436  [40.695625, 41.521846999999994, 41.83386199999...   \n",
       "437  [41.557495, 42.34857, 40.813416, 41.521088, 41...   \n",
       "438  [40.685944, 42.964386, 45.041016, 43.966156, 4...   \n",
       "439  [41.01079, 41.970079999999996, 41.30199, 42.83...   \n",
       "440                                        [40.680664]   \n",
       "441                                         [40.78395]   \n",
       "442  [40.762733000000004, 43.044563000000004, 45.12...   \n",
       "443  [40.610287, 42.27557, 43.066646999999996, 41.5...   \n",
       "444  [40.871365000000004, 41.281063, 40.7879, 40.66...   \n",
       "445                                        [40.814323]   \n",
       "446                                        [40.793816]   \n",
       "447  [42.379875, 44.456505, 42.600395, 43.056602000...   \n",
       "448  [42.701477000000004, 44.778107, 42.921997, 43....   \n",
       "449  [40.98955, 41.406147, 42.965717, 43.708942, 41...   \n",
       "\n",
       "                                        Event_Severity  \n",
       "400  [0.03722599999999687, 1.5462169999999986, 2.62...  \n",
       "401  [0.7031119999999973, 1.8030270000000002, 0.736...  \n",
       "402  [1.590994000000002, 1.8500269999999972, 2.9094...  \n",
       "403  [1.142354999999995, 2.4013899999999992, 3.4295...  \n",
       "404  [1.1475000000000009, 2.306531999999997, 3.3377...  \n",
       "405                               [0.8308779999999985]  \n",
       "406  [1.3455200000000005, 2.604549999999996, 3.6326...  \n",
       "407  [0.8676900000000032, 1.3370800000000003, 0.216...  \n",
       "408                               [0.5808699999999973]  \n",
       "409          [0.0333699999999979, 0.14580399999999827]  \n",
       "410                               [1.6260929999999973]  \n",
       "411  [0.23565499999999417, 1.335569999999997, 0.269...  \n",
       "412           [0.3682499999999962, 0.3768260000000012]  \n",
       "413  [1.0568760000000026, 1.3159099999999953, 2.375...  \n",
       "414                               [0.6614000000000004]  \n",
       "415  [2.227420000000002, 2.0408099999999934, 0.6940...  \n",
       "416  [1.869684999999997, 0.14849299999999488, 0.640...  \n",
       "417  [0.9092159999999936, 3.6577199999999976, 2.285...  \n",
       "418  [0.5182369999999992, 1.9522419999999983, 3.016...  \n",
       "419                               [0.2586039999999983]  \n",
       "420  [1.8496900000000025, 0.1075349999999986, 0.290...  \n",
       "421  [0.5313669999999959, 1.6312799999999967, 0.564...  \n",
       "422  [0.2417599999999993, 0.07769000000000403, 0.09...  \n",
       "423  [1.102247999999996, 2.3612800000000007, 3.3894...  \n",
       "424  [0.13472600000000057, 1.0912959999999998, 3.83...  \n",
       "425          [0.22297499999999815, 0.9831679999999992]  \n",
       "426                               [0.3635849999999934]  \n",
       "427  [0.6947299999999998, 3.2088599999999943, 1.446...  \n",
       "428  [1.318950000000001, 3.6872469999999993, 1.8936...  \n",
       "429  [0.029040000000001953, 0.5715739999999983, 0.6...  \n",
       "430  [0.8489669999999947, 0.6071150000000003, 1.031...  \n",
       "431  [1.7639369999999985, 3.840567, 1.9844569999999...  \n",
       "432  [1.0930729999999969, 0.06813999999999965, 1.08...  \n",
       "433  [2.192477000000004, 4.269106000000001, 2.93383...  \n",
       "434  [0.024615000000004272, 0.8704099999999997, 3.2...  \n",
       "435  [1.8205780000000047, 3.897207999999999, 2.0410...  \n",
       "436  [0.0956249999999983, 0.9218469999999925, 1.233...  \n",
       "437  [0.9574950000000015, 1.7485700000000008, 0.213...  \n",
       "438  [0.0859439999999978, 2.364385999999996, 4.4410...  \n",
       "439  [0.41078999999999866, 1.3700799999999944, 0.70...  \n",
       "440                              [0.08066399999999874]  \n",
       "441                              [0.18394999999999584]  \n",
       "442  [0.1627330000000029, 2.4445630000000023, 4.521...  \n",
       "443  [0.010286999999998159, 1.6755700000000004, 2.4...  \n",
       "444  [0.27136500000000296, 0.6810630000000018, 0.18...  \n",
       "445                              [0.21432300000000026]  \n",
       "446                               [0.1938159999999982]  \n",
       "447  [1.779874999999997, 3.8565049999999985, 2.0003...  \n",
       "448  [2.1014770000000027, 4.178106999999997, 2.3219...  \n",
       "449  [0.38954999999999984, 0.8061469999999957, 2.36...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event1993[400:450]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find breaks in time serise\n",
    "\n",
    "https://stackoverflow.com/questions/40118037/how-can-i-detect-gaps-and-consecutive-periods-in-a-time-series-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2445507.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get event dates\n",
    "days = event1993[event1993['ID_HDC_G0'] == 6279]['Event_Dates']\n",
    "\n",
    "# turn into datetime\n",
    "days = pd.to_datetime(days.values[0])\n",
    "days = days[0].to_julian_date()\n",
    "days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['1983-06-22', '1983-06-23', '1983-06-24', '1983-06-25',\n",
       "               '1983-06-26', '1983-06-27', '1983-06-28', '1983-06-29',\n",
       "               '1983-06-30', '1983-07-01', '1983-07-21', '1983-07-22',\n",
       "               '1983-07-23', '1983-08-01'],\n",
       "              dtype='datetime64[ns]', freq=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turn days back into julian days\n",
    "pd.to_datetime(days, format = '%Y.%m.%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get intensity for each day, note 'Event_Severity' is really intensity\n",
    "intensity = event1993[event1993['ID_HDC_G0'] == 6279]['Event_Severity']\n",
    "intensity = intensity.values[0]\n",
    "intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax = event1993[event1993['ID_HDC_G0'] == 6279]['Event_Temps']\n",
    "tmax = tmax.values[0]\n",
    "tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax[0:3].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tid bit to run julian days back\n",
    "\n",
    "def event_split(days, tmax, intensity, ID_HDC_G0, country):\n",
    "    \"\"\" Searchs a list of dates and isolates sequential dates as a list\n",
    "    \n",
    "    Args:\n",
    "        ID_HDC_G0: city ID as string\n",
    "        tmax: numpy.ndarray of intensities values of tmax values\n",
    "        days: pandas.core.index as julian dates\n",
    "        intensity: numpy.ndarray of intensities values\n",
    "    \"\"\"\n",
    "    \n",
    "    # city id\n",
    "    city_id = ID_HDC_G0\n",
    "    \n",
    "    # country \n",
    "    country = country\n",
    "    \n",
    "    # lists to fill\n",
    "    event_list = []\n",
    "    dur_list = []\n",
    "    intensity_list = []\n",
    "    city_id_list = []\n",
    "    tmax_list = []\n",
    "    avg_temp_list = []\n",
    "    avg_int_list = []\n",
    "    tot_int_list = []\n",
    "    country_list = []\n",
    "    \n",
    "    # data frame out\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    # Loop through dur list and isolate seq days, temps, and intensities\n",
    "    for k, g in groupby(enumerate(days.values), lambda x: x[1]-x[0]):\n",
    "        \n",
    "        days = list(map(itemgetter(1), g)) # isolate seq. days\n",
    "        dur = len(days) # event duration\n",
    "        intense = intensity[0:dur] # intensity of each day during event\n",
    "        temp = tmax[0:dur] # temp of each day during event\n",
    "        temp_avg = mean(temp) # avg. temp during event\n",
    "        avg_int = mean(intense) # avg. intensity during event\n",
    "        tot_int = intense.sum() # total intensity during event\n",
    "        \n",
    "        # turn days back into julian days\n",
    "        # days = pd.to_datetime(days, format = '%Y.%m.%d')\n",
    "        \n",
    "        city_id_list.append(city_id)\n",
    "        event_list.append(days)\n",
    "        dur_list.append(dur)\n",
    "        intensity_list.append(intense)\n",
    "        tmax_list.append(temp)\n",
    "        avg_temp_list.append(temp_avg)\n",
    "        avg_int_list.append(avg_int)\n",
    "        tot_int_list.append(tot_int)\n",
    "        country_list.append(country)\n",
    "        \n",
    "    df_out['ID_HDC_G0'] = city_id_list\n",
    "    df_out['CTR_MN_NM'] = country_list\n",
    "    df_out['duration'] = dur_list\n",
    "    df_out['avg_temp'] = avg_temp_list\n",
    "    df_out['avg_intensity'] = avg_int_list\n",
    "    df_out['tot_intensity'] = tot_int_list\n",
    "    df_out['events'] = event_list\n",
    "    df_out['duration'] = dur_list\n",
    "    df_out['intensity'] = intensity_list\n",
    "    df_out['tmax'] = tmax_list\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_out = event_split(days, tmax, intensity, 'city1', 'country')\n",
    "\n",
    "df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start running it on a one file and then build loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN = '/home/cascade/projects/data_out_urbanheat/CHIRTS-GHS-Events/'\n",
    "fn = 'CHIRTS-GHS-Events1983.csv'\n",
    "events1983 = pd.read_csv(DATA_IN+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "events1983.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in events1983.head(n=2).iterrows():\n",
    "    days = row['Event_Dates']\n",
    "    tmax = row['Event_Temps'] \n",
    "    intensity = row['Event_Severity']\n",
    "    city = row['ID_HDC_G0']\n",
    "    country = row['CTR_MN_NM']\n",
    "    \n",
    "    # turn into datetime\n",
    "    days = pd.to_datetime(days.values[0])\n",
    "    days = days.to_julian_date()\n",
    "    days\n",
    "    \n",
    "    print(days)\n",
    "#     df_test = event_split(days, tmax, intensity, city, country)\n",
    "#     print(df_test['duration'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths \n",
    "\n",
    "# UPDATE AS NEEDED <<<<< ------------------------------------------\n",
    "DAILY_PATH = '/home/cascade/projects/data_out_urbanheat/CHIRTS-GHS-DAILY/'\n",
    "DATA_INTERIM = '/home/cascade/projects/UrbanHeat/data/interim/'\n",
    "DATA_OUT = '/home/cascade/projects/data_out_urbanheat/CHIRTS-GHS-Events/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name\n",
    "fn_out = 'CHIRTS-GHS-Events'\n",
    "dir_nm = DAILY_PATH\n",
    "time_dim = 'date'\n",
    "space_dim = 'ID_HDC_G0'\n",
    "Tthresh = 40.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventL_loop(dir_nm, fn_out, time_dim, space_dim, Tthresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "\n",
    "figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "y = range(1,125)\n",
    "year = '2010'\n",
    "country = 'INDIA'\n",
    "plt.hist(india[year], bins = 125)\n",
    "plt.xlabel('Number of Days in '+year+' where Tmax >40c in ')\n",
    "plt.ylabel('Number of cities')\n",
    "plt.title(country+': For all cities with Tmax >40, how many days in '+year+' were >40C? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP BACK TO POLYGONS AND LOOK AT IT \n",
    "SHP_DIR = '/Users/cascade/Github/UrbanHeat/data/raw/ghs-ucdb/'\n",
    "shp_fn = 'GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_0.shp'\n",
    "shps = gpd.read_file(SHP_DIR+shp_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ghs = gpd.GeoDataFrame()\n",
    "df_ghs['geometry'] = shps.geometry\n",
    "df_ghs['ID_HDC_G0'] = shps.ID_HDC_G0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df_ghs.merge(events, on='ID_HDC_G0', how = 'inner') #<<<<----- NEED TO FIX THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write it out\n",
    "DATA_INTERIM = '/Users/cascade/Github/UrbanHeat/data/interim/'\n",
    "fn_out = 'GHS-TmaxDaily-events.shp'\n",
    "df_merge.to_file(DATA_INTERIM+fn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return the ID and Date where Tmax is greater than 40 as a dict, but will not return actual tempatures \n",
    "\n",
    "Tmax = np.random.randint(20, high=50, size=(3,10)) # Make a 3x10 random list\n",
    "print(Tmax)\n",
    "results = np.where(Tmax > 40) # find the index and rows\n",
    "coords = list(zip(results[0], results[1])) # zip the i and js into tuples\n",
    "\n",
    "b = [(k, list(list(zip(*g))[1])) for k, g in groupby(coords, itemgetter(0))] # group by rows\n",
    "\n",
    "print(b)\n",
    "dict_out = dict(b) # turn into a dict, where keys are city ids and values are dates\n",
    "dict_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict_out.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(Tmax > 40, Tmax, Tmax*0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(Tmax > 40) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(Tmax>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_search(array):\n",
    "    results = np.where(array > 40) # find the index and rows\n",
    "    coords = list(zip(results[0], results[1])) # zip the i and js into tuples\n",
    "    b = [(k, list(list(zip(*g))[1])) for k, g in groupby(coords, itemgetter(0))] # group by rows\n",
    "    dict_out = dict(b) # turn into a dict, where keys are city ids and values are dates\n",
    "\n",
    "    return dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_in = '/Users/cascade/Desktop/GHS-Tmax-DAILY_1983.csv'\n",
    "\n",
    "df = pd.read_csv(file_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_drop = df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.head()\n",
    "arr = df_sub.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_search = temp_search(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some fake data\n",
    "Tmax = np.random.randint(20, high=50, size=(3,10))\n",
    "locs = ['001', '002', '003']\n",
    "times = pd.date_range('2000-01-01', periods=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = xr.DataArray(Tmax, coords=[locs, times], dims=['space', 'times'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = foo.where(foo > 40, drop = True)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for loc in out.space:\n",
    "    print(len(out.sel(space = loc).dropna(dim = 'times').times.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in out.space.values:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.space.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr1993 = csv_to_xr(DAILY_PATH+fn_in, 'date', 'ID_HDC_G0')\n",
    "out = xr1993.where(xr1993 > 40.6, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "40 - out.sel(ID_HDC_G0 = 5885).dropna(dim = 'date').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#event_tot.append(len(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values))\n",
    "\n",
    "id_list = []\n",
    "date_list = []\n",
    "eventL_list = []\n",
    "temp_list = []\n",
    "df_out = pd.DataFrame()\n",
    "\n",
    "# start loop \n",
    "for index, loc in enumerate(out.ID_HDC_G0):\n",
    "\n",
    "    id_list.append(out.ID_HDC_G0.values[index]) # get IDS\n",
    "    date_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values) # get event dates\n",
    "    eventL_list.append(len(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values)) # get event lengths\n",
    "    temp_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').values) #get temp values\n",
    "\n",
    "# write to a data frame\n",
    "df_out['ID_HDC_G0'] = id_list\n",
    "df_out['Event_Length'] = eventL_list\n",
    "df_out['Event_Dates'] = date_list\n",
    "df_out['Event_Temps'] = temp_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run routine\n",
    "# all_events_df = event_loop(DAILY_PATH, 'date', 'ID_HDC_G0', 40.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move IDS to Index \n",
    "\n",
    "all_events_df = all_events_df.set_index(['ID_HDC_G0', 'CTR_MN_NM'], drop = True)\n",
    "all_events_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaNs\n",
    "all_events_df_drop = all_events_df.dropna(how = 'all')\n",
    "all_events_df_drop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_df_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = all_events_df_drop.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out['ID_HDC_G0'] = all_events_df_drop.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_df_drop = all_events_df_drop.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_df_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_events_df_drop.to_csv(DATA_OUT+'20190831_TMax-GHS_TotEvents83-2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india = all_events_df_drop[all_events_df_drop['CTR_MN_NM'] == 'India']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this https://stackoverflow.com/questions/52901387/find-group-of-consecutive-dates-in-pandas-dataframe\n",
    "\n",
    "dt = test[test['ID_HDC_G0'] == 6279]['Event_Dates']\n",
    "day = pd.Timedelta('1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = test[test['ID_HDC_G0'] == 6279]\n",
    "city_list = city.Event_Dates.tolist()\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['dates'] = city_list\n",
    "df.dates.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['1983.06.20', '1983.06.23', '1983.06.24', '1983.06.25',\n",
    "        '1983.06.26', '1983.06.27', '1983.06.28', '1983.06.29',\n",
    "        '1983.06.30', '1983.07.01', '1983.07.21', '1983.07.22',\n",
    "        '1983.07.23', '1983.08.01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dates = pd.to_datetime(dates)\n",
    "shift = pd_dates.shift(1, freq = 'D')\n",
    "day = pd.Timedelta('1d')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['dates'] = pd_dates\n",
    "# df['shift'] = shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_block = ((df - df.shift(-1)).abs() == day)\n",
    "in_block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = df.loc[in_block]\n",
    "filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_list = dt.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_list = ['1983.06.22', '1983.06.23', '1983.06.24', '1983.06.25',\n",
    "       '1983.06.26', '1983.06.27', '1983.06.28', '1983.06.29',\n",
    "       '1983.06.30', '1983.07.01', '1983.07.21', '1983.07.22',\n",
    "       '1983.07.23', '1983.08.01']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_block = ((dt - dt.shift(-1)).abs() == day) | (dt.diff() == day)\n",
    "in_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this https://stackoverflow.com/questions/52901387/find-group-of-consecutive-dates-in-pandas-dataframe\n",
    "\n",
    "dt = test[test['ID_HDC_G0'] == 6279]['Event_Dates']\n",
    "day = pd.Timedelta('1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = test[test['ID_HDC_G0'] == 6279]\n",
    "city_list = city.Event_Dates.tolist()\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['dates'] = city_list\n",
    "df.dates.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['1983.06.20', '1983.06.23', '1983.06.24', '1983.06.25',\n",
    "        '1983.06.26', '1983.06.27', '1983.06.28', '1983.06.29',\n",
    "        '1983.06.30', '1983.07.01', '1983.07.21', '1983.07.22',\n",
    "        '1983.07.23', '1983.08.01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dates = pd.to_datetime(dates)\n",
    "shift = pd_dates.shift(1, freq = 'D')\n",
    "day = pd.Timedelta('1d')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['dates'] = pd_dates\n",
    "# df['shift'] = shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_block = ((df - df.shift(-1)).abs() == day)\n",
    "in_block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = df.loc[in_block]\n",
    "filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_list = dt.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_list = ['1983.06.22', '1983.06.23', '1983.06.24', '1983.06.25',\n",
    "       '1983.06.26', '1983.06.27', '1983.06.28', '1983.06.29',\n",
    "       '1983.06.30', '1983.07.01', '1983.07.21', '1983.07.22',\n",
    "       '1983.07.23', '1983.08.01']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_block = ((dt - dt.shift(-1)).abs() == day) | (dt.diff() == day)\n",
    "in_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Another idea\n",
    "# https://stackoverflow.com/questions/2361945/detecting-consecutive-integers-in-a-list\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "data = [1, 4,5,6, 10, 15,16,17,18, 22, 25,26,27,28]\n",
    "\n",
    "for k, g in groupby(enumerate(data), lambda x: x[1]-x[0]):\n",
    "    print(map(itemgetter(1), g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [1,  4,5,6, 10, 15,16,17,18, 22, 25,26,27,28]\n",
    "for k, g in groupby(enumerate(L), lambda x: x[1]-x[0] ) :\n",
    "  print (list(map(itemgetter(1), g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['1983.06.20', '1983.06.23', '1983.06.24', '1983.06.25',\n",
    "        '1983.06.26', '1983.06.27', '1983.06.28', '1983.06.29',\n",
    "        '1983.06.30', '1983.07.01', '1983.07.21', '1983.07.22',\n",
    "        '1983.07.23', '1983.08.01']\n",
    "\n",
    "pd_dates = pd.to_datetime(dates)\n",
    "df_dates = pd.DataFrame()\n",
    "df_dates['dates'] = pd_dates\n",
    "\n",
    "\n",
    "\n",
    "test = df_dates['dates'].apply(lambda x: x.toordinal())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, g in groupby(enumerate(test), lambda x: x[1]-x[0]):\n",
    "  print (list(map(itemgetter(1), g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#event1993[['Event_Dates','Event_Severity']].apply(event_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events_shift = events.shift(1, freq = 'D')\n",
    "\n",
    "#events.to_julian_date() - events_shift.to_julian_date()\n",
    "\n",
    "# turn into list \n",
    "# events_list = [list(i) for i in events.to_list()][0]\n",
    "\n",
    "# turn into ordinal dates\n",
    "# (pd.to_datetime(events.values[0]))\n",
    "\n",
    "# pd_events = pd.to_datetime(events_list)\n",
    "# df_events = pd.DataFrame()\n",
    "# df_events['events'] = pd_events\n",
    "\n",
    "# df_events_ord = df_events['events'].apply(lambda x: x.toordinal())\n",
    "# (df_events_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_events = pd.to_datetime(events_list)\n",
    "df_events = pd.DataFrame()\n",
    "df_events['events'] = pd_events\n",
    "\n",
    "\n",
    "\n",
    "df_events_ord = df_events['events'].apply(lambda x: x.toordinal())\n",
    "(df_events_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, g in groupby(enumerate(df_events_ord), lambda x: x[1]-x[0]):\n",
    "    print(list(map(itemgetter(1), g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def events_split():\n",
    "#     events = (test[test['ID_HDC_G0'] == 6279]['Event_Dates'])\n",
    "#     events_list = events.to_list()\n",
    "#     events_list = [list(i) for i in events_list]\n",
    "#     events_list = events_list[0]\n",
    "#     events_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
