{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook is to get the beta fits correctly for distributions\n",
    "\n",
    "- https://stackoverflow.com/questions/54512531/why-doesnt-beta-fit-come-out-right\n",
    "- https://stackoverflow.com/questions/23329331/how-to-properly-fit-a-beta-distribution-in-python  \n",
    "- https://stats.stackexchange.com/questions/414277/fitting-beta-distributions-to-data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Depdencies \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from scipy.optimize import fmin\n",
    "from scipy.stats import beta\n",
    "from scipy.special import gamma as gammaf\n",
    "import scipy\n",
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions\n",
    "def OLS(df, geog, col, alpha):\n",
    "    \n",
    "    \"\"\"Finds linear coef for increase in stat by a given geography from 1983 - 2016, as well\n",
    "    as the pct change in population of the cities within the given geography\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        df = HI stats dataframe\n",
    "        geog = subset geography to calc people days regression\n",
    "        col = col to regress on \n",
    "        alpha = ci alpha for coef \n",
    "    \"\"\"\n",
    "\n",
    "    # Get results\n",
    "    labels = []\n",
    "    #delt_list = [] #CPT 2020.02.26\n",
    "    #r2_list = [] #CPT 2020.02.26\n",
    "    coef_list = []\n",
    "    leftci_list = []\n",
    "    rightci_list = []\n",
    "    p_list = []\n",
    "    df_out = pd.DataFrame()\n",
    "\n",
    "    for label, df_geog in df.groupby(geog):\n",
    "\n",
    "        # Get Data\n",
    "        X_year = np.array(df_geog.groupby('year')['ID_HDC_G0'].mean().index).reshape((-1, 1))\n",
    "        Y_stats = np.array(df_geog.groupby('year')[col].sum()).reshape((-1, 1))\n",
    "\n",
    "        # Add Intercept\n",
    "        X_year_2 = sm.add_constant(X_year)\n",
    "\n",
    "        # Regress\n",
    "        model = sm.OLS(Y_stats, X_year_2).fit() \n",
    "        \n",
    "        # Get slope\n",
    "        # first param in intercept coef, second is slope of line but if slope = 0, then intecept\n",
    "        if len(model.params) == 2:\n",
    "            coef = model.params[1]\n",
    "            \n",
    "        else:\n",
    "            coef = model.params[0]\n",
    "        \n",
    "        # R2 and P\n",
    "        #r2 = model.rsquared_adj #CPT 2020.02.26\n",
    "        p = model.pvalues[0]\n",
    "\n",
    "        # Pop change #CPT 2020.02.26\n",
    "#         delt = df_geog.drop_duplicates('ID_HDC_G0').copy()\n",
    "#         delt['delt_pop'] = delt['P2016'] - delt['P1983']\n",
    "#         delt = delt['delt_pop'].sum()\n",
    "\n",
    "        # GET Left and Right CI\n",
    "        left_ci = model.conf_int(alpha=alpha)[1][0]\n",
    "        right_ci = model.conf_int(alpha=alpha)[1][1]\n",
    "        \n",
    "        # Make lists\n",
    "        labels.append(label)\n",
    "        #r2_list.append(r2) #CPT 2020.02.26\n",
    "        coef_list.append(coef)\n",
    "        p_list.append(p)\n",
    "        leftci_list.append(left_ci)\n",
    "        rightci_list.append(right_ci)\n",
    "        #delt_list.append(delt) #CPT 2020.02.26\n",
    "\n",
    "    # Make data frame\n",
    "    df_out[geog] = labels\n",
    "    #df_out['p_delt'] = delt_list #CPT 2020.02.26\n",
    "    #df_out['r2'] = r2_list #CPT 2020.02.26\n",
    "    df_out['coef'] = coef_list\n",
    "    df_out['p_value'] = [round(elem, 4) for elem in p_list]\n",
    "    df_out['ci_left'] = leftci_list\n",
    "    df_out['ci_right'] = rightci_list \n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_OLS(stats, geog, alpha, p_value):\n",
    "    \"\"\" Function calculate OLS coef of people days due to pop and heat and the \n",
    "    attribution index for distribution plots\n",
    "    \n",
    "    Args:\n",
    "        stats = df to feed in\n",
    "        geog = geography level to conduct analysis (city-level is 'ID-HDC-G0')\n",
    "        alpha = alpha for CI coef \n",
    "        p_value = p_value to drop out coefs\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get coef for people days\n",
    "    out = OLS(stats, geog, 'people_days', alpha = alpha)\n",
    "    out.rename(columns={\"coef\": \"coef_pdays\"}, inplace = True)\n",
    "    out.rename(columns={\"p_value\": \"p_value_pdays\"}, inplace = True)\n",
    "    out.rename(columns={\"ci_left\": \"ci_left_pdays\"}, inplace = True)\n",
    "    out.rename(columns={\"ci_right\": \"ci_right_pdays\"}, inplace = True)\n",
    "    \n",
    "    # Get people days due to heat coef\n",
    "    heat = OLS(stats, geog, 'people_days_heat', alpha = alpha) # get stats \n",
    "    heat.rename(columns={\"coef\": \"coef_heat\"}, inplace = True)\n",
    "    heat.rename(columns={\"p_value\": \"p_value_heat\"}, inplace = True)\n",
    "    heat.rename(columns={\"ci_left\": \"ci_left_heat\"}, inplace = True)\n",
    "    heat.rename(columns={\"ci_right\": \"ci_right_heat\"}, inplace = True)\n",
    "    out = out.merge(heat, on = geog, how = 'left') # merge\n",
    "    \n",
    "    # Get people days due to pop\n",
    "    pop = OLS(stats, geog, 'people_days_pop', alpha = alpha) # get stats \n",
    "    pop.rename(columns={\"coef\": \"coef_pop\"}, inplace = True)\n",
    "    pop.rename(columns={\"p_value\": \"p_value_pop\"}, inplace = True)\n",
    "    pop.rename(columns={\"ci_left\": \"ci_left_pop\"}, inplace = True)\n",
    "    pop.rename(columns={\"ci_right\": \"ci_right_pop\"}, inplace = True)\n",
    "    out = out.merge(pop, on = geog, how = 'left') # merge\n",
    "    \n",
    "    # Get total days\n",
    "    totDays = OLS(stats, geog, 'total_days', alpha = alpha) # get stats \n",
    "    totDays.rename(columns={\"coef\": \"coef_totDays\"}, inplace = True)\n",
    "    totDays.rename(columns={\"p_value\": \"p_value_totDays\"}, inplace = True)\n",
    "    totDays.rename(columns={\"ci_left\": \"ci_left_totDays\"}, inplace = True)\n",
    "    totDays.rename(columns={\"ci_right\": \"ci_right_totDays\"}, inplace = True)\n",
    "    out = out.merge(totDays, on = geog, how = 'left') # merge\n",
    "    \n",
    "    # drop all neg or zero pday slopes\n",
    "    out = out[out['coef_pdays'] > 0]\n",
    "    out = out[out['coef_heat'] > 0]\n",
    "    out = out[out['coef_pop'] > 0]\n",
    "    out = out[out['coef_totDays'] > 0]\n",
    "    \n",
    "    # drop out P_values only for p-days ... idea is where is exposure increasing sig. \n",
    "    out = out[out['p_value_pdays'] < p_value]\n",
    "\n",
    "    # attrib coef --- creates range -1 to 1 index of heat vs. population as a driver of total pdays increase\n",
    "    out['coef_attrib'] = (out['coef_pop'] - out['coef_heat']) / (out['coef_pop'] + out['coef_heat']) # normalize dif\n",
    "    \n",
    "    # I am not sure if this works correcetly ... CPT 2020.02.27\n",
    "    out['coef_attrib_left'] = (out['ci_left_pop'] - out['ci_left_heat']) / (out['ci_left_pop'] + out['ci_left_heat']) # normalize dif\n",
    "    out['coef_attrib_right'] = (out['ci_right_pop'] - out['ci_right_heat']) / (out['ci_right_pop'] + out['ci_left_heat']) # normalize dif\n",
    "    \n",
    "    # normalize coef of attribution \n",
    "    norm = out['coef_attrib']\n",
    "    out['coef_attrib_norm'] = (norm-min(norm))/(max(norm)-min(norm))\n",
    "    norm = out['coef_attrib_left']\n",
    "    out['coef_attrib_norm_left'] = (norm-min(norm))/(max(norm)-min(norm))\n",
    "    norm = out['coef_attrib_right']\n",
    "    out['coef_attrib_norm_right'] = (norm-min(norm))/(max(norm)-min(norm))\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_fit(data):\n",
    "    \n",
    "    \"\"\"Function fits a beta distribution curve basde on these using scipy beta.fit\n",
    "    https://stackoverflow.com/questions/23329331/how-to-properly-fit-a-beta-distribution-in-python\n",
    "    https://stats.stackexchange.com/questions/414277/fitting-beta-distributions-to-data\n",
    "    \n",
    "    Args: Normalized data that we want to put together \n",
    "    \"\"\"\n",
    " \n",
    "    #### Fit Data\n",
    "    alpha3,beta3,xx,yy=beta.fit(data, loc = 0, scale = 1)\n",
    "    fitted=lambda x,a,bc:gammaf(a+b)/gammaf(a)/gammaf(b)*x**(a-1)*(1-x)**(b-1) #pdf of beta\n",
    "\n",
    "    xx=np.linspace(0,max(data),len(data)) # line space of X\n",
    "    xx = xx[1:] # drop first value of zero because it makes plot look funky\n",
    "    yy = fitted(xx,alpha3,beta3) # fit values to beta pdf function\n",
    "    \n",
    "    return xx, yy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load Data\n",
    "# file path\n",
    "DATA_IN = \"/home/cascade/projects/UrbanHeat/data/\"  # Note: Need ?dl=1 to make sure this file gets read correctly\n",
    "FIG_OUT = \"/home/cascade/projects/UrbanHeat/figures/\"\n",
    "\n",
    "# Raw Heat\n",
    "FN_IN = 'processed/All_data_HI406_figdata.csv'\n",
    "HI_STATS = pd.read_csv(DATA_IN+FN_IN)\n",
    "\n",
    "# scale the date in the plot \n",
    "scale = 10**9 \n",
    "\n",
    "# cols we want to add to HI_STATS\n",
    "cols = ['region', 'intermediate-region', 'sub-region','CTR_MN_NM', 'ID_HDC_G0'] \n",
    "\n",
    "# open all the data\n",
    "meta_fn = 'processed/All_data_HI406_meta.csv' # open all the data\n",
    "all_data = pd.read_csv(DATA_IN+meta_fn)\n",
    "\n",
    "# drop ID duplicates\n",
    "meta = all_data.drop_duplicates('ID_HDC_G0')\n",
    "meta = meta[cols]\n",
    "\n",
    "# Add in geography we may want to use\n",
    "HI_STATS = HI_STATS.merge(meta, on = 'ID_HDC_G0', how = 'inner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Beta Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Chunk the data\n",
    "geog = 'sub-region'\n",
    "loc = 'Western Asia'##>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> WHY 'Western Asia'\n",
    "chunk = HI_STATS[HI_STATS[geog] == loc]\n",
    "plotdata = run_OLS(chunk, 'ID_HDC_G0', alpha = 0.05, p_value =0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set lims\n",
    "ylim = [0,5]\n",
    "xlim = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = plotdata['coef_attrib_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=np.mean(data[data<.9])\n",
    "var=np.var(data[data<.9],ddof=1)\n",
    "a=mean**2*(1-mean)/var-mean\n",
    "b=a*(1-mean)/mean\n",
    "#a,b,xx,yy=beta.fit(data, loc=0, scale=1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot using the pdf in scipy\n",
    "xx = np.linspace(0,max(data[data<.9]),len(data[data<.9])) # line space of X\n",
    "yy = beta.pdf(xx, a, b, loc=0, scale=1)\n",
    "\n",
    "plt.hist(data[data<1], density = True, bins = 30)\n",
    "plt.plot(xx, yy)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# using manu calc of beta function\n",
    "\n",
    "fitted=lambda x,a,b:gammaf(a+b)/gammaf(a)/gammaf(b)*x**(a-1)*(1-x)**(b-1) #pdf of beta\n",
    "xx1 = np.linspace(0,max(data),len(data)) # line space of X\n",
    "yy1 = fitted(xx1, a, b)\n",
    "plt.hist(data, density = True, bins = 30)\n",
    "plt.plot(xx1, yy1)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY MOMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = plotdata['coef_attrib_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/12232/calculating-the-parameters-of-a-beta-distribution-using-the-mean-and-variance\n",
    "\n",
    "mean=np.mean(data)\n",
    "var=np.var(data,ddof=1)\n",
    "a=mean**2*(1-mean)/var-mean\n",
    "b=a*(1-mean)/mean\n",
    "print('alpha', a)\n",
    "print('beta', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(0,max(data),len(data)) # line space of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get R-2\n",
    "\n",
    "y, x = np.histogram(data, bins=20, density=True)\n",
    "\n",
    "# Step 2. Shift the x bin locations to the center of bins.\n",
    "x = (x + np.roll(x, -1))[:-1] / 2.0\n",
    "\n",
    "# Step 3. Calculate the values of pdx(x) for all x.\n",
    "pdf = beta.pdf(x, a, b)\n",
    "\n",
    "# Step 4. Determine the residual and total sum of the squares.\n",
    "ss_error = np.sum(np.power(y - pdf, 2.0))\n",
    "ss_yy = np.sum(np.power(y - y.mean(), 2.0))\n",
    "\n",
    "r_2 = 1 - ( ss_error / ss_yy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(r_2)\n",
    "scipy.stats.kstest(data, 'beta', args=(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = beta.pdf(x, a, b, loc=0, scale=1)\n",
    "plt.hist(data, density = True, bins = 30)\n",
    "# plt.plot(xx, yy)\n",
    "# plt.xlim(xlim)\n",
    "# plt.ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.13782542, 0.04594181, 0.13782542, 0.27565084, 0.04594181,\n",
    "        0.        , 0.13782542, 0.45941807, 0.36753446, 0.36753446,\n",
    "        0.22970904, 0.22970904, 0.68912711, 0.50535988, 0.73506891,\n",
    "        0.82695253, 0.96477795, 0.87289433, 1.47013783, 1.65390505,\n",
    "        2.48085758, 2.94027565, 2.43491577, 2.75650842, 2.89433384,\n",
    "        1.74578867, 1.74578867, 0.96477795, 0.73506891, 1.14854518]\n",
    "x = [0.        , 0.03333333, 0.06666667, 0.1       , 0.13333333,\n",
    "        0.16666667, 0.2       , 0.23333333, 0.26666667, 0.3       ,\n",
    "        0.33333333, 0.36666667, 0.4       , 0.43333333, 0.46666667,\n",
    "        0.5       , 0.53333333, 0.56666667, 0.6       , 0.63333333,\n",
    "        0.66666667, 0.7       , 0.73333333, 0.76666667, 0.8       ,\n",
    "        0.83333333, 0.86666667, 0.9       , 0.93333333, 0.96666667,\n",
    "        1.        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.vstack((x, y[1:])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = KernelDensity(kernel='linear', bandwidth=0.2).fit(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = kde.score_samples(arr)\n",
    "X = list(range(0, len(Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X$,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data, kernel = 'cos', clip = [0.03, .97], gridsize = 200)\n",
    "plt.hist(data, alpha = 0.05, color = 'b', density = True, bins = 30);\n",
    "plt.title('Western Asia')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
