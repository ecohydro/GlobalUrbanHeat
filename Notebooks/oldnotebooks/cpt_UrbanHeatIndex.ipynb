{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban Heat Index\n",
    "\n",
    "By Cascade Tuholske 2020.01.21\n",
    "\n",
    "Notebook is designed to take areal-averaged CHIRTS Tmax for each GHS-UCDB and down-scaled MERRA-2 humidity data and calculate the heat index for each city with a Tmax >80F.\n",
    "\n",
    "[NOAA Heat Index Equation](https://www.wpc.ncep.noaa.gov/html/heatindex_equation.shtml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THINGS I HAVE Coded**\n",
    "- C to F function\n",
    "- Rothfusz regression and adjustments\n",
    "- Steadman's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTES**\n",
    "At first I thought the heat index values for the hottest areas were insane (> 140F), but I spot checked the results and the [NOAA Heat Index Table](https://www.kjrh.com/weather/weather-blog-what-exaclty-is-the-heat-index) simply reds out values where Tmax >40C and RH > 50%. So I guess were are on track ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from random import random\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import geopandas as gpd \n",
    "import glob\n",
    "from statistics import mean\n",
    "import julian\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C_to_F(Tmax_C):\n",
    "    \"Function converts temp in C to F\"\n",
    "    Tmax_F = (Tmax_C * (9/5)) + 32\n",
    "    \n",
    "    return Tmax_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_to_C(Tmax_F):\n",
    "    \"Function converts temp in F to C\"\n",
    "    Tmax_C = (Tmax_F - 32) * (5/9)\n",
    "    \n",
    "    return Tmax_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_xr(file_in, time_dim, space_dim):\n",
    "    \n",
    "    \"\"\" Function reads in a csv w/ GHS-UCDB IDs and temp, isolates the temp\n",
    "    and returns a xarray data array with dims set to city ids and dates\n",
    "    \n",
    "    Args:\n",
    "        file_in = file name and path\n",
    "        time_dim = name for time dim as a str ... use date :-)\n",
    "        space_dim = col name for GHS-UCDB IDs as an str (ID_HDC_G0)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(file_in) # read the file in as a df\n",
    "    print(df.shape)\n",
    "    \n",
    "    df_id = df[space_dim] # get IDs\n",
    "    df = df.iloc[:,3:] # get only temp columns\n",
    "    df.index = df_id # set index values\n",
    "    df_drop = df.dropna() # Drop cities w/ no temp record \n",
    "    print(len(df_drop))\n",
    "    \n",
    "    arr = df_drop.to_numpy() # turn temp cols into an np array\n",
    "    \n",
    "    # make xr Data Array w/ data as temp and dims as spece (e.g. id)\n",
    "    \n",
    "    # Note 2019 09 17 changed to xr.Dataset from xr.Dataarray\n",
    "    xr_da = xr.DataArray(arr, coords=[df_drop.index, df_drop.columns], \n",
    "                            dims=[space_dim, time_dim])\n",
    "    return xr_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatindex(Tmax, RH, unit_in, unit_out):\n",
    "    \n",
    "    \"\"\"Make Heat Index from 2m air and relative humidity following NOAA's guidelines: \n",
    "    https://www.wpc.ncep.noaa.gov/html/heatindex_equation.shtml. It is assumed that the\n",
    "    tempatures and RH are geographically and temporally aligned in the x-arrays and can be stacked\n",
    "    to the funciton.\n",
    "    \n",
    "    --- update as needed cpt 2020.02.17\n",
    "    \n",
    "    Args:\n",
    "        Tmax = x-array of tempatures\n",
    "        RH = x-array of realtive humitity\n",
    "        unit_in = F or C, will convert C to F to apply heat index\n",
    "        unit_out = If C is desired, will convert data to C\n",
    "        \n",
    "    Returns HI\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make all data as float\n",
    "    Tmax = Tmax.astype('float')\n",
    "    RH = RH.astype('float')\n",
    "    \n",
    "    # 1 convert C to F if needed\n",
    "    if unit_in == 'C':\n",
    "        Tmax = C_to_F(Tmax)\n",
    "        \n",
    "    # 2 Apply Steadman's and average with Tmax\n",
    "    USE_STEADMAN = (0.5 * (Tmax + 61.0 + ((Tmax-68.0)*1.2) + (RH*0.094)) + Tmax) / 2 < 80\n",
    "    STEADMAN = USE_STEADMAN * (0.5 * (Tmax + 61.0 + ((Tmax-68.0)*1.2) + (RH*0.094))) #.astype(int)\n",
    "    \n",
    "    # 3 Use Rothfusz if (STEADMAN + Tmax) / 2 > 80\n",
    "    USE_ROTH = (0.5 * (Tmax + 61.0 + ((Tmax-68.0)*1.2) + (RH*0.094)) + Tmax) / 2 > 80\n",
    "    ROTH = USE_ROTH * (-42.379 + 2.04901523*Tmax + 10.14333127*RH - .22475541*Tmax *RH - .00683783*Tmax*Tmax - .05481717*RH*RH + .00122874*Tmax*Tmax*RH + .00085282*Tmax*RH*RH - .00000199*Tmax*Tmax*RH*RH)\n",
    "\n",
    "    # 3 Adjust Roth 1\n",
    "    USE_ADJ1 = (RH < 13) & (Tmax > 80) & (Tmax < 112)\n",
    "    ADJ1_RH = USE_ADJ1 * RH #.astype(int)\n",
    "    ADJ1_RH = ADJ1_RH.where(ADJ1_RH != 0) #ADJ1_RH[ADJ1_RH == 0] = np.nan\n",
    "    ADJ1_Tmax = USE_ADJ1 * Tmax # .astype(int)\n",
    "    ADJ1_Tmax = ADJ1_Tmax.where(ADJ1_Tmax != 0) #ADJ1_Tmax[ADJ1_Tmax == 0] = np.nan\n",
    "    ADJ1 = ((13-ADJ1_RH)/4)*np.sqrt((17-abs(ADJ1_Tmax-95.))/17)\n",
    "    ADJ1 = np.nan_to_num(ADJ1, 0)\n",
    "    \n",
    "    ADJ1_ROTH = ROTH * USE_ADJ1\n",
    "    ADJ1_ROTH = ADJ1_ROTH - ADJ1\n",
    "    \n",
    "    # 4 Adjust Roth 2\n",
    "    USE_ADJ2 = (RH > 85) & (Tmax > 80) & (Tmax < 87)\n",
    "    ADJ2_RH = USE_ADJ2 * RH #.astype(int)\n",
    "    ADJ2_RH = ADJ2_RH.where(ADJ2_RH != 0) #ADJ2_RH[ADJ2_RH == 0] = np.nan\n",
    "    ADJ2_Tmax = USE_ADJ2.astype(int) * Tmax\n",
    "    ADJ2_Tmax = ADJ2_Tmax.where(ADJ2_Tmax != 0) #ADJ2_Tmax[ADJ2_Tmax == 0] = np.nan\n",
    "    ADJ2 = ((ADJ2_RH-85)/10) * ((87-ADJ2_Tmax)/5)\n",
    "    ADJ2 = np.nan_to_num(ADJ2, 0)\n",
    "    \n",
    "    ADJ2_ROTH = ROTH * USE_ADJ2\n",
    "    ADJ2_ROTH = ADJ2_ROTH + ADJ2\n",
    "    \n",
    "    # Roth w/o adjustments\n",
    "    ROTH = ROTH * ~USE_ADJ1 * ~USE_ADJ2\n",
    "    \n",
    "    # sum the stacked arrays\n",
    "    HI = ROTH + STEADMAN + ADJ1_ROTH +  ADJ2_ROTH \n",
    "    \n",
    "    # Convert HI to C if desired\n",
    "    if unit_out == 'C':\n",
    "        HI = F_to_C(HI)\n",
    "    \n",
    "    # return for test\n",
    "    # return STEADMAN, ADJ1_ROTH, ADJ2_ROTH, ROTH, HI\n",
    "    \n",
    "    return HI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_heatindex(DIR_Tmax, DIR_RH, DIR_HI, unit_in, unit_out):\n",
    "    \"\"\"Function applies NOAA's heatindex to two pair directories w/ CSVs of realitive humidity\n",
    "    and tempatures, respective, in a pairwise fashion\n",
    "    \n",
    "    Args:\n",
    "        DIR_Tmax = the directory where Tmax .csv files are stored\n",
    "        DIR_RH = the directory where RH .csv files are stored\n",
    "        DIR_HI = the directory where HI files will be written\n",
    "        unit_in = temp unit for Tmax (C or F)\n",
    "        unit_out = desired temp unit for HI (C or F) for the output\n",
    "    \"\"\"\n",
    "    Tmax_fn_list = glob.glob(DIR_Tmax+'*.csv')\n",
    "    RH_fn_list = glob.glob(DIR_RH+'*.csv')\n",
    "\n",
    "    for Tmax_fn, RH_fn in zip(sorted(Tmax_fn_list),sorted(RH_fn_list)):\n",
    "    \n",
    "        # Check the years RH and Tmax \n",
    "        Tmax_year = Tmax_fn.split('GHS-Tmax-DAILY_')[1].split('.csv')[0]\n",
    "        print('Tmax year is ',Tmax_year)\n",
    "        RH_year = RH_fn.split('GHS-Tmax-RH_')[1].split('.csv')[0]\n",
    "        print('RH year is ', RH_year)\n",
    "\n",
    "        # Read csv as x-array\n",
    "        Tmax_xr = csv_to_xr(Tmax_fn, time_dim = 'date', space_dim = 'ID_HDC_G0')\n",
    "        RH_xr = csv_to_xr(RH_fn, time_dim = 'date', space_dim = 'ID_HDC_G0')\n",
    "\n",
    "        # Make heat index\n",
    "        hi = heatindex(Tmax_xr, RH_xr, unit_in = unit_in, unit_out = unit_out)\n",
    "\n",
    "        # Get countries and city ids\n",
    "        df_out = pd.read_csv(Tmax_fn)\n",
    "        df_out = df_out[['ID_HDC_G0', 'CTR_MN_NM']]\n",
    "\n",
    "        # write to csv\n",
    "        hi_df = hi.to_pandas()\n",
    "        #hi_df['ID_HDC_G0'] = hi_df.index\n",
    "        df_out = df_out.merge(hi_df, on = 'ID_HDC_G0', how = 'inner')\n",
    "        df_out_nm = 'GHS-HI-DAILY_'+Tmax_year+'.csv'\n",
    "        df_out.to_csv(DIR_HI+df_out_nm)\n",
    "        print(RH_year, ' done \\n')\n",
    "    \n",
    "    print('ALL DONE!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tmax year is  1983\n",
      "RH year is  1983\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "1983  done \n",
      "\n",
      "Tmax year is  1984\n",
      "RH year is  1984\n",
      "(13135, 369)\n",
      "13067\n",
      "(13135, 369)\n",
      "13067\n",
      "1984  done \n",
      "\n",
      "Tmax year is  1985\n",
      "RH year is  1985\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "1985  done \n",
      "\n",
      "Tmax year is  1986\n",
      "RH year is  1986\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "1986  done \n",
      "\n",
      "Tmax year is  1987\n",
      "RH year is  1987\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "1987  done \n",
      "\n",
      "Tmax year is  1988\n",
      "RH year is  1988\n",
      "(13135, 369)\n",
      "13067\n",
      "(13135, 369)\n",
      "13067\n",
      "1988  done \n",
      "\n",
      "Tmax year is  1989\n",
      "RH year is  1989\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "1989  done \n",
      "\n",
      "Tmax year is  1990\n",
      "RH year is  1990\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "1990  done \n",
      "\n",
      "Tmax year is  1991\n",
      "RH year is  1991\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "1991  done \n",
      "\n",
      "Tmax year is  1992\n",
      "RH year is  1992\n",
      "(13135, 369)\n",
      "13067\n",
      "(13135, 369)\n",
      "13067\n",
      "1992  done \n",
      "\n",
      "Tmax year is  1993\n",
      "RH year is  1993\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "1993  done \n",
      "\n",
      "Tmax year is  1994\n",
      "RH year is  1994\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "1994  done \n",
      "\n",
      "Tmax year is  1995\n",
      "RH year is  1995\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "1995  done \n",
      "\n",
      "Tmax year is  1996\n",
      "RH year is  1996\n",
      "(13135, 369)\n",
      "13067\n",
      "(13135, 369)\n",
      "13067\n",
      "1996  done \n",
      "\n",
      "Tmax year is  1997\n",
      "RH year is  1997\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "1997  done \n",
      "\n",
      "Tmax year is  1998\n",
      "RH year is  1998\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "1998  done \n",
      "\n",
      "Tmax year is  1999\n",
      "RH year is  1999\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "1999  done \n",
      "\n",
      "Tmax year is  2000\n",
      "RH year is  2000\n",
      "(13135, 369)\n",
      "13067\n",
      "(13135, 369)\n",
      "13067\n",
      "2000  done \n",
      "\n",
      "Tmax year is  2001\n",
      "RH year is  2001\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "2001  done \n",
      "\n",
      "Tmax year is  2002\n",
      "RH year is  2002\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "2002  done \n",
      "\n",
      "Tmax year is  2003\n",
      "RH year is  2003\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "2003  done \n",
      "\n",
      "Tmax year is  2004\n",
      "RH year is  2004\n",
      "(13135, 369)\n",
      "13067\n",
      "(13135, 369)\n",
      "13067\n",
      "2004  done \n",
      "\n",
      "Tmax year is  2005\n",
      "RH year is  2005\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "2005  done \n",
      "\n",
      "Tmax year is  2006\n",
      "RH year is  2006\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "2006  done \n",
      "\n",
      "Tmax year is  2007\n",
      "RH year is  2007\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "2007  done \n",
      "\n",
      "Tmax year is  2008\n",
      "RH year is  2008\n",
      "(13135, 369)\n",
      "13067\n",
      "(13135, 369)\n",
      "13067\n",
      "2008  done \n",
      "\n",
      "Tmax year is  2009\n",
      "RH year is  2009\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "2009  done \n",
      "\n",
      "Tmax year is  2010\n",
      "RH year is  2010\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "2010  done \n",
      "\n",
      "Tmax year is  2011\n",
      "RH year is  2011\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "2011  done \n",
      "\n",
      "Tmax year is  2012\n",
      "RH year is  2012\n",
      "(13135, 369)\n",
      "13067\n",
      "(13135, 369)\n",
      "13067\n",
      "2012  done \n",
      "\n",
      "Tmax year is  2013\n",
      "RH year is  2013\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "2013  done \n",
      "\n",
      "Tmax year is  2014\n",
      "RH year is  2014\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "2014  done \n",
      "\n",
      "Tmax year is  2015\n",
      "RH year is  2015\n",
      "(13135, 368)\n",
      "13067\n",
      "(13135, 368)\n",
      "13067\n",
      "2015  done \n",
      "\n",
      "Tmax year is  2016\n",
      "RH year is  2016\n",
      "(13135, 369)\n",
      "13067\n",
      "(13135, 369)\n",
      "13067\n",
      "2016  done \n",
      "\n",
      "ALL DONE!\n"
     ]
    }
   ],
   "source": [
    "DIR_Tmax = '/home/cascade/projects/UrbanHeat/data/interim/CHIRTS-GHS-DAILY-Tmax/'\n",
    "DIR_RH = '/home/cascade/projects/UrbanHeat/data/interim/CHIRTS-GHS-DAILY-RH/'\n",
    "DIR_HI = '/home/cascade/projects/UrbanHeat/data/interim/CHIRTS-GHS-DAILY-HI/'\n",
    "\n",
    "# FIRE HI\n",
    "apply_heatindex(DIR_Tmax, DIR_RH, DIR_HI, unit_in = 'C', unit_out = 'C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with the conditions above test\n",
    "temp = xr.DataArray([70, 100, 85, 100])\n",
    "rh = xr.DataArray([5, 10, 90, 20])\n",
    "a,b,c,d, e = heatindex(temp, rh, unit_in = 'F', unit_out = 'F')\n",
    "print('Stead ', a)\n",
    "print('Adj1', b)\n",
    "print('Adj2', c)\n",
    "print('Roth', d)\n",
    "print('All HI values', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with actual data from HI chart : https://www.weather.gov/safety/heat-index\n",
    "# Results fit the table\n",
    "\n",
    "temp = xr.DataArray([80, 82, 94, 104])\n",
    "rh = xr.DataArray([40, 40, 40, 40])\n",
    "a,b,c,d, e = heatindex(temp, rh, unit_in = 'F', unit_out = 'F')\n",
    "print(rh[0], temp[0], a)\n",
    "print(rh[0], temp[1], b)\n",
    "print(rh[0], temp[2], c)\n",
    "print(rh[0], temp[3], d)\n",
    "print('All HI values', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test With CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_RH = '/home/cascade/projects/UrbanHeat/data/interim/CHIRTS-GHS-DAILY-RH/'\n",
    "DIR_Tmax = '/home/cascade/projects/UrbanHeat/data/interim/CHIRTS-GHS-DAILY-Tmax/'\n",
    "DIR_HI = '/home/cascade/projects/UrbanHeat/data/interim/CHIRTS-GHS-DAILY-HI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN_RH = 'GHS-Tmax-RH_1984.csv'\n",
    "FN_Tmax = 'GHS-Tmax-DAILY_1984.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13135, 369)\n",
      "13067\n",
      "(13135, 369)\n",
      "13067\n"
     ]
    }
   ],
   "source": [
    "RH = csv_to_xr(DIR_RH+FN_RH, space_dim = 'ID_HDC_G0', time_dim = 'date')\n",
    "Tmax = csv_to_xr(DIR_Tmax+FN_Tmax, space_dim = 'ID_HDC_G0', time_dim = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugh = pd.read_csv(DIR_RH+FN_RH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID_HDC_G0</th>\n",
       "      <th>CTR_MN_NM</th>\n",
       "      <th>1984.01.01</th>\n",
       "      <th>1984.01.02</th>\n",
       "      <th>1984.01.03</th>\n",
       "      <th>1984.01.04</th>\n",
       "      <th>1984.01.05</th>\n",
       "      <th>1984.01.06</th>\n",
       "      <th>1984.01.07</th>\n",
       "      <th>...</th>\n",
       "      <th>1984.12.22</th>\n",
       "      <th>1984.12.23</th>\n",
       "      <th>1984.12.24</th>\n",
       "      <th>1984.12.25</th>\n",
       "      <th>1984.12.26</th>\n",
       "      <th>1984.12.27</th>\n",
       "      <th>1984.12.28</th>\n",
       "      <th>1984.12.29</th>\n",
       "      <th>1984.12.30</th>\n",
       "      <th>1984.12.31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5782</td>\n",
       "      <td>Russia</td>\n",
       "      <td>73.26125</td>\n",
       "      <td>95.588640</td>\n",
       "      <td>82.617790</td>\n",
       "      <td>98.068370</td>\n",
       "      <td>85.623245</td>\n",
       "      <td>85.803970</td>\n",
       "      <td>93.096270</td>\n",
       "      <td>...</td>\n",
       "      <td>89.103000</td>\n",
       "      <td>91.507324</td>\n",
       "      <td>83.534970</td>\n",
       "      <td>83.701180</td>\n",
       "      <td>86.040480</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>86.380646</td>\n",
       "      <td>84.141860</td>\n",
       "      <td>84.683655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3316</td>\n",
       "      <td>Russia</td>\n",
       "      <td>78.29920</td>\n",
       "      <td>77.267720</td>\n",
       "      <td>75.377580</td>\n",
       "      <td>71.599396</td>\n",
       "      <td>72.656975</td>\n",
       "      <td>83.402626</td>\n",
       "      <td>87.227480</td>\n",
       "      <td>...</td>\n",
       "      <td>87.097860</td>\n",
       "      <td>83.475450</td>\n",
       "      <td>76.018845</td>\n",
       "      <td>90.875305</td>\n",
       "      <td>88.562870</td>\n",
       "      <td>86.980260</td>\n",
       "      <td>80.649250</td>\n",
       "      <td>71.785830</td>\n",
       "      <td>86.918040</td>\n",
       "      <td>92.814255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5645</td>\n",
       "      <td>Russia</td>\n",
       "      <td>75.45298</td>\n",
       "      <td>80.180900</td>\n",
       "      <td>71.191290</td>\n",
       "      <td>76.220985</td>\n",
       "      <td>74.785530</td>\n",
       "      <td>75.802470</td>\n",
       "      <td>85.675850</td>\n",
       "      <td>...</td>\n",
       "      <td>83.241470</td>\n",
       "      <td>99.460300</td>\n",
       "      <td>86.276450</td>\n",
       "      <td>80.986660</td>\n",
       "      <td>86.651370</td>\n",
       "      <td>87.038666</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>58.798153</td>\n",
       "      <td>74.991400</td>\n",
       "      <td>75.359690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3185</td>\n",
       "      <td>Finland</td>\n",
       "      <td>86.57464</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>96.645905</td>\n",
       "      <td>88.796890</td>\n",
       "      <td>93.723660</td>\n",
       "      <td>92.737060</td>\n",
       "      <td>88.126305</td>\n",
       "      <td>...</td>\n",
       "      <td>92.221825</td>\n",
       "      <td>94.210785</td>\n",
       "      <td>97.585330</td>\n",
       "      <td>98.801636</td>\n",
       "      <td>96.193380</td>\n",
       "      <td>92.674510</td>\n",
       "      <td>93.881500</td>\n",
       "      <td>90.576410</td>\n",
       "      <td>88.716830</td>\n",
       "      <td>94.450800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3539</td>\n",
       "      <td>Russia</td>\n",
       "      <td>82.58847</td>\n",
       "      <td>83.099075</td>\n",
       "      <td>84.778280</td>\n",
       "      <td>85.860880</td>\n",
       "      <td>84.793810</td>\n",
       "      <td>91.218970</td>\n",
       "      <td>88.119606</td>\n",
       "      <td>...</td>\n",
       "      <td>78.398300</td>\n",
       "      <td>84.420200</td>\n",
       "      <td>82.888150</td>\n",
       "      <td>87.185104</td>\n",
       "      <td>78.806526</td>\n",
       "      <td>84.010660</td>\n",
       "      <td>87.359116</td>\n",
       "      <td>86.407010</td>\n",
       "      <td>83.318504</td>\n",
       "      <td>93.662796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ID_HDC_G0 CTR_MN_NM  1984.01.01  1984.01.02  1984.01.03  \\\n",
       "0           0       5782    Russia    73.26125   95.588640   82.617790   \n",
       "1           1       3316    Russia    78.29920   77.267720   75.377580   \n",
       "2           2       5645    Russia    75.45298   80.180900   71.191290   \n",
       "3           3       3185   Finland    86.57464  100.000000   96.645905   \n",
       "4           4       3539    Russia    82.58847   83.099075   84.778280   \n",
       "\n",
       "   1984.01.04  1984.01.05  1984.01.06  1984.01.07  ...  1984.12.22  \\\n",
       "0   98.068370   85.623245   85.803970   93.096270  ...   89.103000   \n",
       "1   71.599396   72.656975   83.402626   87.227480  ...   87.097860   \n",
       "2   76.220985   74.785530   75.802470   85.675850  ...   83.241470   \n",
       "3   88.796890   93.723660   92.737060   88.126305  ...   92.221825   \n",
       "4   85.860880   84.793810   91.218970   88.119606  ...   78.398300   \n",
       "\n",
       "   1984.12.23  1984.12.24  1984.12.25  1984.12.26  1984.12.27  1984.12.28  \\\n",
       "0   91.507324   83.534970   83.701180   86.040480  100.000000  100.000000   \n",
       "1   83.475450   76.018845   90.875305   88.562870   86.980260   80.649250   \n",
       "2   99.460300   86.276450   80.986660   86.651370   87.038666  100.000000   \n",
       "3   94.210785   97.585330   98.801636   96.193380   92.674510   93.881500   \n",
       "4   84.420200   82.888150   87.185104   78.806526   84.010660   87.359116   \n",
       "\n",
       "   1984.12.29  1984.12.30  1984.12.31  \n",
       "0   86.380646   84.141860   84.683655  \n",
       "1   71.785830   86.918040   92.814255  \n",
       "2   58.798153   74.991400   75.359690  \n",
       "3   90.576410   88.716830   94.450800  \n",
       "4   86.407010   83.318504   93.662796  \n",
       "\n",
       "[5 rows x 369 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ugh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = heatindex(Tmax, RH, unit_in = 'C', unit_out = 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pd = test.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>date</th>\n",
       "      <th>1984.01.01</th>\n",
       "      <th>1984.01.02</th>\n",
       "      <th>1984.01.03</th>\n",
       "      <th>1984.01.04</th>\n",
       "      <th>1984.01.05</th>\n",
       "      <th>1984.01.06</th>\n",
       "      <th>1984.01.07</th>\n",
       "      <th>1984.01.08</th>\n",
       "      <th>1984.01.09</th>\n",
       "      <th>1984.01.10</th>\n",
       "      <th>...</th>\n",
       "      <th>1984.12.22</th>\n",
       "      <th>1984.12.23</th>\n",
       "      <th>1984.12.24</th>\n",
       "      <th>1984.12.25</th>\n",
       "      <th>1984.12.26</th>\n",
       "      <th>1984.12.27</th>\n",
       "      <th>1984.12.28</th>\n",
       "      <th>1984.12.29</th>\n",
       "      <th>1984.12.30</th>\n",
       "      <th>1984.12.31</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_HDC_G0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5782</td>\n",
       "      <td>-19.603278</td>\n",
       "      <td>-16.509366</td>\n",
       "      <td>-17.688222</td>\n",
       "      <td>-15.782093</td>\n",
       "      <td>-20.513494</td>\n",
       "      <td>-20.024169</td>\n",
       "      <td>-19.646038</td>\n",
       "      <td>-20.865014</td>\n",
       "      <td>-23.657549</td>\n",
       "      <td>-23.093398</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.028032</td>\n",
       "      <td>-23.353347</td>\n",
       "      <td>-29.363638</td>\n",
       "      <td>-30.545672</td>\n",
       "      <td>-25.763434</td>\n",
       "      <td>-12.875356</td>\n",
       "      <td>-19.427420</td>\n",
       "      <td>-38.983521</td>\n",
       "      <td>-37.406984</td>\n",
       "      <td>-38.011216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3316</td>\n",
       "      <td>-17.271016</td>\n",
       "      <td>-15.895727</td>\n",
       "      <td>-15.972710</td>\n",
       "      <td>-19.058956</td>\n",
       "      <td>-8.680372</td>\n",
       "      <td>-7.762624</td>\n",
       "      <td>-12.484643</td>\n",
       "      <td>-11.473094</td>\n",
       "      <td>-14.277118</td>\n",
       "      <td>-16.592355</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.660659</td>\n",
       "      <td>-6.635546</td>\n",
       "      <td>-1.218867</td>\n",
       "      <td>0.262298</td>\n",
       "      <td>-0.362172</td>\n",
       "      <td>-4.487592</td>\n",
       "      <td>-7.101775</td>\n",
       "      <td>-11.484543</td>\n",
       "      <td>-3.092274</td>\n",
       "      <td>-1.625348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5645</td>\n",
       "      <td>-13.006575</td>\n",
       "      <td>-13.387874</td>\n",
       "      <td>-16.111549</td>\n",
       "      <td>-16.823017</td>\n",
       "      <td>-16.055264</td>\n",
       "      <td>-10.907404</td>\n",
       "      <td>-11.638051</td>\n",
       "      <td>-20.410790</td>\n",
       "      <td>-18.092921</td>\n",
       "      <td>-18.279321</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.435531</td>\n",
       "      <td>-12.028642</td>\n",
       "      <td>-17.997921</td>\n",
       "      <td>-18.128609</td>\n",
       "      <td>-7.626927</td>\n",
       "      <td>-11.732642</td>\n",
       "      <td>-11.622293</td>\n",
       "      <td>-28.838632</td>\n",
       "      <td>-38.556875</td>\n",
       "      <td>-38.875387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3185</td>\n",
       "      <td>-12.230020</td>\n",
       "      <td>-6.028401</td>\n",
       "      <td>-5.054383</td>\n",
       "      <td>-11.613501</td>\n",
       "      <td>-9.214698</td>\n",
       "      <td>-12.086568</td>\n",
       "      <td>-17.891322</td>\n",
       "      <td>-19.904414</td>\n",
       "      <td>-20.389427</td>\n",
       "      <td>-17.960100</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.138911</td>\n",
       "      <td>-2.682360</td>\n",
       "      <td>0.229404</td>\n",
       "      <td>0.537775</td>\n",
       "      <td>-3.693834</td>\n",
       "      <td>-8.996114</td>\n",
       "      <td>-8.323893</td>\n",
       "      <td>-8.845587</td>\n",
       "      <td>-5.864510</td>\n",
       "      <td>-5.683066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3539</td>\n",
       "      <td>-14.200230</td>\n",
       "      <td>-11.768106</td>\n",
       "      <td>-6.437049</td>\n",
       "      <td>-5.531272</td>\n",
       "      <td>-6.381153</td>\n",
       "      <td>-9.047180</td>\n",
       "      <td>-4.707061</td>\n",
       "      <td>-11.671482</td>\n",
       "      <td>-12.853189</td>\n",
       "      <td>-14.914830</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.467633</td>\n",
       "      <td>-10.850993</td>\n",
       "      <td>-11.458981</td>\n",
       "      <td>-7.923813</td>\n",
       "      <td>-13.172307</td>\n",
       "      <td>-10.326792</td>\n",
       "      <td>-6.703562</td>\n",
       "      <td>-7.223617</td>\n",
       "      <td>-8.078176</td>\n",
       "      <td>-6.381526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13046</td>\n",
       "      <td>17.613376</td>\n",
       "      <td>22.058742</td>\n",
       "      <td>24.395478</td>\n",
       "      <td>25.647603</td>\n",
       "      <td>13.644481</td>\n",
       "      <td>14.314127</td>\n",
       "      <td>18.348333</td>\n",
       "      <td>23.937890</td>\n",
       "      <td>24.927829</td>\n",
       "      <td>17.254387</td>\n",
       "      <td>...</td>\n",
       "      <td>15.221688</td>\n",
       "      <td>18.797521</td>\n",
       "      <td>20.291237</td>\n",
       "      <td>17.869695</td>\n",
       "      <td>17.951916</td>\n",
       "      <td>18.895673</td>\n",
       "      <td>16.726665</td>\n",
       "      <td>18.384778</td>\n",
       "      <td>23.172863</td>\n",
       "      <td>23.966231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1116</td>\n",
       "      <td>22.075079</td>\n",
       "      <td>16.267125</td>\n",
       "      <td>12.688323</td>\n",
       "      <td>19.627985</td>\n",
       "      <td>17.939651</td>\n",
       "      <td>17.380007</td>\n",
       "      <td>17.910172</td>\n",
       "      <td>12.984902</td>\n",
       "      <td>16.819428</td>\n",
       "      <td>16.627256</td>\n",
       "      <td>...</td>\n",
       "      <td>23.879452</td>\n",
       "      <td>24.191684</td>\n",
       "      <td>24.259476</td>\n",
       "      <td>26.468886</td>\n",
       "      <td>27.553832</td>\n",
       "      <td>21.417997</td>\n",
       "      <td>25.000973</td>\n",
       "      <td>27.329142</td>\n",
       "      <td>17.769248</td>\n",
       "      <td>18.438768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1114</td>\n",
       "      <td>14.621860</td>\n",
       "      <td>11.978608</td>\n",
       "      <td>10.111684</td>\n",
       "      <td>11.588975</td>\n",
       "      <td>13.347414</td>\n",
       "      <td>13.904955</td>\n",
       "      <td>12.043949</td>\n",
       "      <td>9.579040</td>\n",
       "      <td>12.948700</td>\n",
       "      <td>11.529167</td>\n",
       "      <td>...</td>\n",
       "      <td>19.486219</td>\n",
       "      <td>20.325655</td>\n",
       "      <td>20.207977</td>\n",
       "      <td>22.711821</td>\n",
       "      <td>20.904695</td>\n",
       "      <td>15.772708</td>\n",
       "      <td>18.978708</td>\n",
       "      <td>17.572250</td>\n",
       "      <td>13.807125</td>\n",
       "      <td>14.189227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1161</td>\n",
       "      <td>16.435140</td>\n",
       "      <td>13.025421</td>\n",
       "      <td>11.186426</td>\n",
       "      <td>10.404007</td>\n",
       "      <td>13.656055</td>\n",
       "      <td>16.519101</td>\n",
       "      <td>13.936635</td>\n",
       "      <td>10.985319</td>\n",
       "      <td>13.015603</td>\n",
       "      <td>12.364984</td>\n",
       "      <td>...</td>\n",
       "      <td>16.614053</td>\n",
       "      <td>18.709732</td>\n",
       "      <td>19.058225</td>\n",
       "      <td>19.568761</td>\n",
       "      <td>21.089036</td>\n",
       "      <td>16.398204</td>\n",
       "      <td>18.190541</td>\n",
       "      <td>19.615014</td>\n",
       "      <td>13.992034</td>\n",
       "      <td>13.808236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1169</td>\n",
       "      <td>14.655994</td>\n",
       "      <td>10.482207</td>\n",
       "      <td>9.821729</td>\n",
       "      <td>12.660409</td>\n",
       "      <td>11.062720</td>\n",
       "      <td>13.647890</td>\n",
       "      <td>11.995908</td>\n",
       "      <td>9.528171</td>\n",
       "      <td>11.919572</td>\n",
       "      <td>10.176583</td>\n",
       "      <td>...</td>\n",
       "      <td>18.596047</td>\n",
       "      <td>18.465220</td>\n",
       "      <td>17.751205</td>\n",
       "      <td>20.361490</td>\n",
       "      <td>18.960489</td>\n",
       "      <td>13.705230</td>\n",
       "      <td>16.227883</td>\n",
       "      <td>17.312927</td>\n",
       "      <td>13.142785</td>\n",
       "      <td>13.427932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13067 rows × 366 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "date       1984.01.01  1984.01.02  1984.01.03  1984.01.04  1984.01.05  \\\n",
       "ID_HDC_G0                                                               \n",
       "5782       -19.603278  -16.509366  -17.688222  -15.782093  -20.513494   \n",
       "3316       -17.271016  -15.895727  -15.972710  -19.058956   -8.680372   \n",
       "5645       -13.006575  -13.387874  -16.111549  -16.823017  -16.055264   \n",
       "3185       -12.230020   -6.028401   -5.054383  -11.613501   -9.214698   \n",
       "3539       -14.200230  -11.768106   -6.437049   -5.531272   -6.381153   \n",
       "...               ...         ...         ...         ...         ...   \n",
       "13046       17.613376   22.058742   24.395478   25.647603   13.644481   \n",
       "1116        22.075079   16.267125   12.688323   19.627985   17.939651   \n",
       "1114        14.621860   11.978608   10.111684   11.588975   13.347414   \n",
       "1161        16.435140   13.025421   11.186426   10.404007   13.656055   \n",
       "1169        14.655994   10.482207    9.821729   12.660409   11.062720   \n",
       "\n",
       "date       1984.01.06  1984.01.07  1984.01.08  1984.01.09  1984.01.10  ...  \\\n",
       "ID_HDC_G0                                                              ...   \n",
       "5782       -20.024169  -19.646038  -20.865014  -23.657549  -23.093398  ...   \n",
       "3316        -7.762624  -12.484643  -11.473094  -14.277118  -16.592355  ...   \n",
       "5645       -10.907404  -11.638051  -20.410790  -18.092921  -18.279321  ...   \n",
       "3185       -12.086568  -17.891322  -19.904414  -20.389427  -17.960100  ...   \n",
       "3539        -9.047180   -4.707061  -11.671482  -12.853189  -14.914830  ...   \n",
       "...               ...         ...         ...         ...         ...  ...   \n",
       "13046       14.314127   18.348333   23.937890   24.927829   17.254387  ...   \n",
       "1116        17.380007   17.910172   12.984902   16.819428   16.627256  ...   \n",
       "1114        13.904955   12.043949    9.579040   12.948700   11.529167  ...   \n",
       "1161        16.519101   13.936635   10.985319   13.015603   12.364984  ...   \n",
       "1169        13.647890   11.995908    9.528171   11.919572   10.176583  ...   \n",
       "\n",
       "date       1984.12.22  1984.12.23  1984.12.24  1984.12.25  1984.12.26  \\\n",
       "ID_HDC_G0                                                               \n",
       "5782       -23.028032  -23.353347  -29.363638  -30.545672  -25.763434   \n",
       "3316        -1.660659   -6.635546   -1.218867    0.262298   -0.362172   \n",
       "5645       -14.435531  -12.028642  -17.997921  -18.128609   -7.626927   \n",
       "3185        -1.138911   -2.682360    0.229404    0.537775   -3.693834   \n",
       "3539        -9.467633  -10.850993  -11.458981   -7.923813  -13.172307   \n",
       "...               ...         ...         ...         ...         ...   \n",
       "13046       15.221688   18.797521   20.291237   17.869695   17.951916   \n",
       "1116        23.879452   24.191684   24.259476   26.468886   27.553832   \n",
       "1114        19.486219   20.325655   20.207977   22.711821   20.904695   \n",
       "1161        16.614053   18.709732   19.058225   19.568761   21.089036   \n",
       "1169        18.596047   18.465220   17.751205   20.361490   18.960489   \n",
       "\n",
       "date       1984.12.27  1984.12.28  1984.12.29  1984.12.30  1984.12.31  \n",
       "ID_HDC_G0                                                              \n",
       "5782       -12.875356  -19.427420  -38.983521  -37.406984  -38.011216  \n",
       "3316        -4.487592   -7.101775  -11.484543   -3.092274   -1.625348  \n",
       "5645       -11.732642  -11.622293  -28.838632  -38.556875  -38.875387  \n",
       "3185        -8.996114   -8.323893   -8.845587   -5.864510   -5.683066  \n",
       "3539       -10.326792   -6.703562   -7.223617   -8.078176   -6.381526  \n",
       "...               ...         ...         ...         ...         ...  \n",
       "13046       18.895673   16.726665   18.384778   23.172863   23.966231  \n",
       "1116        21.417997   25.000973   27.329142   17.769248   18.438768  \n",
       "1114        15.772708   18.978708   17.572250   13.807125   14.189227  \n",
       "1161        16.398204   18.190541   19.615014   13.992034   13.808236  \n",
       "1169        13.705230   16.227883   17.312927   13.142785   13.427932  \n",
       "\n",
       "[13067 rows x 366 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pd['1984.07.01'].sort_values(ascending = False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dir Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files from both RH and temp\n",
    "\n",
    "DIR_Tmax = '/home/cascade/projects/UrbanHeat/data/interim/CHIRTS-GHS-DAILY-Tmax/'\n",
    "DIR_RH = '/home/cascade/projects/UrbanHeat/data/interim/CHIRTS-GHS-DAILY-RH/'\n",
    "DIR_HI = '/home/cascade/projects/UrbanHeat/data/interim/CHIRTS-GHS-DAILY-HI/'\n",
    "\n",
    "print(DIR_Tmax)\n",
    "print(DIR_RH) \n",
    "\n",
    "Tmax_fn_list = glob.glob(DIR_Tmax+'*.csv')\n",
    "RH_fn_list = glob.glob(DIR_RH+'*.csv')\n",
    "\n",
    "for Tmax_fn, RH_fn in zip(sorted(Tmax_fn_list),sorted(RH_fn_list)):\n",
    "    \n",
    "    # Check the years RH and Tmax \n",
    "    Tmax_year = Tmax_fn.split('GHS-Tmax-DAILY_')[1].split('.csv')[0]\n",
    "    print('Tmax year is ',Tmax_year)\n",
    "    RH_year = RH_fn.split('GHS-Tmax-RH_')[1].split('.csv')[0]\n",
    "    print('RH year is ', RH_year)\n",
    "    \n",
    "    # Read csv as x-array\n",
    "    Tmax_xr = csv_to_xr(Tmax_fn, time_dim = 'date', space_dim = 'ID_HDC_G0')\n",
    "    RH_xr = csv_to_xr(RH_fn, time_dim = 'date', space_dim = 'ID_HDC_G0')\n",
    "    \n",
    "    # Make heat index\n",
    "    hi = heatindex(Tmax_xr, RH_xr, unit_in = 'C', unit_out = 'F')\n",
    "    \n",
    "\n",
    "    # CASCADE GO LOOK AT HOW X-ARRAYS ARE WRITTEN TO CSVS IN EARLIER CODE <<<<---- \n",
    "    \n",
    "    # write to csv\n",
    "    df = hi.to_pandas()\n",
    "    df_out_nm = 'GHS-HI-DAILY_'+Tmax_year+'.csv'\n",
    "    df.to_csv(DIR_HI+df_out_nm)\n",
    "    print(RH_year, ' done \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check HI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get GHS-UCDB\n",
    "ghs = gpd.read_file('/home/cascade/projects/UrbanHeat/data/raw/GHS_UCDB/GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_0.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = gpd.GeoDataFrame()\n",
    "out['geometry'] = ghs['geometry']\n",
    "out['ID_HDC_G0'] = ghs['ID_HDC_G0']\n",
    "\n",
    "HI_out = gpd.GeoDataFrame()\n",
    "HI_out['ID_HDC_G0'] = df.index\n",
    "HI_out['2016.07.01'] = df['2016.07.01']\n",
    "\n",
    "out = pd.merge(out, HI_out, on = 'ID_HDC_G0', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out.to_file('/home/cascade/projects/UrbanHeat/HI_20160701_test.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check hotcheck values \n",
    "\n",
    "Tmax =  pd.read_csv(Tmax_fn)\n",
    "RH = pd.read_csv(RH_fn)\n",
    "\n",
    "check = pd.DataFrame()\n",
    "check['2016.07.01.HI'] = df['2016.07.01'].sort_values(ascending = False).head(50)\n",
    "check = check.merge(Tmax[['ID_HDC_G0', '2016.07.01']], on = 'ID_HDC_G0', how = 'inner')\n",
    "check.rename(columns = {'2016.07.01':'2016.07.01.Tmax'}, inplace = True)\n",
    "check = check.merge(RH[['ID_HDC_G0', '2016.07.01']], on = 'ID_HDC_G0', how = 'inner')\n",
    "check.rename(columns = {'2016.07.01':'2016.07.01.RH'}, inplace = True)\n",
    "# check['2016.07.01.RH'] = RH['2016.07.01'].sort_values(ascending = False).head(50)\n",
    "\n",
    "\n",
    "# df['2016.07.01'].sort_values(ascending = False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkTmax = xr.DataArray(check['2016.07.01.Tmax'].to_numpy())\n",
    "checkRH = xr.DataArray(check['2016.07.01.RH'].to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check the heat index\n",
    "checkHI = heatindex(checkTmax, checkRH, unit_in = 'C', unit_out = 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with the conditions above test\n",
    "temp = xr.DataArray([42, 41, 40, 39])\n",
    "rh = xr.DataArray([52, 52, 52, 52])\n",
    "heatindex(temp, rh, unit_in = 'C', unit_out = 'F')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Well it makes sense\n",
    "\n",
    "These crazy high values make sense when checked with NOAA's table. But once you have a Tmax >40C and RH > 40%, the HI values is crazy high ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_xr(file_in, time_dim, space_dim):\n",
    "    \n",
    "    \"\"\" Function reads in a csv w/ GHS-UCDB IDs and temp, isolates the temp\n",
    "    and returns a xarray data array with dims set to city ids and dates\n",
    "    \n",
    "    Args:\n",
    "        file_in = file name and path\n",
    "        time_dim = name for time dim as a str ... use date :-)\n",
    "        space_dim = col name for GHS-UCDB IDs as an str (ID_HDC_G0)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(file_in) # read the file in as a df\n",
    "    print(df.shape)\n",
    "    \n",
    "    df_id = df[space_dim] # get IDs\n",
    "    df = df.iloc[:,3:] # get only temp columns\n",
    "    df.index = df_id # set index values\n",
    "    df_drop = df.dropna() # Drop cities w/ no temp record \n",
    "    print(len(df_drop))\n",
    "    \n",
    "    arr = df_drop.to_numpy() # turn temp cols into an np array\n",
    "    \n",
    "    # make xr Data Array w/ data as temp and dims as spece (e.g. id)\n",
    "    \n",
    "    # Note 2019 09 17 changed to xr.Dataset from xr.Dataarray\n",
    "    temp_xr_da = xr.DataArray(arr, coords=[df_temp_drop.index, df_temp_drop.columns], \n",
    "                            dims=[space_dim, time_dim])\n",
    "    \n",
    "    return xr_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #2 Function finds all the Tmax Events and writes it to a dateframe w/ dates for each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmax_days(xarray, Tthresh):\n",
    "    \"\"\" Function finds all the tmax days in a year and sums total days per year \n",
    "    greater than a threshold within a year where Tmax > Tthresh for each city. Returns the total number of days,\n",
    "    the dates, the tempatures, and the intensity (daily Tmax - Tthresh)\n",
    "    \n",
    "    Args: \n",
    "        xarray = an xarray object with dims = (space, times)\n",
    "        Tthresh = int of temp threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty lists & df\n",
    "    id_list = []\n",
    "    date_list = []\n",
    "    dayTot_list = []\n",
    "    tmax_list = []\n",
    "    intensity_list = []\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    # subset xarray\n",
    "    out = xarray.where(xarray > Tthresh, drop = True)\n",
    "\n",
    "    # start loop \n",
    "    for index, loc in enumerate(out.ID_HDC_G0):\n",
    "        id_list.append(out.ID_HDC_G0.values[index]) # get IDS\n",
    "        date_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values) # get event dates\n",
    "        \n",
    "        # this is actually getting the total events of all, 2019-09-22\n",
    "        dayTot_list.append(len(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values)) # get event totals\n",
    "        \n",
    "        tmax_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').values) # get temp values\n",
    "        intensity_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').values - Tthresh) # get severity\n",
    "\n",
    "    # write to a data frame\n",
    "    df_out['ID_HDC_G0'] = id_list\n",
    "    df_out['total_days'] = dayTot_list\n",
    "    df_out['dates'] = date_list\n",
    "    df_out['tmax'] = tmax_list\n",
    "    df_out['tmax_tntensity'] = intensity_list\n",
    "\n",
    "    # return df_out\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #3 Function splits the dataset into Tmax events (continuous days >Tmax) for each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jul_convert(dates):\n",
    "    \"Function turn days into julian datetime\"\n",
    "    jul_days = pd.to_datetime(dates).to_julian_date()\n",
    "    \n",
    "    return jul_days\n",
    "\n",
    "def event_split(dates, ID_HDC_G0, intensity, tmax, total_days):\n",
    "    \"\"\" Searchs a list of dates and isolates sequential dates as a list, then calculates event stats.\n",
    "    See comments in code for more details. \n",
    "    \n",
    "    Args:\n",
    "        dates: pandas.core.index as julian dates\n",
    "        ID_HDC_G0: city ID as string\n",
    "        country: country for each city as string\n",
    "        intensity: numpy.ndarray of intensities values\n",
    "        tmax: numpy.ndarray of intensities values of tmax values\n",
    "        total_days: total number of tmax days in a year for a given city\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # city id\n",
    "    city_id = ID_HDC_G0\n",
    "    tot_days = total_days\n",
    "    \n",
    "    # lists to fill\n",
    "    city_id_list = []\n",
    "    tot_days_list = []\n",
    "    event_dates_list = []\n",
    "    dur_list = []\n",
    "    intensity_list = []\n",
    "    tmax_list = []\n",
    "    avg_temp_list = []\n",
    "    avg_int_list = []\n",
    "    tot_int_list = []\n",
    "    \n",
    "    # data frame out\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    # turn days into julian days\n",
    "    jul_days = jul_convert(dates)\n",
    "    \n",
    "    # Counters to make sure we write the correct event dates to a list, don't want julian days in output\n",
    "    counter = 0\n",
    "    start = 0\n",
    "    end = 0\n",
    "    \n",
    "    # Loop through dur list and isolate seq days, temps, and intensities\n",
    "    for k, g in groupby(enumerate(jul_days.values), lambda x: x[1]-x[0]):\n",
    "        \n",
    "        seq = list(map(itemgetter(1), g)) # isolate seq. days\n",
    "        dur = len(seq) # duration of each event\n",
    "        \n",
    "        counter = counter + dur # add duration to counter\n",
    "        end = counter # end of current event\n",
    "        \n",
    "        event_dates = dates[start:end] # dates of tmax days during each event\n",
    "        intense = intensity[start:end] # intensity of each day during event\n",
    "        temp = tmax[start:end] # temp of each day during event\n",
    "        avg_temp = mean(temp) # avg. temp during event\n",
    "        avg_int = mean(intense) # avg. intensity during event\n",
    "        tot_int = intense.sum() # total intensity during event\n",
    "        \n",
    "        start = counter # reset start to current end (e.g. counter)\n",
    "        \n",
    "        # fill lists\n",
    "        city_id_list.append(city_id)\n",
    "        tot_days_list.append(tot_days)\n",
    "        dur_list.append(dur)\n",
    "        event_dates_list.append(event_dates)\n",
    "        intensity_list.append(intense)\n",
    "        tmax_list.append(temp)\n",
    "        avg_temp_list.append(avg_temp)\n",
    "        avg_int_list.append(avg_int)\n",
    "        tot_int_list.append(tot_int)\n",
    "\n",
    "    # write out as a dateframe\n",
    "    df_out['ID_HDC_G0'] = city_id_list\n",
    "    df_out['total_days'] = tot_days_list\n",
    "    df_out['duration'] = dur_list\n",
    "    df_out['avg_temp'] = avg_temp_list\n",
    "    df_out['avg_intensity'] = avg_int_list\n",
    "    df_out['tot_intensity'] = tot_int_list\n",
    "    df_out['event_dates'] = event_dates_list\n",
    "    df_out['duration'] = dur_list\n",
    "    df_out['intensity'] = intensity_list\n",
    "    df_out['tmax'] = tmax_list\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #4 Function feeds output from function 2 into function 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmax_stats(df_in):\n",
    "    \"\"\" runs event_split functionon a dataframe to produce desired tmax stats\n",
    "    \n",
    "        NOTE - If you add arguments to event_split to make more states, \n",
    "        be sure to update this function\n",
    "    \n",
    "        args:\n",
    "            df: input dataframe\n",
    "        \n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    # NOTE - If you add arguments to event_split to make more stats, \n",
    "    # be sure to update this function\n",
    "    \n",
    "    for index, row in df_in.iterrows():\n",
    "        dates = row['dates'] # Get event dates\n",
    "        intensity = row['tmax_tntensity'] # Get intensity for each day\n",
    "        tmax = row['tmax'] # Get tmax for each day\n",
    "        ID_HDC_G0 = row['ID_HDC_G0'] # get city id\n",
    "        total_days = row['total_days'] # get total number of tmax days\n",
    "\n",
    "        df = event_split(dates, ID_HDC_G0, intensity, tmax, total_days)\n",
    "\n",
    "        df_out = df_out.append(df)\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #5 Loops through a file list and applies functions 1 - 4 to the data to produce Tmax stats for all tmax events in a given year across all cities in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_loop(dir_in, dir_out, fn_out, time_dim, space_dim, Tthresh):\n",
    "    \n",
    "    \"\"\" Loop through a dir with csvs to apply csv_to_xr and\n",
    "    tmax_stats function and save out a .csv for each year\n",
    "    \n",
    "    Args:\n",
    "        dir_in = dir path to loop through\n",
    "        dir_out = dir path to save files out\n",
    "        fn_out = string to label out files\n",
    "        time_dim = name for time dim as a str ... use date :-) for csv_to_xr function\n",
    "        space_dim = col name for GHS-UCDB IDs as an str (ID_HDC_G0) for csv_to_xr function\n",
    "        Tthresh = int of temp threshold for temp_event function -- 40.6 is used\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the GHS-ID List with GeoPANDAS read_file\n",
    "    ghs_ids_fn = 'GHS-UCSB-IDS.csv'\n",
    "    ghs_ids_df = pd.read_csv(DATA_INTERIM+ghs_ids_fn)\n",
    "        \n",
    "    # Git File list\n",
    "    fn_list = glob.glob(dir_in+'*.csv')\n",
    "    \n",
    "    for fn in sorted(fn_list):\n",
    "        \n",
    "        # Get year for arg for temp_event function\n",
    "        year = fn.split('GHS-Tmax-DAILY_')[1].split('.csv')[0]\n",
    "        print(year)\n",
    "        \n",
    "        # read csv as a data array\n",
    "        temp_xr_da = csv_to_xr(fn, time_dim, space_dim)\n",
    "        \n",
    "        # data array to tmax events, out as df\n",
    "        df_days = tmax_days(temp_xr_da, Tthresh)\n",
    "        \n",
    "        # tmax events stats, out as df\n",
    "        df_out = tmax_stats(df_days)\n",
    "        \n",
    "        # merge to get countries\n",
    "        ghs_ids_df_out = ghs_ids_df.merge(df_out, on='ID_HDC_G0', how = 'inner') \n",
    "        \n",
    "        # write it all out\n",
    "        ghs_ids_df_out.to_csv(dir_out+fn_out+year+'.csv')\n",
    "\n",
    "        print(year, 'SAVED!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_in = '/home/cascade/projects/data_out_urbanheat/CHIRTS-GHS-DAILY/' # output from avg temp\n",
    "DATA_INTERIM = '/home/cascade/projects/UrbanHeat/data/interim/' # ghs ID list\n",
    "dir_out = '/home/cascade/projects/data_out_urbanheat/CHIRTS-GHS-Events-Stats/'\n",
    "fn_out = 'CHIRTS-GHS-Events-Stats'\n",
    "time_dim = 'date'\n",
    "space_dim = 'ID_HDC_G0'\n",
    "Tthresh = 40.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stats_loop(dir_in, dir_out, fn_out, time_dim, space_dim, Tthresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA/QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "\n",
    "dir_out = '/home/cascade/projects/data_out_urbanheat/CHIRTS-GHS-Events-Stats/'\n",
    "fn_out = 'CHIRTS-GHS-Events-Stats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_df = pd.read_csv(dir_out+fn_out+'1983.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_city = qc_df[qc_df['ID_HDC_G0'] == 5534]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths\n",
    "DAILY_PATH = '/home/cascade/projects/data_out_urbanheat/CHIRTS-GHS-DAILY/' # output from avg temp\n",
    "DATA_INTERIM = '/home/cascade/projects/data_out_urbanheat/data/interim/'\n",
    "DATA_OUT = '/home/cascade/projects/data_out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name to test\n",
    "fn_in = 'GHS-Tmax-DAILY_1983.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open a raw file\n",
    "xr1983 = csv_to_xr(DAILY_PATH+fn_in, 'date', 'ID_HDC_G0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days1983 = tmax_days(xr1983, 40.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = days1983[0:30]\n",
    "test\n",
    "\n",
    "# Maybe add in days_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build routine for loop through a csv\n",
    "\n",
    "df_out = pd.DataFrame()\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    dates = row['dates'] # Get event dates\n",
    "    intensity = row['tmax_tntensity'] # Get intensity for each day\n",
    "    tmax = row['tmax'] # Get tmax for each day\n",
    "    ID_HDC_G0 = row['ID_HDC_G0'] # get city id\n",
    "    total_days = row['total_days'] # get total number of tmax days\n",
    "    \n",
    "    df = event_split(dates, ID_HDC_G0, intensity, tmax, total_days)\n",
    "    \n",
    "    df_out = df_out.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing code\n",
    "2019.10.19 Cascade Tuholkse\n",
    "\n",
    "Need to fix ```event split``` function\n",
    "\n",
    "Somewhere in 1984 is this event sequence: ['1984.01.01' '1984.01.02' '1984.01.07']\n",
    "\n",
    "\n",
    "**FOUND PROBLEM AND IT IS FIXED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths\n",
    "DAILY_PATH = '/home/cascade/projects/data_out_urbanheat/CHIRTS-GHS-DAILY/' # output from avg temp\n",
    "DATA_INTERIM = '/home/cascade/projects/data_out_urbanheatv/data/interim/'\n",
    "DATA_OUT = '/home/cascade/projects/data_out_urbanheat/testout/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name to test\n",
    "fn_in = 'GHS-Tmax-DAILY_1983.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open a raw file\n",
    "xr1983 = csv_to_xr(DAILY_PATH+fn_in, 'date', 'ID_HDC_G0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the tmax days\n",
    "tmax1983 = tmax_days(xr1983, 40.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a subset\n",
    "\n",
    "test = tmax1983[tmax1983['ID_HDC_G0'] == 6279]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test.iterrows():\n",
    "    dates = row['dates'] # Get event dates\n",
    "    intensity = row['tmax_tntensity'] # Get intensity for each day\n",
    "    tmax = row['tmax'] # Get tmax for each day\n",
    "    ID_HDC_G0 = row['ID_HDC_G0'] # get city id\n",
    "    total_days = row['total_days'] # get total number of tmax days\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to jul days\n",
    "jul_days = pd.to_datetime(dates).to_julian_date()\n",
    "jul_days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mjd = 2445507.5 - 43200\n",
    "dt = julian.from_jd(mjd, fmt='mjd')\n",
    "print(dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['1983.06.20', '1983.06.23', '1983.06.24', '1983.06.25',\n",
    "        '1983.06.26', '1983.06.27', '1983.06.28', '1983.06.29',\n",
    "        '1983.06.30', '1983.07.01', '1983.07.21', '1983.07.22',\n",
    "        '1983.07.23', '1983.08.01']\n",
    "\n",
    "pd_dates = pd.to_datetime(dates)\n",
    "df_dates = pd.DataFrame()\n",
    "df_dates['dates'] = pd_dates\n",
    "\n",
    "\n",
    "\n",
    "test = df_dates['dates'].apply(lambda x: x.toordinal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "date.fromordinal(test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dates = dates[0:10] # dates of tmax days during each event\n",
    "print(event_dates)\n",
    "print(len(event_dates))\n",
    "\n",
    "event_dates = dates[10:13] # dates of tmax days during each event\n",
    "print(event_dates)\n",
    "print(len(event_dates))\n",
    "\n",
    "event_dates = dates[13:14] # dates of tmax days during each event\n",
    "print(event_dates)\n",
    "print(len(event_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dates_list = []\n",
    "counter = 0\n",
    "start = 0\n",
    "end = 0\n",
    "\n",
    "for k, g in groupby(enumerate(jul_days.values), lambda x: x[1]-x[0]):\n",
    "    \n",
    "    len_dates = len(dates) # len of all Tmax dates for a given city\n",
    "#   print(len(dates))\n",
    "    \n",
    "    seq = list(map(itemgetter(1), g)) # isolate seq. days\n",
    "    dur = len(seq) # duration of each event \n",
    "    \n",
    "    counter = counter + dur\n",
    "    end = counter\n",
    "    \n",
    "    print(end)\n",
    "    \n",
    "    event_dates = dates[start:end] # dates of tmax days during each event\n",
    "    print(event_dates)\n",
    "    \n",
    "    start = counter\n",
    "    \n",
    "# #     print('dur= ', dur)\n",
    "    \n",
    "#     event_dates = dates[0:10] # dates of tmax days during each event\n",
    "#     print(event_dates)\n",
    "    \n",
    "#     event_dates = dates[11:13] # dates of tmax days during each event\n",
    "#     print(event_dates)\n",
    "    \n",
    "#     event_dates = dates[:len_dates] # dates of tmax days during each event\n",
    "#     print(event_dates)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     counter = counter + dur\n",
    "\n",
    "#     print('counter = ', counter)\n",
    "    \n",
    "#     dif = dur - counter\n",
    "#     print('dif = ', dif)\n",
    "    \n",
    "#     start = start \n",
    "#     print('start = ',start)\n",
    "    \n",
    "# #     end = counter + dur\n",
    "# #     print(\"start = \",end)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     counter = counter + dur\n",
    "#     print(counter)\n",
    "#     end = counter + dur\n",
    "#     start\n",
    "#     event_dates = dates[start:end] # dates of tmax days during each event\n",
    "#     print(event_dates)\n",
    "# #     intense = intensity[0:dur] # intensity of each day during event\n",
    "#     temp = tmax[0:dur] # temp of each day during event\n",
    "#     avg_temp = mean(temp) # avg. temp during event\n",
    "#     avg_int = mean(intense) # avg. intensity during event\n",
    "#     tot_int = intense.sum() # total intensity during event\n",
    "    \n",
    "#     event_dates_list.append(event_dates)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['event_dates'] = event_dates_list\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import more_itertools as mit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tmax stats ------> CLEARLY NOTE WORKING\n",
    "\n",
    "# tmax1983_sub_stats = tmax_stats(tmax1983_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tmax1983['dates'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = tmax1983_sub['dates']\n",
    "#jul_days = pd.to_datetime(tmax1983['dates']).to_julian_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to break up events\n",
    "\n",
    "for index, row in tmax1983_sub.iterrows():\n",
    "    dates = row['dates'] # Get event dates\n",
    "    intensity = row['tmax_tntensity'] # Get intensity for each day\n",
    "    tmax = row['tmax'] # Get tmax for each day\n",
    "    ID_HDC_G0 = row['ID_HDC_G0'] # get city id\n",
    "    total_days = row['total_days'] # get total number of tmax days\n",
    "\n",
    "#     df = event_split(dates, ID_HDC_G0, intensity, tmax, total_days)\n",
    "\n",
    "#     df_out = df_out.append(df)\n",
    "\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_out['events'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Heat Index\n",
    "\n",
    "Get some paired values from the [NOAA CHART](https://www.weather.gov/safety/heat-index) and test the output. \n",
    "\n",
    "These results seem to be working well, though 104 F and 60 RH produced a higher number than the chart ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [25, 27, 35, 40, 41] # temp C\n",
    "RH = 60\n",
    "test = pd.DataFrame()\n",
    "test['temp'] = temp\n",
    "test['RH'] = RH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, row in test.iterrows():\n",
    "    out = make_hi(row['temp'], row['RH'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the temp x array from C to F\n",
    "\n",
    "temp_xr_da = C_to_F(temp_xr_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply steadman's\n",
    "\n",
    "HI_steadman = steadman_hi(temp_xr_da, RH_xr_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## this doesn't work \n",
    "#HI_rothfusz = rothfusz_hi(HI_steadman, temp_xr_da, RH_xr_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C_to_F(Tmax_C):\n",
    "    \"Function converts temp in c to f\"\n",
    "    Tmax_F = (Tmax_C * (9/5)) + 32\n",
    "    \n",
    "    return Tmax_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steadman_hi(Tmax_F, RH):\n",
    "    \"Simple heat index calculation\"\n",
    "    \n",
    "    HI_steadman = 0.5 * (Tmax_F + 61.0 + ((Tmax_F-68.0)*1.2) + (RH*0.094))\n",
    "    \n",
    "    HI_steadman = (HI_steadman + Tmax_F) / 2\n",
    "    \n",
    "    return HI_steadman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rothfusz_hi(HI_steadman, Tmax_F, RH):\n",
    "    \n",
    "    \"Heat Index applied to Steadman's heat index >80F\"\n",
    "    \n",
    "    if (HI_steadman > 80):\n",
    "    \n",
    "        HI_rothfusz = -42.379 + 2.04901523*Tmax_F + 10.14333127*RH - .22475541*Tmax_F*RH - .00683783*Tmax_F*Tmax_F - .05481717*RH*RH + .00122874*Tmax_F*Tmax_F*RH + .00085282*Tmax_F*RH*RH - .00000199*Tmax_F*Tmax_F*RH*RH\n",
    "    \n",
    "    if (RH < 13) & (Tmax_F > 80) & (Tmax_F < 112):\n",
    "        adjustment = ((13-RH)/4)*math.sqrt((17-abs(Tmax_F-95))/17)\n",
    "        HI_rothfusz = HI_rothfusz - adjustment \n",
    "    \n",
    "    \"If the RH is greater than 85% and the temperature is between 80 and 87 degrees F\" \n",
    "    if (RH > 85) & (Tmax_F > 80) & (Tmax_F < 87):\n",
    "        adjustment = ((RH-85)/10) * ((87-Tmax_F)/5)\n",
    "        HI_rothfusz = HI_rothfusz + adjustment \n",
    "    \n",
    "    return HI_rothfusz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hi(Tmax_F, RH):\n",
    "    \"Calculates the heat index for a CHIRTS Tmax value\"\n",
    "    Tmax_F = C_to_F(Tmax_C)\n",
    "#     print('F is ', Tmax_F)\n",
    "#     print('H is ', RH)\n",
    "    HI_steadman = steadman_hi(Tmax_F, RH)\n",
    "#     print('HI_steadman is ', HI_steadman)\n",
    "    HI_rothfusz = rothfusz_hi(HI_steadman, Tmax_F, RH)\n",
    "#     print('HI_rothfusz is ', HI_rothfusz)\n",
    "    \n",
    "    return HI_rothfusz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_RH = '/home/cascade/projects/UrbanHeat/data/interim/CHIRTS-GHS-DAILY-RH/'\n",
    "DIR_RAW = '/home/cascade/projects/UrbanHeat/data/interim/CHIRTS-GHS-DAILY-TEMP/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN_RH = 'GHS-Tmax-RH_1983.csv'\n",
    "FN_RAW = 'GHS-Tmax-DAILY_1983.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RH = pd.read_csv(DIR_RH+FN_RH)\n",
    "RAW = pd.read_csv(DIR_RAW+FN_RAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RH = csv_to_xr(DIR_RH+FN_RH, time_dim = 'date', space_dim = 'ID_HDC_G0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_id = RAW['ID_HDC_G0'] # get IDs\n",
    "df_temp = RAW.iloc[:,3:] # get only temp columns\n",
    "df_temp.index = df_temp_id # set index values\n",
    "df_temp_drop = df_temp.dropna() # Drop cities w/ no temp record \n",
    "print(len(df_temp_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RH_id = RH['ID_HDC_G0'] # get IDs\n",
    "df_RH = RH.iloc[:,3:] # get only temp columns\n",
    "df_RH.index = df_RH_id # set index values\n",
    "df_RH_drop = df_RH.dropna() # Drop cities w/ no temp record \n",
    "print(len(df_RH_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try np arrays\n",
    "# temp_arr = df_temp_drop.to_numpy()\n",
    "# rh_arr = df_RH_drop.to_numpy()\n",
    "\n",
    "# Make them into a data array\n",
    "temp_xr_da = xr.DataArray(df_temp_drop, coords=[df_temp_drop.index, df_temp_drop.columns], \n",
    "                            dims=['ID_HDC_G0', 'date'])\n",
    "RH_xr_da = xr.DataArray(df_RH_drop, coords=[df_RH_drop.index, df_RH_drop.columns], \n",
    "                            dims=['ID_HDC_G0', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = hi(temp_xr_da, RH_xr_da, 'C') # STILL THROWS ERROR FIX IT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leap Year Load Issue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
