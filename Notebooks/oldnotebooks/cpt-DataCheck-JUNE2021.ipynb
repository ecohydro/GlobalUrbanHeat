{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "driving-marking",
   "metadata": {},
   "source": [
    "# Data Check\n",
    "\n",
    "This notebook was created in June 2021 to double check the data for the MS before I submit it.\n",
    "\n",
    "1. First check WBGT32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cosmetic-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "solar-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/cascade/projects/UrbanHeat/data/processed/PNAS-DATA-v2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hydraulic-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at WBGT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incorporate-dominant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/cascade/projects/UrbanHeat/data/processed/PNAS-DATA-v2/WBGT32_1D_EXP-COUNTRIES.csv',\n",
       " '/home/cascade/projects/UrbanHeat/data/processed/PNAS-DATA-v2/WBGT32_1D_STATS.json',\n",
       " '/home/cascade/projects/UrbanHeat/data/processed/PNAS-DATA-v2/WBGT32_1D_TREND_ALL.json',\n",
       " '/home/cascade/projects/UrbanHeat/data/processed/PNAS-DATA-v2/WBGT32_1D_TREND_EXP05.json',\n",
       " '/home/cascade/projects/UrbanHeat/data/processed/PNAS-DATA-v2/WBGT32_1D_TREND_HEATP05.json',\n",
       " '/home/cascade/projects/UrbanHeat/data/processed/PNAS-DATA-v2/WBGT32_1D_EXP-REGION.csv',\n",
       " '/home/cascade/projects/UrbanHeat/data/processed/PNAS-DATA-v2/WBGT32_1D_EXP-TOP50.csv',\n",
       " '/home/cascade/projects/UrbanHeat/data/processed/PNAS-DATA-v2/WBGT32_1D_EXP.json']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wbgt32 = glob.glob(path+'WBGT32*')\n",
    "wbgt32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "proof-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with stats\n",
    "\n",
    "stats = pd.read_json(path+'WBGT32_1D_STATS.json', orient = 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "allied-budapest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_HDC_G0</th>\n",
       "      <th>year</th>\n",
       "      <th>duration</th>\n",
       "      <th>avg_temp</th>\n",
       "      <th>avg_intensity</th>\n",
       "      <th>tot_intensity</th>\n",
       "      <th>event_dates</th>\n",
       "      <th>intensity</th>\n",
       "      <th>tmax</th>\n",
       "      <th>UID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8598</td>\n",
       "      <td>1983</td>\n",
       "      <td>2</td>\n",
       "      <td>32.611001</td>\n",
       "      <td>0.611001</td>\n",
       "      <td>1.222003</td>\n",
       "      <td>[1983.04.27, 1983.04.28]</td>\n",
       "      <td>[0.2689637534, 0.9530389138]</td>\n",
       "      <td>[32.2689637534, 32.9530389138]</td>\n",
       "      <td>UID-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8598</td>\n",
       "      <td>1983</td>\n",
       "      <td>3</td>\n",
       "      <td>32.446675</td>\n",
       "      <td>0.446675</td>\n",
       "      <td>1.340026</td>\n",
       "      <td>[1983.04.30, 1983.05.01, 1983.05.02]</td>\n",
       "      <td>[0.3854449233, 0.8051994005, 0.1493818284]</td>\n",
       "      <td>[32.3854449233, 32.8051994005, 32.1493818284]</td>\n",
       "      <td>UID-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8598</td>\n",
       "      <td>1983</td>\n",
       "      <td>6</td>\n",
       "      <td>32.779170</td>\n",
       "      <td>0.779170</td>\n",
       "      <td>4.675023</td>\n",
       "      <td>[1983.05.04, 1983.05.05, 1983.05.06, 1983.05.0...</td>\n",
       "      <td>[0.1244153536, 1.4033823586, 1.1066183878, 0.9...</td>\n",
       "      <td>[32.1244153536, 33.4033823586, 33.1066183878, ...</td>\n",
       "      <td>UID-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8598</td>\n",
       "      <td>1983</td>\n",
       "      <td>10</td>\n",
       "      <td>32.904408</td>\n",
       "      <td>0.904408</td>\n",
       "      <td>9.044082</td>\n",
       "      <td>[1983.05.14, 1983.05.15, 1983.05.16, 1983.05.1...</td>\n",
       "      <td>[0.6712998044, 0.9472656874000001, 0.707608355...</td>\n",
       "      <td>[32.6712998044, 32.9472656874, 32.7076083552, ...</td>\n",
       "      <td>UID-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8598</td>\n",
       "      <td>1983</td>\n",
       "      <td>10</td>\n",
       "      <td>32.786520</td>\n",
       "      <td>0.786520</td>\n",
       "      <td>7.865203</td>\n",
       "      <td>[1983.05.26, 1983.05.27, 1983.05.28, 1983.05.2...</td>\n",
       "      <td>[0.1442364976, 0.42303558750000003, 0.88428633...</td>\n",
       "      <td>[32.1442364976, 32.4230355875, 32.8842863386, ...</td>\n",
       "      <td>UID-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_HDC_G0  year  duration   avg_temp  avg_intensity  tot_intensity  \\\n",
       "0       8598  1983         2  32.611001       0.611001       1.222003   \n",
       "1       8598  1983         3  32.446675       0.446675       1.340026   \n",
       "2       8598  1983         6  32.779170       0.779170       4.675023   \n",
       "3       8598  1983        10  32.904408       0.904408       9.044082   \n",
       "4       8598  1983        10  32.786520       0.786520       7.865203   \n",
       "\n",
       "                                         event_dates  \\\n",
       "0                           [1983.04.27, 1983.04.28]   \n",
       "1               [1983.04.30, 1983.05.01, 1983.05.02]   \n",
       "2  [1983.05.04, 1983.05.05, 1983.05.06, 1983.05.0...   \n",
       "3  [1983.05.14, 1983.05.15, 1983.05.16, 1983.05.1...   \n",
       "4  [1983.05.26, 1983.05.27, 1983.05.28, 1983.05.2...   \n",
       "\n",
       "                                           intensity  \\\n",
       "0                       [0.2689637534, 0.9530389138]   \n",
       "1         [0.3854449233, 0.8051994005, 0.1493818284]   \n",
       "2  [0.1244153536, 1.4033823586, 1.1066183878, 0.9...   \n",
       "3  [0.6712998044, 0.9472656874000001, 0.707608355...   \n",
       "4  [0.1442364976, 0.42303558750000003, 0.88428633...   \n",
       "\n",
       "                                                tmax    UID  \n",
       "0                     [32.2689637534, 32.9530389138]  UID-0  \n",
       "1      [32.3854449233, 32.8051994005, 32.1493818284]  UID-1  \n",
       "2  [32.1244153536, 33.4033823586, 33.1066183878, ...  UID-2  \n",
       "3  [32.6712998044, 32.9472656874, 32.7076083552, ...  UID-3  \n",
       "4  [32.1442364976, 32.4230355875, 32.8842863386, ...  UID-4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ignored-monroe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1 259291\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_pop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f8934854ef26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstep1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtot_days\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstep2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pdays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_pop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstep3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_years\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_pop' is not defined"
     ]
    }
   ],
   "source": [
    "step1 = tot_days(stats)\n",
    "print('step1',len(step1))\n",
    "step2 = make_pdays(step1, df_pop, scale)\n",
    "print('step2',len(step2))\n",
    "step3 = add_years(step2)\n",
    "print('step3',len(step3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "boxed-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#### Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#### Functions\n",
    "def tot_days(df):\n",
    "    \"\"\" Calulates the total number of days per year when a heat threshold was met\n",
    "    \"\"\"\n",
    "    df_out = df[['ID_HDC_G0','year','duration']].groupby(['ID_HDC_G0','year']).sum().reset_index()\n",
    "    df_out.rename(columns={'duration':'tot_days'}, inplace=True)\n",
    "    \n",
    "    return df_out\n",
    "    \n",
    "def make_pdays(df_stats, df_pop, scale):\n",
    "    \n",
    "    \"\"\"  Makes a dataframe with Tmax stats and population to calc people days.\n",
    "    \n",
    "    Args: \n",
    "        df_stats = Tmax stats output\n",
    "        df_pop = interpolated GHS-UCDB population\n",
    "        scale = if you want to divide the data ... 10**9 is best for global scale\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make Population Long Format\n",
    "    pop_long = pd.wide_to_long(df_pop, stubnames = 'P', i = 'ID_HDC_G0', j = 'year')\n",
    "    pop_long.reset_index(level=0, inplace=True)\n",
    "    pop_long.reset_index(level=0, inplace=True)\n",
    "    pop_long = pop_long.drop('Unnamed: 0', axis = 1)\n",
    "    \n",
    "    # Get Total Days \n",
    "    data = df_stats\n",
    "    pdays = pd.DataFrame()\n",
    "    pdays['ID_HDC_G0'] = data['ID_HDC_G0']\n",
    "    pdays['year'] = data['year']\n",
    "    pdays['tot_days'] = data['tot_days']\n",
    "\n",
    "    # Merge\n",
    "    pdays_merge = pdays.merge(pop_long, on=['ID_HDC_G0', 'year'], how = 'left')\n",
    "\n",
    "    # Now get people days from 1983 and change\n",
    "    p = pd.DataFrame()\n",
    "    p['ID_HDC_G0'] = df_pop['ID_HDC_G0']\n",
    "    p['P1983'] = df_pop['P1983']\n",
    "    p['P2016'] = df_pop['P2016']\n",
    "\n",
    "    pdays_merge = pdays_merge.merge(p ,on=['ID_HDC_G0'], how = 'left')\n",
    "    \n",
    "    # Calc p days = total days i * pop i \n",
    "    pdays_merge['people_days'] = pdays_merge['tot_days'] * pdays_merge['P'] / scale # total people days\n",
    "    \n",
    "    # Pdays due to heat increase = total days total days >40.6 / yr * Pop in 1983\n",
    "    pdays_merge['people_days_heat'] = pdays_merge['tot_days'] * pdays_merge['P1983'] / scale # people days w/ pop con\n",
    "    \n",
    "    # Pdays due to pop increase = total days i * (pop i - pop 83)\n",
    "    pdays_merge['people_days_pop'] = pdays_merge['tot_days'] *(pdays_merge['P'] - pdays_merge['P1983']) / scale # dif\n",
    "\n",
    "    return pdays_merge\n",
    "\n",
    "def add_years(df):\n",
    "    \"\"\" Function adds zero to people days for all missing years for each city \n",
    "    so that regressions aren't screwed up. New data points have NAN for P column. \n",
    "    If needed, look them up in interim/GHS-UCDB-Interp.csv\"\"\"\n",
    "    \n",
    "    years = list(np.unique(df['year']))\n",
    "    row_list = []\n",
    "\n",
    "    for city in list(np.unique(df['ID_HDC_G0'])):\n",
    "        city_id = city # Get city Id \n",
    "        city_df = df.loc[df['ID_HDC_G0'] == city] # find the location\n",
    "        city_years = list(np.unique(city_df['year'])) # figure out the number of years\n",
    "\n",
    "        years_dif = list(set(years) - set(city_years)) # find the missing years\n",
    "\n",
    "        if len(years_dif) > 0: # add in the missing years\n",
    "            for year in years_dif: # add rows with dummy data and zeros\n",
    "                row = []\n",
    "                row.append(city)\n",
    "                row.append(year)\n",
    "                row.append(0) # tot_days = 0 days\n",
    "                row.append(np.nan) # population for that year is not needed\n",
    "                row.append(df[(df['ID_HDC_G0'] == city)]['P1983'].values[0])\n",
    "                row.append(df[(df['ID_HDC_G0'] == city)]['P1983'].values[0])\n",
    "                row.append(0) # people_days = 0 days\n",
    "                row.append(0) # people_days_heat = 0 days\n",
    "                row.append(0) # people_days_pop = 0 days\n",
    "\n",
    "                row_list.append(row) # append row list\n",
    "\n",
    "    df_new = pd.DataFrame(row_list, columns= df.columns) # merge the new rows into a df\n",
    "\n",
    "    df_new = df.append(df_new) # add the rows back to the original data frame\n",
    "\n",
    "    # Drop any city with zero people in 1983\n",
    "    df_new = df_new[df_new['P1983'] > 0]\n",
    "\n",
    "    return df_new\n",
    "\n",
    "def OLS(df, geog, col, alpha):\n",
    "    \n",
    "    \"\"\"Finds linear coef for increase in stat by a given geography from 1983 - 2016, as well\n",
    "    as the pct change in population of the cities within the given geography\n",
    "    \n",
    "    NOTE 2020.03.01 - This will throw a run time warning if all values of a col are zero (e.g. can regress\n",
    "    a bunch of zeros) ... See note in run_OLS. CPT \n",
    "    \n",
    "    NOTE 2020.03.01 - Later in the day this issue is resolved by removing the offending cities. See comments\n",
    "    in code. CPT\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        df = HI stats dataframe\n",
    "        geog = subset geography to calc people days regression\n",
    "        col = col to regress on \n",
    "        alpha = ci alpha for coef\n",
    "    \"\"\"\n",
    "\n",
    "    # Get results\n",
    "    labels = []\n",
    "    coef_list = []\n",
    "    leftci_list = []\n",
    "    rightci_list = []\n",
    "    p_list = []\n",
    "    df_out = pd.DataFrame()\n",
    "\n",
    "    for label, df_geog in df.groupby(geog):\n",
    "\n",
    "        # Get Data\n",
    "        X_year = np.array(df_geog.groupby('year')['ID_HDC_G0'].mean().index).reshape((-1, 1))\n",
    "        Y_stats = np.array(df_geog.groupby('year')[col].sum()).reshape((-1, 1))\n",
    "\n",
    "        # Add Intercept\n",
    "        X_year_2 = sm.add_constant(X_year)\n",
    "\n",
    "        # Regress\n",
    "        model = sm.OLS(Y_stats, X_year_2).fit() \n",
    "        \n",
    "        # Get slope\n",
    "        # first param in intercept coef, second is slope of line but if slope = 0, then intecept\n",
    "        if len(model.params) == 2:\n",
    "            coef = model.params[1]\n",
    "            \n",
    "        else:\n",
    "            coef = model.params[0]\n",
    "        \n",
    "        # P value\n",
    "        p = model.pvalues[0]\n",
    "\n",
    "        # Make lists\n",
    "        labels.append(label)\n",
    "\n",
    "        coef_list.append(coef)\n",
    "        p_list.append(p)\n",
    "\n",
    "    # Make data frame\n",
    "    df_out[geog] = labels\n",
    "\n",
    "    df_out['coef'] = coef_list\n",
    "    df_out['p_value'] = [round(elem, 4) for elem in p_list]\n",
    "\n",
    "    return df_out\n",
    "\n",
    "def run_OLS(stats, geog, alpha):\n",
    "    \"\"\" Function calculate OLS coef of people days due to pop and heat and the \n",
    "    attribution index for distribution plots.\n",
    "    \n",
    "        \n",
    "    NOTE 2020.03.01 - This will throw a run time warning if all values of a col are zero (e.g. can regress\n",
    "    a bunch of zeros, now can we). This will happen if people_days, people_days_pop, people_days_heat or \n",
    "    total_days is zero for all years for a given city. This is still OK for our analysis. What is happening is\n",
    "    that for some cities, the people-days due to heat is zero, meaning pday increases in only due to population. \n",
    "    \n",
    "    This is because with the GHS-UCDB some city's population in 1983 is zero, which forces the pdays due to heat\n",
    "    to be zero.\n",
    "    \n",
    "    NOTE 2020.03.01 - Later in the day this issue is resolved by removing the offending cities. See comments\n",
    "    in code.\n",
    "    \n",
    "    -- CPT  \n",
    "    \n",
    "    Args:\n",
    "        stats = df to feed in\n",
    "        geog = geography level to conduct analysis (city-level is 'ID-HDC-G0')\n",
    "        alpha = alpha for CI coef   \n",
    "    \"\"\"\n",
    "    # Get coef for people days\n",
    "    out = OLS(stats, geog, 'people_days', alpha = alpha)\n",
    "    out.rename(columns={\"coef\": \"coef_pdays\"}, inplace = True)\n",
    "    out.rename(columns={\"p_value\": \"p_value_pdays\"}, inplace = True)\n",
    "    \n",
    "    # Get people days due to heat coef\n",
    "    heat = OLS(stats, geog, 'people_days_heat', alpha = alpha) # get stats \n",
    "    heat.rename(columns={\"coef\": \"coef_heat\"}, inplace = True)\n",
    "    heat.rename(columns={\"p_value\": \"p_value_heat\"}, inplace = True)\n",
    "    out = out.merge(heat, on = geog, how = 'left') # merge\n",
    "    \n",
    "    # Get people days due to pop\n",
    "    pop = OLS(stats, geog, 'people_days_pop', alpha = alpha) # get stats \n",
    "    pop.rename(columns={\"coef\": \"coef_pop\"}, inplace = True)\n",
    "    pop.rename(columns={\"p_value\": \"p_value_pop\"}, inplace = True)\n",
    "    out = out.merge(pop, on = geog, how = 'left') # merge\n",
    "    \n",
    "    # Get total days\n",
    "    totDays = OLS(stats, geog, 'tot_days', alpha = alpha) # get stats \n",
    "    totDays.rename(columns={\"coef\": \"coef_totDays\"}, inplace = True)\n",
    "    totDays.rename(columns={\"p_value\": \"p_value_totDays\"}, inplace = True)\n",
    "    out = out.merge(totDays, on = geog, how = 'left') # merge\n",
    "    \n",
    "    # attrib coef --- creates range -1 to 1 index of heat vs. population as a driver of total pdays increase\n",
    "    out['coef_attrib'] = (out['coef_pop'] - out['coef_heat']) / (out['coef_pop'] + out['coef_heat']) # normalize dif\n",
    "    \n",
    "    # drop all neg or zero pday slopes (e.g. cooling cities)\n",
    "    out = out[out['coef_pdays'] > 0]\n",
    "    out = out[out['coef_heat'] > 0]\n",
    "    out = out[out['coef_pop'] > 0]\n",
    "    \n",
    "    # normalize coef of attribution \n",
    "    norm = out['coef_attrib']\n",
    "    out['coef_attrib_norm'] = (norm-min(norm))/(max(norm)-min(norm))\n",
    "    \n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
