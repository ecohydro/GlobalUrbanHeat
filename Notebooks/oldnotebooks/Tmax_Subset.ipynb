{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tmax Subset\n",
    "\n",
    "A notebook to subset Tmax daily for the 13000 GHS urban areas to identify dates >40c, consecuritve days >40 c etc.\n",
    "\n",
    "Moved to cpt_tmax_stats_final to clean up all the code on 2019-09-24\n",
    "\n",
    "**Need to subset**\n",
    "- Days per year (done)\n",
    "- Duration of each event (done)\n",
    "- Intensity of each day during each event (>40.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depdencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from random import random\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import geopandas as gpd \n",
    "import glob\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_xr(file_in, time_dim, space_dim):\n",
    "    \n",
    "    \"\"\" Function reads in a csv w/ GHS-UCDB IDs and temp, isolates the temp\n",
    "    and returns a xarray data array with dims set to city ids and dates\n",
    "    \n",
    "    Args:\n",
    "        file_in = file name and path\n",
    "        time_dim = name for time dim as a str ... use date :-)\n",
    "        space_dim = col name for GHS-UCDB IDs as an str (ID_HDC_G0)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(file_in) # read the file in as a df\n",
    "    print(df.shape)\n",
    "    \n",
    "    df_id = df[space_dim] # get IDs\n",
    "    df_temp = df.iloc[:,3:] # get only temp columns\n",
    "    df_temp.index = df_id # set index values\n",
    "    df_temp_drop = df_temp.dropna() # Drop cities w/ no temp record \n",
    "    print(len(df_temp_drop))\n",
    "    \n",
    "    temp_np = df_temp_drop.to_numpy() # turn temp cols into an np array\n",
    "    \n",
    "    # make xr Data Array w/ data as temp and dims as spece (e.g. id)\n",
    "    \n",
    "    # Note 2019 09 17 changed to xr.Dataset from xr.Dataarray\n",
    "    temp_xr_da = xr.DataArray(temp_np, coords=[df_temp_drop.index, df_temp_drop.columns], \n",
    "                            dims=[space_dim, time_dim])\n",
    "    \n",
    "    return temp_xr_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_eventTot(xarray, Tthresh, year):\n",
    "    \"\"\" Function returns the number of days within a year where Tmax > Tthresh for each city.\n",
    "    \n",
    "    Args: \n",
    "        xarray = an xarray object with dims = (space, times)\n",
    "        Tthresh = int of temp threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    ## NOTE FOR SOME REASON out.ID_HDC_G0 cannot be fed a string ... note sure why so be careful with col names\n",
    "    out = xarray.where(xarray > Tthresh, drop = True)\n",
    "    id_list = []\n",
    "    event_tot = []\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    for index, loc in enumerate(out.ID_HDC_G0):\n",
    "        id_list.append(out.ID_HDC_G0.values[index])\n",
    "        event_tot.append(len(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values))\n",
    "    \n",
    "    df_out['ID_HDC_G0'] = id_list\n",
    "    df_out[year] = event_tot\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eventTot_loop(dir_nm, time_dim, space_dim, Tthresh):\n",
    "    \n",
    "    \"\"\" Loop through a dir with csvs to calc the total number of events greater than a threshold.\n",
    "        Leap years explain the difference in shapes 368 vs 369\n",
    "    \n",
    "    Args:\n",
    "        dir_nm = dir path to loop through\n",
    "        time_dim = name for time dim as a str ... use date :-) for csv_to_xr function\n",
    "        space_dim = col name for GHS-UCDB IDs as an str (ID_HDC_G0) for csv_to_xr function\n",
    "        Tthresh = int of temp threshold for temp_event function -- 40.6 is\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the GHS-ID List with GeoPANDAS read_file\n",
    "    ghs_ids_fn = 'GHS-UCSB-IDS.csv'\n",
    "    ghs_ids_df = pd.read_csv(DATA_INTERIM+ghs_ids_fn)\n",
    "    \n",
    "    # Git File list\n",
    "    fn_list = glob.glob(DAILY_PATH+'*.csv')\n",
    "    \n",
    "    for fn in sorted(fn_list):\n",
    "        \n",
    "        # Get year for arg for temp_event function\n",
    "        year = fn.split('GHS-Tmax-DAILY_')[1].split('.csv')[0]\n",
    "        print(year)\n",
    "        \n",
    "        temp_xr_da = csv_to_xr(fn, time_dim, space_dim)\n",
    "        \n",
    "        df_out = temp_eventTot(temp_xr_da, Tthresh, year)\n",
    "        \n",
    "        ghs_ids_df = ghs_ids_df.merge(df_out, on='ID_HDC_G0', how = 'outer') #<<<<----- NEED TO FIX THIS\n",
    "    \n",
    "    # build in later drop all NA GHS-IDs\n",
    "    \n",
    "    return ghs_ids_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_eventL_a(xarray, Tthresh, year): #<---------------- # NEED TO RENAME or\n",
    "    \"\"\" Function calculates the length of each Tmax threshold event as the number of days in a row\n",
    "    greater than a threshold within a year where Tmax > Tthresh for each city.\n",
    "    \n",
    "    Args: \n",
    "        xarray = an xarray object with dims = (space, times)\n",
    "        Tthresh = int of temp threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    ## NOTE FOR SOME REASON out.ID_HDC_G0 cannot be fed a string ... note sure why so be careful with col names\n",
    "    out = xarray.where(xarray > Tthresh, drop = True)\n",
    "    id_list = []\n",
    "    event_L = []\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    for index, loc in enumerate(out.ID_HDC_G0):\n",
    "        id_list.append(out.ID_HDC_G0.values[index])\n",
    "        event_tot.append(len(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values))\n",
    "    \n",
    "    df_out['ID_HDC_G0'] = id_list\n",
    "    df_out[year] = event_tot\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_eventL(xarray, Tthresh):\n",
    "    \"\"\" Function calculates the length of each Tmax threshold event as the number of days in a row\n",
    "    greater than a threshold within a year where Tmax > Tthresh for each city. Returns the length,\n",
    "    the dates, the tempatures, and the severity (daily Tmax - Tthresh)\n",
    "    \n",
    "    Args: \n",
    "        xarray = an xarray object with dims = (space, times)\n",
    "        Tthresh = int of temp threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty lists & df\n",
    "    id_list = []\n",
    "    date_list = []\n",
    "    dayTot_list = []\n",
    "    temp_list = []\n",
    "    severity_list = []\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    # subset xarry\n",
    "    out = xarray.where(xarray > Tthresh, drop = True)\n",
    "\n",
    "    # start loop \n",
    "    for index, loc in enumerate(out.ID_HDC_G0):\n",
    "        id_list.append(out.ID_HDC_G0.values[index]) # get IDS\n",
    "        date_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values) # get event dates\n",
    "        \n",
    "        # this is actually getting the total events of all 2019-09-22\n",
    "        dayTot_list.append(len(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values)) # get event totals\n",
    "        \n",
    "        temp_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').values) # get temp values\n",
    "        severity_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').values - Tthresh) # get severity\n",
    "\n",
    "    # write to a data frame\n",
    "    df_out['ID_HDC_G0'] = id_list\n",
    "    df_out['Days_Total'] = dayTot_list\n",
    "    df_out['Event_Dates'] = date_list\n",
    "    df_out['Event_Temps'] = temp_list\n",
    "    df_out['Event_Severity'] = severity_list\n",
    "\n",
    "    # return df_out\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eventL_loop(dir_nm, fn_out, time_dim, space_dim, Tthresh):\n",
    "    \n",
    "    \"\"\" Loop through a dir with csvs to apply temp_eventL function and save out a .csv for each year\n",
    "    \n",
    "    Args:\n",
    "        dir_nm = dir path to loop through\n",
    "        fn_out = string to label out files\n",
    "        time_dim = name for time dim as a str ... use date :-) for csv_to_xr function\n",
    "        space_dim = col name for GHS-UCDB IDs as an str (ID_HDC_G0) for csv_to_xr function\n",
    "        Tthresh = int of temp threshold for temp_event function -- 40.6 is used\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the GHS-ID List with GeoPANDAS read_file\n",
    "    ghs_ids_fn = 'GHS-UCSB-IDS.csv'\n",
    "    ghs_ids_df = pd.read_csv(DATA_INTERIM+ghs_ids_fn)\n",
    "        \n",
    "    # Git File list\n",
    "    fn_list = glob.glob(DAILY_PATH+'*.csv')\n",
    "    \n",
    "    for fn in sorted(fn_list):\n",
    "        \n",
    "        # Get year for arg for temp_event function\n",
    "        year = fn.split('GHS-Tmax-DAILY_')[1].split('.csv')[0]\n",
    "        print(year)\n",
    "        \n",
    "        temp_xr_da = csv_to_xr(fn, time_dim, space_dim)\n",
    "        \n",
    "        df_out = temp_eventL(temp_xr_da, Tthresh)\n",
    "                \n",
    "        ghs_ids_df_out = ghs_ids_df.merge(df_out, on='ID_HDC_G0', how = 'inner') #<<<<----- NEED TO FIX THIS\n",
    "\n",
    "        ghs_ids_df_out.to_csv(DATA_OUT+fn_out+year+'.csv')\n",
    "\n",
    "        print(year, 'SAVED!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths\n",
    "DAILY_PATH = '/home/cascade/projects/data_out_urbanheat/CHIRTS-GHS-DAILY/'\n",
    "DATA_INTERIM = '/home/cascade/projects/UrbanHeat/data/interim/'\n",
    "DATA_OUT = '/home/cascade/projects/data_out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name to test\n",
    "fn_in = 'GHS-Tmax-DAILY_1983.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13135, 368)\n",
      "13067\n"
     ]
    }
   ],
   "source": [
    "xr1993 = csv_to_xr(DAILY_PATH+fn_in, 'date', 'ID_HDC_G0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (ID_HDC_G0: 13067, date: 365)>\n",
       "array([[-43.921947, -33.71345 , -33.054974, ..., -12.416152, -13.232986,\n",
       "        -15.403823],\n",
       "       [ -4.804248,  -3.914425,  -7.533999, ...,  -5.186461, -10.945722,\n",
       "        -16.29516 ],\n",
       "       [-23.904118, -17.422953, -13.182008, ..., -12.788978, -11.337886,\n",
       "        -10.00939 ],\n",
       "       ...,\n",
       "       [ 16.028023,  17.73603 ,  20.493294, ...,  14.559421,  15.160739,\n",
       "         15.184024],\n",
       "       [ 16.420553,  17.87142 ,  22.519674, ...,  15.680964,  16.169733,\n",
       "         16.039179],\n",
       "       [ 16.6943  ,  17.559229,  21.480919, ...,  14.446052,  15.235602,\n",
       "         14.005591]])\n",
       "Coordinates:\n",
       "  * ID_HDC_G0  (ID_HDC_G0) int64 5782 3316 5645 3185 ... 1116 1114 1161 1169\n",
       "  * date       (date) object '1983.01.01' '1983.01.02' ... '1983.12.31'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr1993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "event1993 = temp_eventL(xr1993, 40.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#event1993[400:450]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find breaks in time serise\n",
    "\n",
    "https://stackoverflow.com/questions/40118037/how-can-i-detect-gaps-and-consecutive-periods-in-a-time-series-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1983.06.22', '1983.06.23', '1983.06.24', '1983.06.25',\n",
       "       '1983.06.26', '1983.06.27', '1983.06.28', '1983.06.29',\n",
       "       '1983.06.30', '1983.07.01', '1983.07.21', '1983.07.22',\n",
       "       '1983.07.23', '1983.08.01'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get event dates\n",
    "dates = event1993[event1993['ID_HDC_G0'] == 6279]['Event_Dates']\n",
    "dates = dates.values[0]\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Float64Index([2445507.5, 2445508.5, 2445509.5, 2445510.5, 2445511.5, 2445512.5,\n",
       "              2445513.5, 2445514.5, 2445515.5, 2445516.5, 2445536.5, 2445537.5,\n",
       "              2445538.5, 2445547.5],\n",
       "             dtype='float64')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jul_convert(dates):\n",
    "    \"function turn days into julian datetime\"\n",
    "    jul_days = pd.to_datetime(dates).to_julian_date()\n",
    "    \n",
    "    return jul_days\n",
    "\n",
    "jul_convert(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.037226, 1.546217, 2.625624, 0.237475, 1.43565 , 2.84871 ,\n",
       "       1.85326 , 2.231856, 2.046236, 0.42042 , 0.017912, 1.03619 ,\n",
       "       1.202315, 1.34712 ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get intensity for each day, note 'Event_Severity' is really intensity\n",
    "intensity = event1993[event1993['ID_HDC_G0'] == 6279]['Event_Severity']\n",
    "intensity = intensity.values[0]\n",
    "intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40.637226, 42.146217, 43.225624, 40.837475, 42.03565 , 43.44871 ,\n",
       "       42.45326 , 42.831856, 42.646236, 41.02042 , 40.617912, 41.63619 ,\n",
       "       41.802315, 41.94712 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmax = event1993[event1993['ID_HDC_G0'] == 6279]['Event_Temps']\n",
    "tmax = tmax.values[0]\n",
    "tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tid bit to run julian days back\n",
    "\n",
    "def jul_convert(dates):\n",
    "    \"Function turn days into julian datetime\"\n",
    "    jul_days = pd.to_datetime(dates).to_julian_date()\n",
    "    \n",
    "    return jul_days\n",
    "\n",
    "def event_split(dates, ID_HDC_G0, country, intesneity, tmax):\n",
    "    \"\"\" Searchs a list of dates and isolates sequential dates as a list, then calculates event stats.\n",
    "    See comments in code for more details. \n",
    "    \n",
    "    Args:\n",
    "        dates: pandas.core.index as julian dates\n",
    "        ID_HDC_G0: city ID as string\n",
    "        country: country of city as string\n",
    "        intensity: numpy.ndarray of intensities values\n",
    "        tmax: numpy.ndarray of intensities values of tmax values\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # city id\n",
    "    city_id = ID_HDC_G0\n",
    "    \n",
    "    # country \n",
    "    country = country\n",
    "    \n",
    "    # lists to fill\n",
    "    city_id_list = []\n",
    "    country_list = []\n",
    "    dates_list = []\n",
    "    dur_list = []\n",
    "    intensity_list = []\n",
    "    tmax_list = []\n",
    "    avg_temp_list = []\n",
    "    avg_int_list = []\n",
    "    tot_int_list = []\n",
    "    \n",
    "    # data frame out\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    # turn days into julian days\n",
    "    jul_days = jul_convert(dates)\n",
    "    \n",
    "    # Loop through dur list and isolate seq days, temps, and intensities\n",
    "    for k, g in groupby(enumerate(jul_days.values), lambda x: x[1]-x[0]):\n",
    "        \n",
    "        seq = list(map(itemgetter(1), g)) # isolate seq. days\n",
    "        dur = len(seq) # duration of each event\n",
    "        days = dates[0:dur] # dates of tmax days during each event\n",
    "        intense = intensity[0:dur] # intensity of each day during event\n",
    "        temp = tmax[0:dur] # temp of each day during event\n",
    "        avg_temp = mean(temp) # avg. temp during event\n",
    "        avg_int = mean(intense) # avg. intensity during event\n",
    "        tot_int = intense.sum() # total intensity during event\n",
    "        \n",
    "        # fill lists\n",
    "        city_id_list.append(city_id)\n",
    "        country_list.append(country)\n",
    "        dur_list.append(dur)\n",
    "        dates_list.append(days)\n",
    "        intensity_list.append(intense)\n",
    "        tmax_list.append(temp)\n",
    "        avg_temp_list.append(avg_temp)\n",
    "        avg_int_list.append(avg_int)\n",
    "        tot_int_list.append(tot_int)\n",
    "     \n",
    "    # write out as a dateframe\n",
    "    df_out['ID_HDC_G0'] = city_id_list\n",
    "    df_out['CTR_MN_NM'] = country_list\n",
    "    df_out['duration'] = dur_list\n",
    "    df_out['avg_temp'] = avg_temp_list\n",
    "    df_out['avg_intensity'] = avg_int_list\n",
    "    df_out['tot_intensity'] = tot_int_list\n",
    "    df_out['events'] = dates_list\n",
    "    df_out['duration'] = dur_list\n",
    "    df_out['intensity'] = intensity_list\n",
    "    df_out['tmax'] = tmax_list\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_HDC_G0</th>\n",
       "      <th>CTR_MN_NM</th>\n",
       "      <th>duration</th>\n",
       "      <th>avg_temp</th>\n",
       "      <th>avg_intensity</th>\n",
       "      <th>tot_intensity</th>\n",
       "      <th>events</th>\n",
       "      <th>intensity</th>\n",
       "      <th>tmax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>City</td>\n",
       "      <td>Counrty</td>\n",
       "      <td>10</td>\n",
       "      <td>42.128267</td>\n",
       "      <td>1.528267</td>\n",
       "      <td>15.282674</td>\n",
       "      <td>[1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...</td>\n",
       "      <td>[0.03722599999999687, 1.5462169999999986, 2.62...</td>\n",
       "      <td>[40.637226, 42.146217, 43.225623999999996, 40....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>City</td>\n",
       "      <td>Counrty</td>\n",
       "      <td>3</td>\n",
       "      <td>42.003022</td>\n",
       "      <td>1.403022</td>\n",
       "      <td>4.209067</td>\n",
       "      <td>[1983.06.22, 1983.06.23, 1983.06.24]</td>\n",
       "      <td>[0.03722599999999687, 1.5462169999999986, 2.62...</td>\n",
       "      <td>[40.637226, 42.146217, 43.225623999999996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>City</td>\n",
       "      <td>Counrty</td>\n",
       "      <td>1</td>\n",
       "      <td>40.637226</td>\n",
       "      <td>0.037226</td>\n",
       "      <td>0.037226</td>\n",
       "      <td>[1983.06.22]</td>\n",
       "      <td>[0.03722599999999687]</td>\n",
       "      <td>[40.637226]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_HDC_G0 CTR_MN_NM  duration   avg_temp  avg_intensity  tot_intensity  \\\n",
       "0      City   Counrty        10  42.128267       1.528267      15.282674   \n",
       "1      City   Counrty         3  42.003022       1.403022       4.209067   \n",
       "2      City   Counrty         1  40.637226       0.037226       0.037226   \n",
       "\n",
       "                                              events  \\\n",
       "0  [1983.06.22, 1983.06.23, 1983.06.24, 1983.06.2...   \n",
       "1               [1983.06.22, 1983.06.23, 1983.06.24]   \n",
       "2                                       [1983.06.22]   \n",
       "\n",
       "                                           intensity  \\\n",
       "0  [0.03722599999999687, 1.5462169999999986, 2.62...   \n",
       "1  [0.03722599999999687, 1.5462169999999986, 2.62...   \n",
       "2                              [0.03722599999999687]   \n",
       "\n",
       "                                                tmax  \n",
       "0  [40.637226, 42.146217, 43.225623999999996, 40....  \n",
       "1         [40.637226, 42.146217, 43.225623999999996]  \n",
       "2                                        [40.637226]  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_split_test(dates, 'City', 'Counrty', intensity, tmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start running it on a one file and then build loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN = '/home/cascade/projects/data_out_urbanheat/CHIRTS-GHS-Events/'\n",
    "fn = 'CHIRTS-GHS-Events1983.csv'\n",
    "events1983 = pd.read_csv(DATA_IN+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID_HDC_G0</th>\n",
       "      <th>CTR_MN_NM</th>\n",
       "      <th>Event_Length</th>\n",
       "      <th>Event_Dates</th>\n",
       "      <th>Event_Temps</th>\n",
       "      <th>Event_Severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2784</td>\n",
       "      <td>Germany</td>\n",
       "      <td>1</td>\n",
       "      <td>['1983.07.27']</td>\n",
       "      <td>[44.45975]</td>\n",
       "      <td>[3.85975]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2372</td>\n",
       "      <td>France</td>\n",
       "      <td>1</td>\n",
       "      <td>['1983.07.31']</td>\n",
       "      <td>[43.331635]</td>\n",
       "      <td>[2.731635]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6156</td>\n",
       "      <td>Kazakhstan</td>\n",
       "      <td>1</td>\n",
       "      <td>['1983.07.31']</td>\n",
       "      <td>[41.336376]</td>\n",
       "      <td>[0.736376]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2833</td>\n",
       "      <td>Germany</td>\n",
       "      <td>1</td>\n",
       "      <td>['1983.07.27']</td>\n",
       "      <td>[47.49318]</td>\n",
       "      <td>[6.89318]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2885</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1</td>\n",
       "      <td>['1983.07.27']</td>\n",
       "      <td>[44.01389]</td>\n",
       "      <td>[3.41389]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ID_HDC_G0   CTR_MN_NM  Event_Length     Event_Dates  \\\n",
       "0           0       2784     Germany             1  ['1983.07.27']   \n",
       "1           1       2372      France             1  ['1983.07.31']   \n",
       "2           2       6156  Kazakhstan             1  ['1983.07.31']   \n",
       "3           3       2833     Germany             1  ['1983.07.27']   \n",
       "4           4       2885     Austria             1  ['1983.07.27']   \n",
       "\n",
       "   Event_Temps Event_Severity  \n",
       "0   [44.45975]      [3.85975]  \n",
       "1  [43.331635]     [2.731635]  \n",
       "2  [41.336376]     [0.736376]  \n",
       "3   [47.49318]      [6.89318]  \n",
       "4   [44.01389]      [3.41389]  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events1983.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID_HDC_G0</th>\n",
       "      <th>CTR_MN_NM</th>\n",
       "      <th>Event_Length</th>\n",
       "      <th>Event_Dates</th>\n",
       "      <th>Event_Temps</th>\n",
       "      <th>Event_Severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>6279</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>14</td>\n",
       "      <td>['1983.06.22' '1983.06.23' '1983.06.24' '1983....</td>\n",
       "      <td>[40.637226 42.146217 43.225624 40.837475 42.03...</td>\n",
       "      <td>[0.037226 1.546217 2.625624 0.237475 1.43565  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  ID_HDC_G0 CTR_MN_NM  Event_Length  \\\n",
       "400         400       6279  Pakistan            14   \n",
       "\n",
       "                                           Event_Dates  \\\n",
       "400  ['1983.06.22' '1983.06.23' '1983.06.24' '1983....   \n",
       "\n",
       "                                           Event_Temps  \\\n",
       "400  [40.637226 42.146217 43.225624 40.837475 42.03...   \n",
       "\n",
       "                                        Event_Severity  \n",
       "400  [0.037226 1.546217 2.625624 0.237475 1.43565  ...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = events1983[events1983['ID_HDC_G0'] == 6279]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1983.06.22' '1983.06.23' '1983.06.24' '1983.06.25' '1983.06.26'\n",
      " '1983.06.27' '1983.06.28' '1983.06.29' '1983.06.30' '1983.07.01'\n",
      " '1983.07.21' '1983.07.22' '1983.07.23' '1983.08.01']\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "for index, row in test.head(n=2).iterrows():\n",
    "    # Get event dates\n",
    "    days = row['Event_Dates']\n",
    "    #dates = dates.values[0]\n",
    "    print((dates))\n",
    "       \n",
    "#     dates = row['Event_Dates'].apply(literal_eval)\n",
    "#     tmax = row['Event_Temps'] \n",
    "#     intensity = row['Event_Severity']\n",
    "#     city = row['ID_HDC_G0']\n",
    "#     country = row['CTR_MN_NM']\n",
    "    \n",
    "#     print(type(dates))\n",
    "#     print(dates)\n",
    "#     df_test = event_split(dates, tmax, intensity, city, country)\n",
    "#     print(df_test['duration']) \n",
    "    \n",
    "    \n",
    "    #Convert to arrays\n",
    "#     dates = dates.values[0]\n",
    "#     dates\n",
    "    \n",
    "#     df_test = event_split(days, tmax, intensity, city, country)\n",
    "#     print(df_test['duration'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the subset routine you need as one script\n",
    "Done on 2019-0924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths \n",
    "\n",
    "# UPDATE AS NEEDED <<<<< ------------------------------------------\n",
    "DAILY_PATH = '/home/cascade/projects/data_out_urbanheat/CHIRTS-GHS-DAILY/'\n",
    "DATA_INTERIM = '/home/cascade/projects/UrbanHeat/data/interim/'\n",
    "DATA_OUT = '/home/cascade/projects/data_out_urbanheat/CHIRTS-GHS-Events/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name\n",
    "fn_out = 'CHIRTS-GHS-Events'\n",
    "dir_nm = DAILY_PATH\n",
    "time_dim = 'date'\n",
    "space_dim = 'ID_HDC_G0'\n",
    "Tthresh = 40.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventL_loop(dir_nm, fn_out, time_dim, space_dim, Tthresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "\n",
    "figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "y = range(1,125)\n",
    "year = '2010'\n",
    "country = 'INDIA'\n",
    "plt.hist(india[year], bins = 125)\n",
    "plt.xlabel('Number of Days in '+year+' where Tmax >40c in ')\n",
    "plt.ylabel('Number of cities')\n",
    "plt.title(country+': For all cities with Tmax >40, how many days in '+year+' were >40C? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP BACK TO POLYGONS AND LOOK AT IT \n",
    "SHP_DIR = '/Users/cascade/Github/UrbanHeat/data/raw/ghs-ucdb/'\n",
    "shp_fn = 'GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_0.shp'\n",
    "shps = gpd.read_file(SHP_DIR+shp_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ghs = gpd.GeoDataFrame()\n",
    "df_ghs['geometry'] = shps.geometry\n",
    "df_ghs['ID_HDC_G0'] = shps.ID_HDC_G0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df_ghs.merge(events, on='ID_HDC_G0', how = 'inner') #<<<<----- NEED TO FIX THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write it out\n",
    "DATA_INTERIM = '/Users/cascade/Github/UrbanHeat/data/interim/'\n",
    "fn_out = 'GHS-TmaxDaily-events.shp'\n",
    "df_merge.to_file(DATA_INTERIM+fn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return the ID and Date where Tmax is greater than 40 as a dict, but will not return actual tempatures \n",
    "\n",
    "Tmax = np.random.randint(20, high=50, size=(3,10)) # Make a 3x10 random list\n",
    "print(Tmax)\n",
    "results = np.where(Tmax > 40) # find the index and rows\n",
    "coords = list(zip(results[0], results[1])) # zip the i and js into tuples\n",
    "\n",
    "b = [(k, list(list(zip(*g))[1])) for k, g in groupby(coords, itemgetter(0))] # group by rows\n",
    "\n",
    "print(b)\n",
    "dict_out = dict(b) # turn into a dict, where keys are city ids and values are dates\n",
    "dict_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict_out.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(Tmax > 40, Tmax, Tmax*0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(Tmax > 40) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(Tmax>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_search(array):\n",
    "    results = np.where(array > 40) # find the index and rows\n",
    "    coords = list(zip(results[0], results[1])) # zip the i and js into tuples\n",
    "    b = [(k, list(list(zip(*g))[1])) for k, g in groupby(coords, itemgetter(0))] # group by rows\n",
    "    dict_out = dict(b) # turn into a dict, where keys are city ids and values are dates\n",
    "\n",
    "    return dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_in = '/Users/cascade/Desktop/GHS-Tmax-DAILY_1983.csv'\n",
    "\n",
    "df = pd.read_csv(file_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_drop = df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.head()\n",
    "arr = df_sub.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_search = temp_search(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some fake data\n",
    "Tmax = np.random.randint(20, high=50, size=(3,10))\n",
    "locs = ['001', '002', '003']\n",
    "times = pd.date_range('2000-01-01', periods=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = xr.DataArray(Tmax, coords=[locs, times], dims=['space', 'times'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = foo.where(foo > 40, drop = True)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for loc in out.space:\n",
    "    print(len(out.sel(space = loc).dropna(dim = 'times').times.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in out.space.values:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.space.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr1993 = csv_to_xr(DAILY_PATH+fn_in, 'date', 'ID_HDC_G0')\n",
    "out = xr1993.where(xr1993 > 40.6, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "40 - out.sel(ID_HDC_G0 = 5885).dropna(dim = 'date').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#event_tot.append(len(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values))\n",
    "\n",
    "id_list = []\n",
    "date_list = []\n",
    "eventL_list = []\n",
    "temp_list = []\n",
    "df_out = pd.DataFrame()\n",
    "\n",
    "# start loop \n",
    "for index, loc in enumerate(out.ID_HDC_G0):\n",
    "\n",
    "    id_list.append(out.ID_HDC_G0.values[index]) # get IDS\n",
    "    date_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values) # get event dates\n",
    "    eventL_list.append(len(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values)) # get event lengths\n",
    "    temp_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').values) #get temp values\n",
    "\n",
    "# write to a data frame\n",
    "df_out['ID_HDC_G0'] = id_list\n",
    "df_out['Event_Length'] = eventL_list\n",
    "df_out['Event_Dates'] = date_list\n",
    "df_out['Event_Temps'] = temp_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run routine\n",
    "# all_events_df = event_loop(DAILY_PATH, 'date', 'ID_HDC_G0', 40.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move IDS to Index \n",
    "\n",
    "all_events_df = all_events_df.set_index(['ID_HDC_G0', 'CTR_MN_NM'], drop = True)\n",
    "all_events_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaNs\n",
    "all_events_df_drop = all_events_df.dropna(how = 'all')\n",
    "all_events_df_drop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_df_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = all_events_df_drop.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out['ID_HDC_G0'] = all_events_df_drop.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_df_drop = all_events_df_drop.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_df_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_events_df_drop.to_csv(DATA_OUT+'20190831_TMax-GHS_TotEvents83-2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india = all_events_df_drop[all_events_df_drop['CTR_MN_NM'] == 'India']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this https://stackoverflow.com/questions/52901387/find-group-of-consecutive-dates-in-pandas-dataframe\n",
    "\n",
    "dt = test[test['ID_HDC_G0'] == 6279]['Event_Dates']\n",
    "day = pd.Timedelta('1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = test[test['ID_HDC_G0'] == 6279]\n",
    "city_list = city.Event_Dates.tolist()\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['dates'] = city_list\n",
    "df.dates.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['1983.06.20', '1983.06.23', '1983.06.24', '1983.06.25',\n",
    "        '1983.06.26', '1983.06.27', '1983.06.28', '1983.06.29',\n",
    "        '1983.06.30', '1983.07.01', '1983.07.21', '1983.07.22',\n",
    "        '1983.07.23', '1983.08.01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dates = pd.to_datetime(dates)\n",
    "shift = pd_dates.shift(1, freq = 'D')\n",
    "day = pd.Timedelta('1d')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['dates'] = pd_dates\n",
    "# df['shift'] = shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_block = ((df - df.shift(-1)).abs() == day)\n",
    "in_block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = df.loc[in_block]\n",
    "filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_list = dt.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_list = ['1983.06.22', '1983.06.23', '1983.06.24', '1983.06.25',\n",
    "       '1983.06.26', '1983.06.27', '1983.06.28', '1983.06.29',\n",
    "       '1983.06.30', '1983.07.01', '1983.07.21', '1983.07.22',\n",
    "       '1983.07.23', '1983.08.01']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_block = ((dt - dt.shift(-1)).abs() == day) | (dt.diff() == day)\n",
    "in_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this https://stackoverflow.com/questions/52901387/find-group-of-consecutive-dates-in-pandas-dataframe\n",
    "\n",
    "dt = test[test['ID_HDC_G0'] == 6279]['Event_Dates']\n",
    "day = pd.Timedelta('1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = test[test['ID_HDC_G0'] == 6279]\n",
    "city_list = city.Event_Dates.tolist()\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['dates'] = city_list\n",
    "df.dates.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['1983.06.20', '1983.06.23', '1983.06.24', '1983.06.25',\n",
    "        '1983.06.26', '1983.06.27', '1983.06.28', '1983.06.29',\n",
    "        '1983.06.30', '1983.07.01', '1983.07.21', '1983.07.22',\n",
    "        '1983.07.23', '1983.08.01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dates = pd.to_datetime(dates)\n",
    "shift = pd_dates.shift(1, freq = 'D')\n",
    "day = pd.Timedelta('1d')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['dates'] = pd_dates\n",
    "# df['shift'] = shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_block = ((df - df.shift(-1)).abs() == day)\n",
    "in_block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = df.loc[in_block]\n",
    "filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_list = dt.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_list = ['1983.06.22', '1983.06.23', '1983.06.24', '1983.06.25',\n",
    "       '1983.06.26', '1983.06.27', '1983.06.28', '1983.06.29',\n",
    "       '1983.06.30', '1983.07.01', '1983.07.21', '1983.07.22',\n",
    "       '1983.07.23', '1983.08.01']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_block = ((dt - dt.shift(-1)).abs() == day) | (dt.diff() == day)\n",
    "in_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Another idea\n",
    "# https://stackoverflow.com/questions/2361945/detecting-consecutive-integers-in-a-list\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "data = [1, 4,5,6, 10, 15,16,17,18, 22, 25,26,27,28]\n",
    "\n",
    "for k, g in groupby(enumerate(data), lambda x: x[1]-x[0]):\n",
    "    print(map(itemgetter(1), g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [1,  4,5,6, 10, 15,16,17,18, 22, 25,26,27,28]\n",
    "for k, g in groupby(enumerate(L), lambda x: x[1]-x[0] ) :\n",
    "  print (list(map(itemgetter(1), g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['1983.06.20', '1983.06.23', '1983.06.24', '1983.06.25',\n",
    "        '1983.06.26', '1983.06.27', '1983.06.28', '1983.06.29',\n",
    "        '1983.06.30', '1983.07.01', '1983.07.21', '1983.07.22',\n",
    "        '1983.07.23', '1983.08.01']\n",
    "\n",
    "pd_dates = pd.to_datetime(dates)\n",
    "df_dates = pd.DataFrame()\n",
    "df_dates['dates'] = pd_dates\n",
    "\n",
    "\n",
    "\n",
    "test = df_dates['dates'].apply(lambda x: x.toordinal())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, g in groupby(enumerate(test), lambda x: x[1]-x[0]):\n",
    "  print (list(map(itemgetter(1), g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#event1993[['Event_Dates','Event_Severity']].apply(event_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events_shift = events.shift(1, freq = 'D')\n",
    "\n",
    "#events.to_julian_date() - events_shift.to_julian_date()\n",
    "\n",
    "# turn into list \n",
    "# events_list = [list(i) for i in events.to_list()][0]\n",
    "\n",
    "# turn into ordinal dates\n",
    "# (pd.to_datetime(events.values[0]))\n",
    "\n",
    "# pd_events = pd.to_datetime(events_list)\n",
    "# df_events = pd.DataFrame()\n",
    "# df_events['events'] = pd_events\n",
    "\n",
    "# df_events_ord = df_events['events'].apply(lambda x: x.toordinal())\n",
    "# (df_events_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_events = pd.to_datetime(events_list)\n",
    "df_events = pd.DataFrame()\n",
    "df_events['events'] = pd_events\n",
    "\n",
    "\n",
    "\n",
    "df_events_ord = df_events['events'].apply(lambda x: x.toordinal())\n",
    "(df_events_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, g in groupby(enumerate(df_events_ord), lambda x: x[1]-x[0]):\n",
    "    print(list(map(itemgetter(1), g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def events_split():\n",
    "#     events = (test[test['ID_HDC_G0'] == 6279]['Event_Dates'])\n",
    "#     events_list = events.to_list()\n",
    "#     events_list = [list(i) for i in events_list]\n",
    "#     events_list = events_list[0]\n",
    "#     events_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tid bit to run julian days back\n",
    "\n",
    "def jul_convert(dates):\n",
    "    \"function turn days into julian datetime\"\n",
    "    jul_days = pd.to_datetime(dates).to_julian_date()\n",
    "    \n",
    "    return jul_days\n",
    "\n",
    "def event_split(dates, tmax, intensity, ID_HDC_G0, country):\n",
    "    \"\"\" Searchs a list of dates and isolates sequential dates as a list, then calculates event stats.\n",
    "    See comments in code for more details. \n",
    "    \n",
    "    Args:\n",
    "        ID_HDC_G0: city ID as string\n",
    "        tmax: numpy.ndarray of intensities values of tmax values\n",
    "        days: pandas.core.index as julian dates\n",
    "        intensity: numpy.ndarray of intensities values\n",
    "    \"\"\"\n",
    "    \n",
    "    # city id\n",
    "    city_id = ID_HDC_G0\n",
    "    \n",
    "    # country \n",
    "    country = country\n",
    "    \n",
    "    # lists to fill\n",
    "    event_list = []\n",
    "    dur_list = []\n",
    "    intensity_list = []\n",
    "    city_id_list = []\n",
    "    tmax_list = []\n",
    "    avg_temp_list = []\n",
    "    avg_int_list = []\n",
    "    tot_int_list = []\n",
    "    country_list = []\n",
    "    \n",
    "    # data frame out\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    # turn days into julian days\n",
    "    jul_days = jul_convert(dates)\n",
    "    \n",
    "    # Loop through dur list and isolate seq days, temps, and intensities\n",
    "    for k, g in groupby(enumerate(jul_days.values), lambda x: x[1]-x[0]):\n",
    "        \n",
    "        seq = list(map(itemgetter(1), g)) # isolate seq. days\n",
    "        \n",
    "        dur = len(seq) # event duration\n",
    "        days = dates[0:dur] # get dates for each event #<<<<-------  FIX FIX FIX \n",
    "        #print(len(days[0]))\n",
    "        intense = intensity[0:dur] # intensity of each day during event\n",
    "        temp = tmax[0:dur] # temp of each day during event\n",
    "        temp_avg = mean(temp) # avg. temp during event\n",
    "        avg_int = mean(intense) # avg. intensity during event\n",
    "        tot_int = intense.sum() # total intensity during event\n",
    "        \n",
    "        # turn days back into julian days\n",
    "        # days = pd.to_datetime(days, format = '%Y.%m.%d')\n",
    "        \n",
    "        city_id_list.append(city_id)\n",
    "        event_list.append(days) #<<<<-------  FIX FIX FIX \n",
    "        dur_list.append(dur)\n",
    "        intensity_list.append(intense)\n",
    "        tmax_list.append(temp)\n",
    "        avg_temp_list.append(temp_avg)\n",
    "        avg_int_list.append(avg_int)\n",
    "        tot_int_list.append(tot_int)\n",
    "        country_list.append(country)\n",
    "        \n",
    "    df_out['ID_HDC_G0'] = city_id_list\n",
    "    df_out['CTR_MN_NM'] = country_list\n",
    "    df_out['duration'] = dur_list\n",
    "    df_out['avg_temp'] = avg_temp_list\n",
    "    df_out['avg_intensity'] = avg_int_list\n",
    "    df_out['tot_intensity'] = tot_int_list\n",
    "    df_out['events'] = event_list\n",
    "    df_out['duration'] = dur_list\n",
    "    df_out['intensity'] = intensity_list\n",
    "    df_out['tmax'] = tmax_list\n",
    "    \n",
    "    return df_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
