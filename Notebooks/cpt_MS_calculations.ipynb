{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS Calculations\n",
    "\n",
    "Notebook to crunch numbers for the MS.\n",
    "\n",
    "by Cascade Tuholske 2020.02.23 \n",
    "\n",
    "Updated 2020.08.27 - CPT  \n",
    "Updated July/Aug 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Depdencies \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import matplotlib.dates as mdates\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Regressions, no intercept addition is needed because we're using SK LEARN HERE \n",
    "\n",
    "def lm_func(df, col):\n",
    "    \n",
    "    \"simple linear model of a time series data, returns coef\"\n",
    "    \n",
    "    # Get Data\n",
    "    X_year = np.array(df.groupby('year')['ID_HDC_G0'].mean().index).reshape((-1, 1))\n",
    "    Y_stats = np.array(df.groupby('year')[col].sum()).reshape((-1, 1))\n",
    "\n",
    "    # Add Intercept\n",
    "    X_year_2 = sm.add_constant(X_year)\n",
    "\n",
    "    # Regress\n",
    "    model = sm.OLS(Y_stats, X_year_2).fit() \n",
    "        \n",
    "    coef = int(model.params[1])\n",
    "    #coef = int(coef)\n",
    "            \n",
    "    # R2 and P\n",
    "    r2 = model.rsquared_adj\n",
    "    p = model.pvalues[0]\n",
    "    \n",
    "    return coef, round(r2, 2), round(p, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load Data\n",
    "DATA = 'wbgtmax30' # UPDATE \n",
    "\n",
    "# file paths\n",
    "DATA_IN = \"\"  \n",
    "FIG_OUT = \"\"\n",
    "FN_IN = os.path.join(DATA_IN,DATA+'_EXP.json')\n",
    "HI_STATS = pd.read_json(FN_IN, orient = 'split')\n",
    "\n",
    "# Set scale\n",
    "scale = 10**9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drop cites where 1983 had 1 day and none elsewhere\n",
    "\n",
    "print(len(HI_STATS))\n",
    "only83 = HI_STATS.groupby('ID_HDC_G0')['tot_days'].sum() == 1 # sum up total days and find those with 1 day\n",
    "only83 = list(only83[only83 == True].index) # make a list of IDs\n",
    "sub = HI_STATS[HI_STATS['ID_HDC_G0'].isin(only83)] # subset those IDs\n",
    "bad_ids = sub[(sub['year'] == 1983) & (sub['tot_days'] == 1)] # drop those from 1983 only\n",
    "drop_list = list(bad_ids['ID_HDC_G0']) # make a list\n",
    "HI_STATS= HI_STATS[~HI_STATS['ID_HDC_G0'].isin(drop_list)] # drop those from the list\n",
    "print(len(HI_STATS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Add In Meta Data (e.g. geographic data)\n",
    "meta_fn = os.path.join('','GHS-UCDB-IDS.csv')\n",
    "meta_data = pd.read_csv(meta_fn)\n",
    "\n",
    "#### Merge in meta\n",
    "HI_STATS = HI_STATS.merge(meta_data, on = 'ID_HDC_G0', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " HI_STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Check to num days per year greatest > 32\n",
    "check = HI_STATS.sort_values('tot_days', ascending = False)\n",
    "check[['CTR_MN_NM', 'UC_NM_MN', 'year', 'tot_days']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Total Change in people Days\n",
    "data = HI_STATS.groupby('year')['people_days'].sum()\n",
    "year = str(data.index[33])\n",
    "value = str(data.values[33]/10**9)\n",
    "print('person days in 2016 was '+value+' billion')\n",
    "\n",
    "year = str(data.index[0])\n",
    "value = str(data.values[0]/10**9)\n",
    "print('person days in 1983 was '+value+' billion')\n",
    "\n",
    "#### Pct Change in Poeple Days 1983 - 2016\n",
    "pdays16 = data.iloc[len(data) -1]\n",
    "pdays83 = data.iloc[0]\n",
    "out = (data.iloc[len(data) -1] - data.iloc[0]) / data.iloc[0] * 100\n",
    "print('pct increase in people days 83 - 16 is ', out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Rate of change\n",
    "data = HI_STATS\n",
    "coef, r2, p = lm_func(data, 'people_days')\n",
    "print('annual increase in people days ', 'was', coef/10**9, ' p=', p)\n",
    "coef1, r21, p1 = lm_func(data, 'people_days_heat')\n",
    "print('annual increase in people days heat ', 'was', coef1/10**9, ' p=', p)\n",
    "coef2, r22, p2 = lm_func(data, 'people_days_pop')\n",
    "print('annual increase in people days pop ', 'was', coef2/10**9, ' p=', p)\n",
    "print('attrib heat ', 'was', coef1 / coef *100, ' p=', p, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Pct Pday Annual Increase from Heat\n",
    "coef_pdays, r2_pdays, p_pdays = lm_func(HI_STATS, 'people_days') # regress pdays\n",
    "coef_heat, r2_heat, p_heat = lm_func(HI_STATS, 'people_days_heat') # regreas heat\n",
    "\n",
    "print('warming is what pct of total?', coef_heat/coef_pdays *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Pct heat vs pop\n",
    "coef_pop, r2_pdays, p_pdays = lm_func(HI_STATS, 'people_days_pop') # regress pdays\n",
    "coef_heat, r2_heat, p_heat = lm_func(HI_STATS, 'people_days_heat') # regreas heat\n",
    "\n",
    "print('pct of heat vs pop', coef_heat/coef_pop *100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Largest cities compared to global total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Top cities\n",
    "cities_fn = os.path.join('','wbgtmax30_EXP-TOP50.csv')\n",
    "cities = pd.read_csv(cities_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = cities.sort_values('coef_pdays', ascending = False).head(25) # get the top ten cities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What pct of the global annual increase comes from the top 25 cities?\n",
    "ans = top['coef_pdays'].sum() / coef # coef comes from rate of change cell\n",
    "print('Top 25 cities of total annual increase', ans * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_coefs_fn = os.path.join('','wbgtmax30_TREND_PDAYS05.json')\n",
    "city_coefs = pd.read_json(city_coefs_fn, orient = 'split')\n",
    "GHS = gpd.read_file('','GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_0.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(city_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Number of cities w/ sig increase in exposure?\n",
    "print('The pct of cities w/ increases in exposure: ', len(city_coefs)/len(GHS)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(city_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ans = len(city_coefs[(city_coefs['GCPNT_LAT'] < 23.5) & (city_coefs['GCPNT_LAT'] > -23.5)]) / len(city_coefs)\n",
    "print('what pct of pday cities are low lat?', ans*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('what pct of global pop are cities with sig pdays?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_search(country, data_set):\n",
    "    \"what pct of cities had a p-day increase?\"\n",
    "    print('Num of Cities in '+country+' ', len(data_set[data_set['CTR_MN_NM'] == country]) / len(GHS[GHS['CTR_MN_NM'] == country]) *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = city_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_search('Senegal', data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_search('Nigeria', data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_search('India', data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pct of global population exposured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_coefs_fn = os.path.join('','wbgtmax30_TREND_PDAYS05.json')\n",
    "city_coefs = pd.read_json(city_coefs_fn, orient = 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_fn = os.path.join('','GHS-UCDB-Interp.csv')\n",
    "pop = pd.read_csv(pop_fn)\n",
    "\n",
    "p16 = pop[['ID_HDC_G0', 'P2016']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdays_pop = pd.merge(city_coefs[['ID_HDC_G0']], p16, on = 'ID_HDC_G0', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pdays_pop['P2016'].sum() / p16['P2016'].sum() * 100 \n",
    "print('What is the global urban population in 2016', p16['P2016'].sum() / 10**9)\n",
    "print('How many people live in cities with increasing exp in 2016', pdays_pop['P2016'].sum() / 10**9)\n",
    "print('What pct of total urban pop has sig increase exp in 2015', ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From UN-DESA 2018 estimates for total global pop in 2015\n",
    "ans =  pdays_pop['P2016'].sum() / 7383009000 * 100\n",
    "print('What pct of total world pop has sig increase exp in 2016', ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UN-DESA Urban pop in 2015 was  3 981 498\n",
    "p16['P2016'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Heat Days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_totdays_fn = os.path.join('', 'wbgtmax30_TREND_HEATP05.json')\n",
    "city_totdays = pd.read_json(city_totdays_fn, orient = 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('What pct of all cities had sig increase in days/yr > WBGT 30 C ?')\n",
    "print(len(city_totdays)/len(GHS))\n",
    "print(len(city_totdays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('What pct of all cities >1 day / yr in days/yr > WBGT 30 C ?')\n",
    "print(len(city_totdays[city_totdays['coef_totDays'] >= 1])/len(GHS))\n",
    "print(len(city_totdays[city_totdays['coef_totDays'] >= 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many cities day increase per year ... 1, 3\n",
    "top = len(city_totdays)\n",
    "bottom = len(city_totdays[city_totdays['coef_totDays'] >= 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(top)\n",
    "print(bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What are some big cities (>1m people)?\n",
    "hot1m = city_totdays[(city_totdays['coef_totDays'] >= 1.5) & (city_totdays['P2016'] >= 10**6)][['coef_totDays', 'UC_NM_MN', 'P2016']].sort_values('coef_totDays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hot1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.162029e+07 / 10**6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dehli & Kolkata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delhi 6955 & Kolkata 9691\n",
    "K = city_coefs[city_coefs['ID_HDC_G0']== 9691]\n",
    "D = city_coefs[city_coefs['ID_HDC_G0']== 6955]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Share of heat Kolkata', K.coef_heat / K.coef_pdays * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Share of heat Delhi', D.coef_heat / D.coef_pdays * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Populations of specific cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pd.read_csv(DATA_IN+'interim/GHS-UCDB-Interp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9691, Kolkata 1998\n",
    "# 2046, Paris 2003\n",
    "# 4417, Aleppo 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pop[pop['ID_HDC_G0'] == 9691]['P2015'] / 10**3\n",
    "print('Pop of Kolkata in 1998', ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WBGT 32 vs 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbgt32 = pd.read_json('', '/wbgtmax32_TREND_PDAYS05.json', orient = 'split')\n",
    "wbgt30 = pd.read_json('', '/wbgtmax30_TREND_PDAYS05.json', orient = 'split')\n",
    "wbgt28 = pd.read_json('', '/wbgtmax28_TREND_PDAYS05.json', orient = 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('how many cities wbgt 32?', len(wbgt32))\n",
    "print('how many cities wbgt 32?', len(wbgt30))\n",
    "print('how many cities wbgt 28?', len(wbgt28))\n",
    "print('no trend', len(meta_data) - len(wbgt28))\n",
    "print('dif is', 8510 - 6022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regional Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Annual Rates\n",
    "\n",
    "scale = 10**6\n",
    "geog = 'sub-region'\n",
    "\n",
    "for label in np.unique(HI_STATS[geog]):\n",
    "    label = label\n",
    "    data = HI_STATS[HI_STATS[geog] == label]\n",
    "    \n",
    "    #### Rate of change\n",
    "    coef, r2, p = lm_func(data, 'people_days')\n",
    "    print('annual increase in people days '+label, 'was', coef/scale, ' p=', p)\n",
    "    coef1, r21, p1 = lm_func(data, 'people_days_heat')\n",
    "    print('annual increase in people days heat '+label, 'was', coef1/scale, ' p=', p)\n",
    "    coef2, r22, p2 = lm_func(data, 'people_days_pop')\n",
    "    print('annual increase in people days pop '+label, 'was', coef2/scale, ' p=', p)\n",
    "    print('attrib heat '+label, 'was', coef1 / coef *100, ' p=', p, '\\n')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Trends for Africa, N & SS\n",
    "geog = 'region'\n",
    "location = 'Africa'\n",
    "data = HI_STATS[HI_STATS[geog] == location]\n",
    "print(location)\n",
    "\n",
    "#### Total Change in people Days\n",
    "data = data.groupby('year')['people_days'].sum()\n",
    "year = str(data.index[33])\n",
    "value = str(data.values[33]/10**9)\n",
    "print('person days in 2016 was '+value+' billion')\n",
    "\n",
    "year = str(data.index[0])\n",
    "value = str(data.values[0]/10**9)\n",
    "print('person days in 1983 was '+value+' billion')\n",
    "\n",
    "#### Pct Change in Poeple Days 1983 - 2016\n",
    "pdays16 = data.iloc[len(data) -1]\n",
    "pdays83 = data.iloc[0]\n",
    "out = (data.iloc[len(data) -1] - data.iloc[0]) / data.iloc[0] * 100\n",
    "print('pct increase in people days 83 - 16 is ', out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### S Asia as pct of total  global = 5.245146271 B \n",
    "\n",
    "print('pct of total pdays from S Asia is ', 1899.70765 / 10**3 / 5.245146271 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Median Slope\n",
    "region = 'Europe'\n",
    "col = 'coef_heat'\n",
    "geog = 'region'\n",
    "scale = 10**3\n",
    "result = city_coefs[city_coefs[geog]== region][col].median()\n",
    "print(region, col, 'is ', result/scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat Waves\n",
    "\n",
    "- 9691 Kolkata 1998\n",
    "- 2046 Paris 2003\n",
    "- 4417, Aleppo 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(dir_in, geog, location):\n",
    "    \"\"\"Function makes data to plot daily city-level HI Max and average\n",
    "    Args:\n",
    "        dir_in = directory to get data\n",
    "        geog = column for geography, city-level = 'ID_HDC_G0'\n",
    "        location = usually a city id\n",
    "    \"\"\"\n",
    "    \n",
    "    fn_list = sorted(glob.glob(dir_in+'*.csv')) # get data\n",
    "    df_out = pd.DataFrame() # to write dataframe\n",
    "    \n",
    "     # get leap year cols from 2016\n",
    "    hi16 = pd.read_csv(fn_list[33]) \n",
    "    cols = list(hi16.iloc[:,9:].columns)\n",
    "    cols = [year[5:] for year in cols] # cols for data frame\n",
    "    \n",
    "    temp_list = [] # empty list for temps\n",
    "    \n",
    "    # loop through dir and get data\n",
    "    for i, fn in enumerate(fn_list):\n",
    "        df = pd.read_csv(fn) # open data frame\n",
    "        year_label = [(df.columns[9]).split('.')[0]] # get year\n",
    "        row = df[df[geog] == location]\n",
    "        temp = row.iloc[:,9:] # get only temp columns\n",
    "        \n",
    "        # add in col for leap years\n",
    "        if temp.shape[1] == 365:\n",
    "            temp.insert(loc = 59, column = year_label[0]+'.02.29', value = np.nan, allow_duplicates=False)\n",
    "\n",
    "        # Set Index & Columns\n",
    "        temp.index = year_label\n",
    "        temp.columns = cols # revalue to m.d\n",
    "    \n",
    "        # add to list\n",
    "        temp_list.append(temp)\n",
    "    \n",
    "    df_out = pd.concat(temp_list) # make one big dataframe\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "def plot_data(df, year, start, end):#, start, end):\n",
    "    \"\"\" Make the data for a plot\n",
    "    Args: \n",
    "        df = df w/ daily HI max for a given city\n",
    "        year = year you want to plot against average\n",
    "        start = start of plot in julian days (e.g 1 - 365/366)\n",
    "        end = end of plot in julian days\n",
    "    \"\"\"\n",
    "\n",
    "    # Deal with leap year\n",
    "    if year % 4 !=0:\n",
    "        df.drop(columns ='02.29', inplace = True)\n",
    "    \n",
    "    # Subset data\n",
    "    start = start - 1 # zero indexing \n",
    "    subset = df.iloc[:,start:end]\n",
    "    \n",
    "    # HI Max for year\n",
    "    hi_year = subset.loc[str(year)]\n",
    "    \n",
    "    # make 34-avg daily hi and std\n",
    "    means = subset.mean(axis = 0)\n",
    "    stds = subset.std(axis = 0)\n",
    "    \n",
    "    # make colums to date time\n",
    "    cols = pd.to_datetime([str(year)+'.'+date for date in hi_year.index])\n",
    "    \n",
    "    return hi_year, means, cols, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Heat Wave From All DATA\n",
    "def select_city_year(df, city_id, year):\n",
    "    \"Quick search to find city and years within HI_STATS\"\n",
    "    df_out = df[(df['ID_HDC_G0'] == city_id) & (df['year'] == year)]\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open heat events data\n",
    "events_fn = os.path.join('','himax406_STATS.json')\n",
    "events = pd.read_json(events_fn, orient = 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4417, 'Aleppo', 2010]  [9691, 'Kolkata', 1998] \n",
    "city = select_city_year(events, 4417, 2010)\n",
    "city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select event and compare to long term means\n",
    "# Kolkata 1998 UID-3010504, Aleppo 2010 UID-984387 and UID-984386\n",
    "event_id = 'UID-984386'\n",
    "event = city[city['UID'] == event_id]\n",
    "tmax = list(event['tmax'])[0]\n",
    "dates = list(event['event_dates'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at event tmax \n",
    "max(tmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at event dates \n",
    "len(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Heat Index Data\n",
    "data = 'himax'\n",
    "data_label = '$WBGT_{max}$ '\n",
    "DATA_IN = os.path.join('','GHS-'+data+'/') # output from avg wbgt/hi max\n",
    "FIG_OUT = ''\n",
    "DS = u\"\\N{DEGREE SIGN}\"\n",
    "t = 40.6 # wbgt (30) or hi (46.1) threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get long term means\n",
    "# Args\n",
    "#[4417, 'Aleppo'] 2010 [2046, 'Paris'] 2003 [9691, 'Kolkata'] 1998 ['6955, Dehli']\n",
    "\n",
    "# Args\n",
    "city_list = [4417, 'Aleppo']\n",
    "year = 2010\n",
    "\n",
    "# April 1 to Sep 30 (Use Julian Days), or 1 - 182 lagos\n",
    "start = 91 \n",
    "end = 273\n",
    "\n",
    "# Make Data \n",
    "df = make_data(DATA_IN, 'ID_HDC_G0', city_list[0])\n",
    "years, means, cols, stds = plot_data(df, year, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what is the rank of HImax in the entire record?\n",
    "sorted(df.to_numpy().flatten(), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df\n",
    "df = pd.DataFrame()\n",
    "df['dates'] = cols\n",
    "df['max']  = years.to_list()\n",
    "df['avg'] = means.to_list()\n",
    "df['dif'] = df['max'] - df['avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['dates'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dates[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end of heat wave\n",
    "start = dates[0]\n",
    "end = dates[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = df[(df['dates'] >= start) & (df['dates'] <= end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak heat wave\n",
    "out.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[out['dif'] == out['dif'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dif in df['dif']:\n",
    "    print(dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['dif'] > 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phoenix Caclulations\n",
    "1990-6-26 Phoenix, AZ Weather History. https://www.wunderground.com/history/daily/KPHX/date/1990-6-26. Weather Underground. (June 10, 2021).<br>A. Minkler, At 118 degrees, Thursday heat in Phoenix breaks daily record set in 1934. The Arizona Republic (2020) (June 18, 2021).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def c_to_f(C):\n",
    "    \"Convert HI C to F\"\n",
    "    return 1.8 * C + 32 \n",
    "\n",
    "def hi_to_wbgt(HI):\n",
    "    \"\"\" Convert HI to WBGT using emprical relationship from Bernard and Iheanacho 2015\n",
    "    WBGT [◦C] = −0.0034 HI2 + 0.96 HI−34; for HI [◦F]\n",
    "    \"\"\"\n",
    "    \n",
    "    WBGT = -0.0034*HI**2 + 0.96*HI - 34\n",
    "    \n",
    "    return WBGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmax was 122, rh was 11 using https://www.wpc.ncep.noaa.gov/html/heatindex.shtml\n",
    "# then HI was 49 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check out cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_city_year(df, city_id, year):\n",
    "    \"Quick search to find city and years within HI_STATS\"\n",
    "    df_out = df[(df['ID_HDC_G0'] == city_id) & (df['year'] == year)]\n",
    "    \n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = select_city_year(ALL_DATA, 4417, 2010)\n",
    "city"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
