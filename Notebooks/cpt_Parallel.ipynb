{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program finds the TMax from CHIRTSMax Data and a raster with polygon IDS burned\n",
    "# By Cascade Tuholske 2019-08-20\n",
    "\n",
    "#https://sebastianraschka.com/Articles/2014_multiprocessing.html\n",
    "\n",
    "# Dependencies\n",
    "import rasterio \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from rasterstats import zonal_stats\n",
    "from rasterio import features\n",
    "import os\n",
    "import xarray as xr\n",
    "import fnmatch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL TEST \n",
    "# CHIRT_DIR = '/Users/cascade/Github/UrbanHeat/data/test_in/' # <<--- path to loop through\n",
    "# SHP_DIR = '/Users/cascade/Github/PopRaster/data/raw/JRC/ghs-ucdb/'\n",
    "# POLY_RST_DIR = '/Users/cascade/Github/PopRaster/data/interim/'\n",
    "# DATA_OUT = '/Users/cascade/Github/UrbanHeat/data/test_out/'\n",
    "\n",
    "# TANA\n",
    "# CHIRT_DIR = '/home/cascade/tana-spin-cascade/projects/CHIRTMax_Monthly/' # <<--- path to loop through\n",
    "# SHP_DIR = '/home/cascade/tana-crunch-cascade/projects/UrbanHeat/data/raw/GHS_UCDB/'\n",
    "# POLY_RST_DIR = '/home/cascade/tana-crunch-cascade/projects/UrbanHeat/data/interim/'\n",
    "# DATA_OUT = '/home/cascade/tana-crunch-cascade/projects/UrbanHeat/data/processed'\n",
    "\n",
    "# Tana Test\n",
    "CHIRT_DIR = '/home/cascade/tana-crunch-cascade/projects/UrbanHeat/data/test_in/' # <<--- path to loop through\n",
    "DATA_OUT = '/home/cascade/tana-crunch-cascade/projects/UrbanHeat/data/test_out/' # <<--- path to loop through\n",
    "POLY_RST_DIR = '/home/cascade/tana-crunch-cascade/projects/UrbanHeat/data/interim/'\n",
    "SHP_DIR = '/home/cascade/tana-crunch-cascade/projects/UrbanHeat/data/raw/GHS_UCDB/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open Polygon Raster\n",
    "# polyRst_fn = 'GHS_UCDB_Raster_Raster_touched.tif'\n",
    "# polyRst = rasterio.open(POLY_RST_DIR+polyRst_fn)\n",
    "\n",
    "# # Open the file with GeoPANDAS read_file\n",
    "# shp_fn = 'GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_0.shp'\n",
    "# shps = gpd.read_file(SHP_DIR+shp_fn)\n",
    "\n",
    "# # Set fn out, change as needed \n",
    "# fn_out = 'GHS-CHIRT-MONTHLY'  \n",
    "\n",
    "# # Isloate SHP Poly Col to merge back in later \n",
    "# df_ghs = gpd.GeoDataFrame()\n",
    "\n",
    "# df_ghs['ID_HDC_G0'] = shps.ID_HDC_G0\n",
    "# df_ghs['CTR_MN_NM'] = shps.CTR_MN_NM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn polyRst data as Xarray, \n",
    "# polyRst_da = xr.DataArray(polyRst.read(1), dims = ['y', 'x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through dirs in //\n",
    "\n",
    "def temp_ghs(dir_nm):\n",
    "    \n",
    "    # print current process\n",
    "    print(mp.current_process())\n",
    "    \n",
    "    #     print(type(dir_nm))\n",
    "\n",
    "    # Open files \n",
    "    # Open Polygon Raster\n",
    "    polyRst_fn = 'GHS_UCDB_Raster_Raster_touched.tif'\n",
    "    polyRst = rasterio.open(POLY_RST_DIR+polyRst_fn)\n",
    "\n",
    "    # Open the file with GeoPANDAS read_file\n",
    "    shp_fn = 'GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_0.shp'\n",
    "    shps = gpd.read_file(SHP_DIR+shp_fn)\n",
    "\n",
    "    # Set fn out, change as needed \n",
    "    fn_out = 'GHS-CHIRT-MONTHLY'  \n",
    "\n",
    "    # Isloate SHP Poly Col to merge back in later \n",
    "    df_ghs = gpd.GeoDataFrame()\n",
    "\n",
    "    df_ghs['ID_HDC_G0'] = shps.ID_HDC_G0\n",
    "    df_ghs['CTR_MN_NM'] = shps.CTR_MN_NM\n",
    "    \n",
    "    # Turn polyRst data as Xarray, \n",
    "    polyRst_da = xr.DataArray(polyRst.read(1), dims = ['y', 'x'])\n",
    "\n",
    "    # Start Loop\n",
    "    for fn in os.listdir(dir_nm):\n",
    "        \n",
    "        # Set dir name for writing files\n",
    "        dir_year = dir_nm.split(CHIRT_DIR)[1]\n",
    "        \n",
    "        # find all the tif files\n",
    "        if fn.endswith('.tif'):\n",
    "\n",
    "                # NEED TO BUILD META DATA CHECK INTO ROUTINE and throw an error<<<<---------\n",
    "\n",
    "                # Get the date of each chirt file\n",
    "                date = (fn.split('CHIRTSmax.')[1].split('.tif')[0])\n",
    "                print(dir_year)\n",
    "                print(date)\n",
    "\n",
    "                # Open CHIRT Data and turn data into array\n",
    "                tempRst = rasterio.open(dir_nm+'/'+fn)\n",
    "\n",
    "                # Make arrays into x    array DataArray\n",
    "                tempRst_da = xr.DataArray(tempRst.read(1), dims = ['y', 'x']) # y and x are our 2-d labels\n",
    "\n",
    "                # Make xarray dataset\n",
    "                ds = xr.Dataset(data_vars = \n",
    "                        {'ghs' : (['y', 'x'], polyRst_da),\n",
    "                        'temp' : (['y', 'x'], tempRst_da),})\n",
    "\n",
    "                # UPDATED 2019-08-19 Mask the CHIRTS PIXELS FIRST, THEN GHS\n",
    "                # Mask values from chirt that are ocean in ghs and chirt in our ds \n",
    "                ds_mask = ds.where(ds.temp != -9999, drop = False) #<<<<------ need to double check this\n",
    "\n",
    "                # Mask pixels for both ghs and chirts where ghs cities are not present\n",
    "                ds_mask = ds_mask.where(ds_mask.ghs > 0, drop = False)\n",
    "\n",
    "                # Group poly_IDs find temp\n",
    "                avg = ds_mask.groupby('ghs').mean(xr.ALL_DIMS)\n",
    "\n",
    "                # turn GHS IDS and avg. CHIRTMax values into 1-D numpy arrays of equal length\n",
    "                avg_ID = np.array(avg.ghs)\n",
    "                avg_temp = np.array(avg.temp)\n",
    "\n",
    "                print(len(avg_ID))\n",
    "                print(len(avg_temp))\n",
    "\n",
    "                # turn chirt max and IDS into a DF\n",
    "                df_avg = pd.DataFrame()\n",
    "                df_avg[date] = avg_temp\n",
    "                df_avg['ID_HDC_G0'] = avg_ID\n",
    "\n",
    "                # merge the df\n",
    "                df_merge = df_ghs.merge(df_avg, on='ID_HDC_G0', how = 'outer') #<<<<----- NEED TO FIX THIS\n",
    "\n",
    "\n",
    "    #df_merge.to_csv(DATA_OUT+fn_out+'_'+dir_year+'ParPro.csv') # csv out\n",
    "    print('DONE ! ! !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function \n",
    "def test_mp(dir_nm):\n",
    "    # Start Loop\n",
    "    \n",
    "    # print current process\n",
    "    print(mp.current_process())\n",
    "    \n",
    "    dir_year = dir_nm.split(CHIRT_DIR)[1].split('/')[0]\n",
    "\n",
    "    fn_list = []\n",
    "    for fn in os.listdir(dir_nm):\n",
    "        \n",
    "        # find all the tif files\n",
    "        if fn.endswith('.tif'):\n",
    "\n",
    "                # Get the date of each chirt file\n",
    "                date = (fn.split('CHIRTSmax.')[1].split('.tif')[0])\n",
    "                \n",
    "                print(dir_year)\n",
    "                print(date)\n",
    "                \n",
    "                fn_list.append(date)\n",
    "    print(fn_list)\n",
    "    # Save it out\n",
    "    out_df = pd.DataFrame(fn_list)\n",
    "    out_df.to_csv(DATA_OUT+dir_year+'.csv')\n",
    "\n",
    "    #df_merge.to_csv(DATA_OUT+fn_out+'_'+dir_year+'ParPro.csv') # csv out\n",
    "    print('DONE ! ! !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEED TO FIX MERGE, SHOULD BE // otherwise whoot! Check clock speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/cascade/tana-crunch-cascade/projects/UrbanHeat/data/test_in/Year1/',\n",
       " '/home/cascade/tana-crunch-cascade/projects/UrbanHeat/data/test_in/Year2/',\n",
       " '/home/cascade/tana-crunch-cascade/projects/UrbanHeat/data/test_in/Year3/',\n",
       " '/home/cascade/tana-crunch-cascade/projects/UrbanHeat/data/test_in/Year4/']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get dir list\n",
    "from glob import glob\n",
    "\n",
    "dir_list= glob(CHIRT_DIR+'*/')\n",
    "dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ForkProcess(ForkPoolWorker-77, started daemon)>\n",
      "<ForkProcess(ForkPoolWorker-78, started daemon)>\n",
      "<ForkProcess(ForkPoolWorker-79, started daemon)>\n",
      "<ForkProcess(ForkPoolWorker-80, started daemon)>\n",
      "Year3/\n",
      "1983.06\n",
      "Year1/\n",
      "1983.01\n",
      "Year4/\n",
      "1983.10\n",
      "Year2/\n",
      "1983.08\n",
      "13067\n",
      "13067\n",
      "Year1/\n",
      "1983.02\n",
      "13067\n",
      "13067\n",
      "Year4/\n",
      "1983.11\n",
      "13067\n",
      "13067\n",
      "Year3/\n",
      "1983.04\n",
      "13067\n",
      "13067\n",
      "Year2/\n",
      "1983.07\n",
      "13067\n",
      "13067\n",
      "13067\n",
      "13067\n",
      "Year1/\n",
      "1983.03\n",
      "Year4/\n",
      "1983.12\n",
      "13067\n",
      "13067\n",
      "Year3/\n",
      "1983.05\n",
      "13067\n",
      "13067\n",
      "Year2/\n",
      "1983.09\n",
      "13067\n",
      "13067\n",
      "DONE ! ! !\n",
      "13067\n",
      "13067\n",
      "DONE ! ! !\n",
      "13067\n",
      "13067\n",
      "DONE ! ! !\n",
      "13067\n",
      "13067\n",
      "DONE ! ! !\n",
      "71.409677028656\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, Queue, Process\n",
    "import time \n",
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "start = time.time()\n",
    " \n",
    "pool = Pool(7)\n",
    "pool.map(temp_ghs, dir_list)\n",
    "#pool.map_async(temp_ghs, dir_list)\n",
    "pool.close()\n",
    "\n",
    "# for dir_nm in (dir_list):\n",
    "#     proc = Process(target=temp_ghs, args=(dir_nm,))\n",
    "#     proc.start()\n",
    "#     proc.join()\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
