{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats Parallel\n",
    "\n",
    "Notebook by Cascade Tuholske 2021.02.15 <br>\n",
    "Trying to speed up 4_Event_Stats.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from random import random\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import geopandas as gpd \n",
    "import glob\n",
    "from statistics import mean\n",
    "import julian\n",
    "import time \n",
    "import multiprocessing as mp \n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "path = '/home/cascade/projects/UrbanHeat/data/processed/PNAS-DATA-v2/'\n",
    "fns = glob.glob(path+'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for fn in fns:\n",
    "    dfs.append(pd.read_json(fn, orient = 'split'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs: print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(dfs[0]['duration'], bins = 50);\n",
    "plt.yscale('log')\n",
    "plt.title('Duration all events 40.6 >=1 day')\n",
    "plt.xlabel('days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax = dfs[0]['tmax'].to_list()\n",
    "tmax_lst = [item for sublist in tmax for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tmax_lst, bins = 50);\n",
    "plt.yscale('log')\n",
    "plt.title('HImax >=40.6C, for events >=1 day');\n",
    "plt.xlabel('HI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df406 = dfs[0]\n",
    "df406 = df406[df406['duration'] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df406['duration'], bins = 50, color = 'red');\n",
    "plt.yscale('log')\n",
    "plt.title('Duration all events 40.6 >=2 day')\n",
    "plt.xlabel('days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmax = df406['tmax'].to_list()\n",
    "tmax_lst = [item for sublist in tmax for item in sublist]\n",
    "\n",
    "plt.hist(tmax_lst, bins = 50);\n",
    "plt.yscale('log')\n",
    "plt.title('HImax >=40.6C, for events >=2 day');\n",
    "plt.xlabel('HI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_years(df):\n",
    "    \"\"\" Function adds zero to people days for all missing years for each city \n",
    "    so that regressions aren't screwed up\"\"\"\n",
    "    \n",
    "    years = list(np.unique(df['year'])) # Get list of all years\n",
    "    row_list = []\n",
    "    counter = 0\n",
    "    \n",
    "    for city in list(np.unique(df['ID_HDC_G0'])):\n",
    "        city_id = city # Get city Id \n",
    "        city_df = df.loc[df['ID_HDC_G0'] == city] # find the location\n",
    "        city_years = list(np.unique(city_df['year'])) # figure out the number of years\n",
    "        \n",
    "        years_dif = list(set(years) - set(city_years)) # find the missing years\n",
    "        \n",
    "        #print(len(years_dif))\n",
    "        if len(years_dif) > 0: # add in the missing years\n",
    "            \n",
    "            counter = counter + len(years_dif) # counter\n",
    "            \n",
    "            for year in years_dif: # add rows with dummy data and zeros\n",
    "                row = []\n",
    "                row.append(city) # city id\n",
    "                row.append(year) # missing year\n",
    "                row.append(0) # total days\n",
    "                row.append('np.nan') # pop year\n",
    "                row.append(float(df[(df['ID_HDC_G0'] == city)]['P'+str(1983)])) # pop 83\n",
    "                row.append(float(df[(df['ID_HDC_G0'] == city)]['P'+str(2016)])) # pop 16\n",
    "                row.append(0) # days\n",
    "                row.append(0) # pdays 83\n",
    "                row.append(0) # pdays diff\n",
    "                \n",
    "                row_list.append(row)\n",
    "    \n",
    "    df_new = pd.DataFrame(row_list, columns= df.columns) # merge the new rows into a df\n",
    "    \n",
    "    df_new = df.append(df_new) # add the rows back to the original data frame\n",
    "    \n",
    "    # Updated 2020.09.07 CPT - coef can be made for heat when p in 1983 is zero\n",
    "    df_new = df_new[df_new['P1983'] > 0]\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = pd.read_json(path+'HI461_1D_STATS.json', orient = 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_HDC_G0</th>\n",
       "      <th>year</th>\n",
       "      <th>total_days</th>\n",
       "      <th>duration</th>\n",
       "      <th>avg_temp</th>\n",
       "      <th>avg_intensity</th>\n",
       "      <th>tot_intensity</th>\n",
       "      <th>event_dates</th>\n",
       "      <th>intensity</th>\n",
       "      <th>tmax</th>\n",
       "      <th>UID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11118</td>\n",
       "      <td>1983</td>\n",
       "      <td>4549</td>\n",
       "      <td>1</td>\n",
       "      <td>46.733751</td>\n",
       "      <td>0.633751</td>\n",
       "      <td>0.633751</td>\n",
       "      <td>[1983.02.16]</td>\n",
       "      <td>[0.6337506312]</td>\n",
       "      <td>[46.7337506312]</td>\n",
       "      <td>UID-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11118</td>\n",
       "      <td>1983</td>\n",
       "      <td>4549</td>\n",
       "      <td>1</td>\n",
       "      <td>47.262260</td>\n",
       "      <td>1.162260</td>\n",
       "      <td>1.162260</td>\n",
       "      <td>[1983.02.21]</td>\n",
       "      <td>[1.1622601767]</td>\n",
       "      <td>[47.2622601767]</td>\n",
       "      <td>UID-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11118</td>\n",
       "      <td>1983</td>\n",
       "      <td>4549</td>\n",
       "      <td>3</td>\n",
       "      <td>47.079470</td>\n",
       "      <td>0.979470</td>\n",
       "      <td>2.938410</td>\n",
       "      <td>[1983.03.02, 1983.03.03, 1983.03.04]</td>\n",
       "      <td>[0.358618612, 0.5029550792, 2.0768362979]</td>\n",
       "      <td>[46.458618612, 46.6029550792, 48.1768362979]</td>\n",
       "      <td>UID-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11118</td>\n",
       "      <td>1983</td>\n",
       "      <td>4549</td>\n",
       "      <td>9</td>\n",
       "      <td>48.647779</td>\n",
       "      <td>2.547779</td>\n",
       "      <td>22.930013</td>\n",
       "      <td>[1983.03.09, 1983.03.10, 1983.03.11, 1983.03.1...</td>\n",
       "      <td>[1.3213212676000001, 2.3114130899, 4.636047639...</td>\n",
       "      <td>[47.4213212676, 48.4114130899, 50.736047639, 4...</td>\n",
       "      <td>UID-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11118</td>\n",
       "      <td>1983</td>\n",
       "      <td>4549</td>\n",
       "      <td>38</td>\n",
       "      <td>52.665418</td>\n",
       "      <td>6.565418</td>\n",
       "      <td>249.485868</td>\n",
       "      <td>[1983.03.19, 1983.03.20, 1983.03.21, 1983.03.2...</td>\n",
       "      <td>[1.0477397087, 2.7471206706, 3.9829210885, 4.0...</td>\n",
       "      <td>[47.1477397087, 48.8471206706, 50.0829210885, ...</td>\n",
       "      <td>UID-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_HDC_G0  year  total_days  duration   avg_temp  avg_intensity  \\\n",
       "0      11118  1983        4549         1  46.733751       0.633751   \n",
       "1      11118  1983        4549         1  47.262260       1.162260   \n",
       "2      11118  1983        4549         3  47.079470       0.979470   \n",
       "3      11118  1983        4549         9  48.647779       2.547779   \n",
       "4      11118  1983        4549        38  52.665418       6.565418   \n",
       "\n",
       "   tot_intensity                                        event_dates  \\\n",
       "0       0.633751                                       [1983.02.16]   \n",
       "1       1.162260                                       [1983.02.21]   \n",
       "2       2.938410               [1983.03.02, 1983.03.03, 1983.03.04]   \n",
       "3      22.930013  [1983.03.09, 1983.03.10, 1983.03.11, 1983.03.1...   \n",
       "4     249.485868  [1983.03.19, 1983.03.20, 1983.03.21, 1983.03.2...   \n",
       "\n",
       "                                           intensity  \\\n",
       "0                                     [0.6337506312]   \n",
       "1                                     [1.1622601767]   \n",
       "2          [0.358618612, 0.5029550792, 2.0768362979]   \n",
       "3  [1.3213212676000001, 2.3114130899, 4.636047639...   \n",
       "4  [1.0477397087, 2.7471206706, 3.9829210885, 4.0...   \n",
       "\n",
       "                                                tmax    UID  \n",
       "0                                    [46.7337506312]  UID-0  \n",
       "1                                    [47.2622601767]  UID-1  \n",
       "2       [46.458618612, 46.6029550792, 48.1768362979]  UID-2  \n",
       "3  [47.4213212676, 48.4114130899, 50.736047639, 4...  UID-3  \n",
       "4  [47.1477397087, 48.8471206706, 50.0829210885, ...  UID-4  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = exp[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(np.unique(df['year']))\n",
    "row_list = []\n",
    "\n",
    "for city in list(np.unique(df['ID_HDC_G0'])):\n",
    "    city_id = city # Get city Id \n",
    "    city_df = df.loc[df['ID_HDC_G0'] == city] # find the location\n",
    "    city_years = list(np.unique(city_df['year'])) # figure out the number of years\n",
    "\n",
    "    years_dif = list(set(years) - set(city_years)) # find the missing years\n",
    "    \n",
    "    if len(years_dif) > 0: # add in the missing years\n",
    "        for year in years_dif: # add rows with dummy data and zeros\n",
    "            row = []\n",
    "            row.append(city)\n",
    "            row.append(year)\n",
    "            row.append(0) # duration = 0 days\n",
    "            row.append(np.nan) # population for that year is not needed\n",
    "            row.append(df[(df['ID_HDC_G0'] == city)]['P1983'].values[0])\n",
    "            row.append(df[(df['ID_HDC_G0'] == city)]['P1983'].values[0])\n",
    "            row.append(0) # people_days = 0 days\n",
    "            row.append(0) # people_days_heat = 0 days\n",
    "            row.append(0) # people_days_pop = 0 days\n",
    "            \n",
    "            row_list.append(row) # append row list\n",
    "    \n",
    "df_new = pd.DataFrame(row_list, columns= df.columns) # merge the new rows into a df\n",
    "\n",
    "df_new = df.append(df_new) # add the rows back to the original data frame\n",
    "\n",
    "# Drop any city with zero people in 1983\n",
    "df_new = df_new[df_new['P1983'] > 0]\n",
    "\n",
    "return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_HDC_G0</th>\n",
       "      <th>year</th>\n",
       "      <th>duration</th>\n",
       "      <th>P</th>\n",
       "      <th>P1983</th>\n",
       "      <th>P2016</th>\n",
       "      <th>people_days</th>\n",
       "      <th>people_days_heat</th>\n",
       "      <th>people_days_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8716</td>\n",
       "      <td>1983</td>\n",
       "      <td>2</td>\n",
       "      <td>52960.861003</td>\n",
       "      <td>52960.861003</td>\n",
       "      <td>106160.041504</td>\n",
       "      <td>105921.722005</td>\n",
       "      <td>105921.722005</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8716</td>\n",
       "      <td>1984</td>\n",
       "      <td>2</td>\n",
       "      <td>55257.386597</td>\n",
       "      <td>52960.861003</td>\n",
       "      <td>106160.041504</td>\n",
       "      <td>110514.773193</td>\n",
       "      <td>105921.722005</td>\n",
       "      <td>4593.051188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8716</td>\n",
       "      <td>1985</td>\n",
       "      <td>3</td>\n",
       "      <td>57553.912191</td>\n",
       "      <td>52960.861003</td>\n",
       "      <td>106160.041504</td>\n",
       "      <td>172661.736572</td>\n",
       "      <td>158882.583008</td>\n",
       "      <td>13779.153564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8716</td>\n",
       "      <td>1986</td>\n",
       "      <td>3</td>\n",
       "      <td>59850.437785</td>\n",
       "      <td>52960.861003</td>\n",
       "      <td>106160.041504</td>\n",
       "      <td>179551.313354</td>\n",
       "      <td>158882.583008</td>\n",
       "      <td>20668.730347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8716</td>\n",
       "      <td>1987</td>\n",
       "      <td>7</td>\n",
       "      <td>62146.963379</td>\n",
       "      <td>52960.861003</td>\n",
       "      <td>106160.041504</td>\n",
       "      <td>435028.743652</td>\n",
       "      <td>370726.027018</td>\n",
       "      <td>64302.716634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>11060</td>\n",
       "      <td>1993</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84561.272239</td>\n",
       "      <td>84561.272239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>11060</td>\n",
       "      <td>1997</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84561.272239</td>\n",
       "      <td>84561.272239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>11060</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84561.272239</td>\n",
       "      <td>84561.272239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>11060</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84561.272239</td>\n",
       "      <td>84561.272239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>12481</td>\n",
       "      <td>1999</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70441.391992</td>\n",
       "      <td>70441.391992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9996 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID_HDC_G0  year  duration             P         P1983          P2016  \\\n",
       "0        8716  1983         2  52960.861003  52960.861003  106160.041504   \n",
       "1        8716  1984         2  55257.386597  52960.861003  106160.041504   \n",
       "2        8716  1985         3  57553.912191  52960.861003  106160.041504   \n",
       "3        8716  1986         3  59850.437785  52960.861003  106160.041504   \n",
       "4        8716  1987         7  62146.963379  52960.861003  106160.041504   \n",
       "..        ...   ...       ...           ...           ...            ...   \n",
       "25      11060  1993         0           NaN  84561.272239   84561.272239   \n",
       "26      11060  1997         0           NaN  84561.272239   84561.272239   \n",
       "27      11060  2007         0           NaN  84561.272239   84561.272239   \n",
       "28      11060  2008         0           NaN  84561.272239   84561.272239   \n",
       "29      12481  1999         0           NaN  70441.391992   70441.391992   \n",
       "\n",
       "      people_days  people_days_heat  people_days_pop  \n",
       "0   105921.722005     105921.722005         0.000000  \n",
       "1   110514.773193     105921.722005      4593.051188  \n",
       "2   172661.736572     158882.583008     13779.153564  \n",
       "3   179551.313354     158882.583008     20668.730347  \n",
       "4   435028.743652     370726.027018     64302.716634  \n",
       "..            ...               ...              ...  \n",
       "25       0.000000          0.000000         0.000000  \n",
       "26       0.000000          0.000000         0.000000  \n",
       "27       0.000000          0.000000         0.000000  \n",
       "28       0.000000          0.000000         0.000000  \n",
       "29       0.000000          0.000000         0.000000  \n",
       "\n",
       "[9996 rows x 9 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1999}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(np.unique(df_new[df_new['ID_HDC_G0'] == 12481]['year'])) - set(np.unique(test[test['ID_HDC_G0'] == 12481]['year']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(np.unique(df_new[df_new['ID_HDC_G0'] == 12481]['year']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Function adds zero to people days for all missing years for each city \n",
    "so that regressions aren't screwed up\"\"\"\n",
    "\n",
    "years = list(np.unique(exp['year'])) # Get list of all years\n",
    "row_list = []\n",
    "counter = 0\n",
    "\n",
    "for city in list(np.unique(exp['ID_HDC_G0'])):\n",
    "    city_id = city # Get city Id \n",
    "    city_df = exp.loc[exp['ID_HDC_G0'] == city] # find the location\n",
    "    city_years = list(np.unique(city_df['year'])) # figure out the number of years\n",
    "\n",
    "    years_dif = list(set(years) - set(city_years)) # find the missing years\n",
    "\n",
    "    #print(len(years_dif))\n",
    "    if len(years_dif) > 0: # add in the missing years\n",
    "\n",
    "        counter = counter + len(years_dif) # counter\n",
    "\n",
    "        for year in years_dif: # add rows with dummy data and zeros\n",
    "            row = []\n",
    "            row.append(city) # city id\n",
    "            row.append(year) # missing year\n",
    "            row.append(0) # total days\n",
    "            print(exp[(exp['ID_HDC_G0'] == city)]) # pop year\n",
    "#            row.append(float(exp[(exp['ID_HDC_G0'] == city)]['P'+str(year)])) # pop year\n",
    "#             row.append(float(df[(df['ID_HDC_G0'] == city)]['P'+str(1983)])) # pop 83\n",
    "#             row.append(float(df[(df['ID_HDC_G0'] == city)]['P'+str(2016)])) # pop 16\n",
    "#             row.append(0) # days\n",
    "#             row.append(0) # pdays 83\n",
    "#             row.append(0) # pdays diff\n",
    "\n",
    "#             row_list.append(row)\n",
    "\n",
    "# df_new = pd.DataFrame(row_list, columns= df.columns) # merge the new rows into a df\n",
    "\n",
    "# df_new = df.append(df_new) # add the rows back to the original data frame\n",
    "\n",
    "# # Updated 2020.09.07 CPT - coef can be made for heat when p in 1983 is zero\n",
    "# df_new = df_new[df_new['P1983'] > 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = add_years(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = path+'HI406_2D_STATS.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(df_in, orient = 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_HDC_G0</th>\n",
       "      <th>year</th>\n",
       "      <th>duration</th>\n",
       "      <th>avg_temp</th>\n",
       "      <th>avg_intensity</th>\n",
       "      <th>tot_intensity</th>\n",
       "      <th>event_dates</th>\n",
       "      <th>intensity</th>\n",
       "      <th>tmax</th>\n",
       "      <th>UID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8716</td>\n",
       "      <td>1983</td>\n",
       "      <td>2</td>\n",
       "      <td>41.968549</td>\n",
       "      <td>1.368549</td>\n",
       "      <td>2.737099</td>\n",
       "      <td>[1983.04.23, 1983.04.24]</td>\n",
       "      <td>[2.2036853194, 0.5334132134]</td>\n",
       "      <td>[42.8036853194, 41.1334132134]</td>\n",
       "      <td>UID-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8716</td>\n",
       "      <td>1983</td>\n",
       "      <td>16</td>\n",
       "      <td>47.880512</td>\n",
       "      <td>7.280512</td>\n",
       "      <td>116.488197</td>\n",
       "      <td>[1983.04.26, 1983.04.27, 1983.04.28, 1983.04.2...</td>\n",
       "      <td>[0.7365281444, 4.1637614766, 9.6917897375, 6.4...</td>\n",
       "      <td>[41.3365281444, 44.7637614766, 50.2917897375, ...</td>\n",
       "      <td>UID-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8716</td>\n",
       "      <td>1983</td>\n",
       "      <td>51</td>\n",
       "      <td>51.438063</td>\n",
       "      <td>10.838063</td>\n",
       "      <td>552.741209</td>\n",
       "      <td>[1983.05.13, 1983.05.14, 1983.05.15, 1983.05.1...</td>\n",
       "      <td>[3.4425009533, 7.2378939909, 10.2395632742, 9....</td>\n",
       "      <td>[44.0425009533, 47.8378939909, 50.8395632742, ...</td>\n",
       "      <td>UID-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8716</td>\n",
       "      <td>1983</td>\n",
       "      <td>22</td>\n",
       "      <td>53.897499</td>\n",
       "      <td>13.297499</td>\n",
       "      <td>292.544988</td>\n",
       "      <td>[1983.07.04, 1983.07.05, 1983.07.06, 1983.07.0...</td>\n",
       "      <td>[11.9686002231, 16.3046980407, 14.5550595174, ...</td>\n",
       "      <td>[52.5686002231, 56.9046980407, 55.1550595174, ...</td>\n",
       "      <td>UID-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8716</td>\n",
       "      <td>1983</td>\n",
       "      <td>28</td>\n",
       "      <td>50.167647</td>\n",
       "      <td>9.567647</td>\n",
       "      <td>267.894128</td>\n",
       "      <td>[1983.07.27, 1983.07.28, 1983.07.29, 1983.07.3...</td>\n",
       "      <td>[4.8134424765, 3.4528546549, 9.7354032993, 10....</td>\n",
       "      <td>[45.4134424765, 44.0528546549, 50.3354032993, ...</td>\n",
       "      <td>UID-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_HDC_G0  year  duration   avg_temp  avg_intensity  tot_intensity  \\\n",
       "1       8716  1983         2  41.968549       1.368549       2.737099   \n",
       "2       8716  1983        16  47.880512       7.280512     116.488197   \n",
       "3       8716  1983        51  51.438063      10.838063     552.741209   \n",
       "4       8716  1983        22  53.897499      13.297499     292.544988   \n",
       "5       8716  1983        28  50.167647       9.567647     267.894128   \n",
       "\n",
       "                                         event_dates  \\\n",
       "1                           [1983.04.23, 1983.04.24]   \n",
       "2  [1983.04.26, 1983.04.27, 1983.04.28, 1983.04.2...   \n",
       "3  [1983.05.13, 1983.05.14, 1983.05.15, 1983.05.1...   \n",
       "4  [1983.07.04, 1983.07.05, 1983.07.06, 1983.07.0...   \n",
       "5  [1983.07.27, 1983.07.28, 1983.07.29, 1983.07.3...   \n",
       "\n",
       "                                           intensity  \\\n",
       "1                       [2.2036853194, 0.5334132134]   \n",
       "2  [0.7365281444, 4.1637614766, 9.6917897375, 6.4...   \n",
       "3  [3.4425009533, 7.2378939909, 10.2395632742, 9....   \n",
       "4  [11.9686002231, 16.3046980407, 14.5550595174, ...   \n",
       "5  [4.8134424765, 3.4528546549, 9.7354032993, 10....   \n",
       "\n",
       "                                                tmax    UID  \n",
       "1                     [42.8036853194, 41.1334132134]  UID-1  \n",
       "2  [41.3365281444, 44.7637614766, 50.2917897375, ...  UID-2  \n",
       "3  [44.0425009533, 47.8378939909, 50.8395632742, ...  UID-3  \n",
       "4  [52.5686002231, 56.9046980407, 55.1550595174, ...  UID-4  \n",
       "5  [45.4134424765, 44.0528546549, 50.3354032993, ...  UID-5  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['ID_HDC_G0'] == 8716) & (df['year'] == 2000)]['duration'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 1 - Function Loads all Tmax Data as an X-array\n",
    "def read_data(dir_path, space_dim, time_dim):\n",
    "    \"\"\" Function reads in all Tmax .csv files, joins them by date along the x-axis\n",
    "    and returns the whole record as a x-array data array\n",
    "    \n",
    "    Args:   \n",
    "        dir_path = path to .csv files \n",
    "        time_dim = name for time dim as a str ... use date :-)\n",
    "        space_dim = col name for GHS-UCDB IDs as an str (ID_HDC_G0)\n",
    "    \"\"\"\n",
    "    fn_list = sorted(glob.glob(dir_path+'*.csv'))\n",
    "    df_out = pd.DataFrame()\n",
    "    date_list = []\n",
    "\n",
    "    # Open all Tmax files and concat into a df\n",
    "    for i, fn in enumerate(fn_list):    \n",
    "        # Open the CSV\n",
    "        df = pd.read_csv(fn)\n",
    "\n",
    "        # Get the city ids \n",
    "        if i == 1:\n",
    "            df_id = df[space_dim]\n",
    "\n",
    "        # get only the Tmax columns and concate date list \n",
    "        df_temp = df.iloc[:,3:] # get only temp columns\n",
    "        date_list = date_list+list(df_temp.columns)\n",
    "\n",
    "        # Drop cities w/ no temp record \n",
    "        df_temp_drop = df_temp.dropna()\n",
    "\n",
    "        # Merge\n",
    "        df_out = pd.concat([df_out, df_temp_drop], axis=1)\n",
    "        print(df_out.shape)\n",
    "    \n",
    "    # make date into an array\n",
    "    tmax_arr = df_out.to_numpy()\n",
    "\n",
    "    # Make data into an xr.DataArray\n",
    "    tmax_xr_da = xr.DataArray(tmax_arr, coords=[df_id, date_list], \n",
    "                             dims=[space_dim, time_dim])\n",
    "    return tmax_xr_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 2 Function finds all the Tmax Events and writes it to a dateframe w/ dates for each city\n",
    "def tmax_days(xarray, Tthresh):\n",
    "    \"\"\" Function finds all the tmax days in a year and sums total days per year \n",
    "    greater than a threshold within a year where Tmax > Tthresh for each city. Returns the total number of days,\n",
    "    the dates, the tempatures, and the intensity (daily Tmax - Tthresh)\n",
    "    \n",
    "    Args: \n",
    "        xarray = an xarray object with dims = (space, times)\n",
    "        Tthresh = int of temp threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty lists & df\n",
    "    id_list = []\n",
    "    date_list = []\n",
    "    tmax_list = []\n",
    "    intensity_list = []\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    # subset xarray\n",
    "    out = xarray.where(xarray > Tthresh, drop = True)\n",
    "\n",
    "    # start loop \n",
    "    for index, loc in enumerate(out.ID_HDC_G0):\n",
    "        id_list.append(out.ID_HDC_G0.values[index]) # get IDS\n",
    "        date_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values) # get event dates\n",
    "        \n",
    "        # #CPT 2020.02.23 \n",
    "        # dayTot_list.append(len(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').date.values)) # get event totals\n",
    "        \n",
    "        tmax_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').values) # get temp values\n",
    "        intensity_list.append(out.sel(ID_HDC_G0 = loc).dropna(dim = 'date').values - Tthresh) # get severity\n",
    "\n",
    "    # write to a data frame\n",
    "    df_out['ID_HDC_G0'] = id_list\n",
    "    # df_out['total_days'] = dayTot_list #CPT 2020.02.23\n",
    "    df_out['dates'] = date_list\n",
    "    df_out['tmax'] = tmax_list\n",
    "    df_out['tmax_tntensity'] = intensity_list\n",
    "\n",
    "    # return df_out\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 3 Function splits the dataset into Tmax events (continuous days >Tmax) for each city\n",
    "def jul_convert(dates):\n",
    "    \"Function turn days into julian datetime\"\n",
    "    jul_days = pd.to_datetime(dates).to_julian_date()\n",
    "    \n",
    "    return jul_days\n",
    "\n",
    "def event_split(dates, ID_HDC_G0, intensity, tmax): #, total_days): #CPT 2020.02.23\n",
    "    \n",
    "    \"\"\" Searchs a list of dates and isolates sequential dates as a list, then calculates event stats.\n",
    "    See comments in code for more details. \n",
    "    \n",
    "    Args:\n",
    "        dates: pandas.core.index as julian dates\n",
    "        ID_HDC_G0: city ID as string\n",
    "        intensity: numpy.ndarray of intensities values\n",
    "        tmax: numpy.ndarray of intensities values of tmax values\n",
    "        total_days: total number of tmax days in a year for a given city\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # city id\n",
    "    city_id = ID_HDC_G0\n",
    "    # tot_days = total_days #CPT 2020.02.23\n",
    "    \n",
    "    # lists to fill\n",
    "    city_id_list = []\n",
    "    # tot_days_list = [] #CPT 2020.02.23\n",
    "    event_dates_list = []\n",
    "    dur_list = []\n",
    "    intensity_list = []\n",
    "    tmax_list = []\n",
    "    avg_temp_list = []\n",
    "    avg_int_list = []\n",
    "    tot_int_list = []\n",
    "    year_list = []\n",
    "    \n",
    "    # data frame out\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    # turn days into julian days\n",
    "    jul_days = jul_convert(dates)\n",
    "    \n",
    "    # Counters to make sure we write the correct event dates to a list, don't want julian days in output\n",
    "    counter = 0\n",
    "    start = 0\n",
    "    end = 0\n",
    "    \n",
    "    # Loop through dur list and isolate seq days, temps, and intensities\n",
    "    for k, g in groupby(enumerate(jul_days.values), lambda x: x[1]-x[0]):\n",
    "        \n",
    "        seq = list(map(itemgetter(1), g)) # isolate seq. days\n",
    "        dur = len(seq) # duration of each event\n",
    "        \n",
    "        counter = counter + dur # add duration to counter\n",
    "        end = counter # end of current event\n",
    "        \n",
    "        event_dates = dates[start:end] # dates of tmax days during each event\n",
    "        intense = intensity[start:end] # intensity of each day during event\n",
    "        temp = tmax[start:end] # temp of each day during event\n",
    "        avg_temp = mean(temp) # avg. temp during event\n",
    "        avg_int = mean(intense) # avg. intensity during event\n",
    "        tot_int = np.sum(intense) # total intensity during event \n",
    "        \n",
    "        start = counter # reset start to current end (e.g. counter)\n",
    "        year = event_dates[0].split('.')[0]\n",
    "        \n",
    "        # fill lists\n",
    "        city_id_list.append(city_id)\n",
    "        year_list.append(year)\n",
    "        # tot_days_list.append(tot_days) #CPT 2020.02.23\n",
    "        dur_list.append(dur)\n",
    "        event_dates_list.append(event_dates)\n",
    "        intensity_list.append(intense)\n",
    "        tmax_list.append(temp)\n",
    "        avg_temp_list.append(avg_temp)\n",
    "        avg_int_list.append(avg_int)\n",
    "        tot_int_list.append(tot_int)\n",
    "\n",
    "    # write out as a dateframe\n",
    "    df_out['ID_HDC_G0'] = city_id_list\n",
    "    df_out['year'] = year_list\n",
    "    # df_out['total_days'] = tot_days_list #CPT 2020.02.23\n",
    "    df_out['duration'] = dur_list\n",
    "    df_out['avg_temp'] = avg_temp_list\n",
    "    df_out['avg_intensity'] = avg_int_list\n",
    "    df_out['tot_intensity'] = tot_int_list\n",
    "    df_out['event_dates'] = event_dates_list\n",
    "    df_out['duration'] = dur_list\n",
    "    df_out['intensity'] = intensity_list\n",
    "    df_out['tmax'] = tmax_list\n",
    "\n",
    "    return df_out\n",
    "\n",
    "#### Step 4 function feeds output from function 3 into function 4\n",
    "def tmax_stats(df_in):\n",
    "    \"\"\" runs event_split functionon a dataframe to produce desired tmax stats\n",
    "\n",
    "        NOTE - If you add arguments to event_split to make more states,\n",
    "        be sure to update this function\n",
    "\n",
    "        args:\n",
    "            df: input dataframe\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "\n",
    "    # NOTE - If you add arguments to event_split to make more stats,\n",
    "    # be sure to update this function\n",
    "\n",
    "    for index, row in df_in.iterrows():\n",
    "        dates = row['dates'] # Get event dates\n",
    "        intensity = row['tmax_tntensity'] # Get intensity for each day\n",
    "        tmax = row['tmax'] # Get tmax for each day\n",
    "        ID_HDC_G0 = row['ID_HDC_G0'] # get city id\n",
    "        # total_days = row['total_days'] # get total number of tmax days -- CPT 2020.02.23\n",
    "\n",
    "        df = event_split(dates, ID_HDC_G0, intensity, tmax)# , total_days) #CPT 2020.02.23\n",
    "\n",
    "        df_out = df_out.append(df)\n",
    "\n",
    "    return df_out\n",
    "\n",
    "#### Step 5 function threads it all together\n",
    "def run_stats(dir_path, space_dim, time_dim, Tthresh, fn_out):\n",
    "    \n",
    "    \"\"\" Function ties all the Tmax Stats functions together and writes final stats for each Tmax \n",
    "    event to a .csv file. Returns results as a dataframe if needed\n",
    "    \n",
    "    Args:\n",
    "        dir_path = path to .csv files \n",
    "        time_dim = name for time dim as a str ... use date :-)\n",
    "        space_dim = col name for GHS-UCDB IDs as an str (ID_HDC_G0)\n",
    "        Tthresh = float of temp threshold\n",
    "        fn_out = file and path to write final csv\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # read in data\n",
    "    step1= read_data(dir_path, space_dim = space_dim, time_dim = time_dim)\n",
    "    #step1_sub = step1[:,:10] # subset data for testing\n",
    "    print('Stack x-array made')\n",
    "    \n",
    "    # Mask data based on Tmax threshold ... we're using 40.6C\n",
    "    step2 = tmax_days(step1, Tthresh)\n",
    "    print('Tmax masked')\n",
    "    \n",
    "    \n",
    "    # Calculate stats\n",
    "    step3 = tmax_stats(step2)\n",
    "    print('Stats made')\n",
    "\n",
    "    # Save file out\n",
    "    step3.to_json(fn_out, orient = 'split')\n",
    "    \n",
    "    return step3\n",
    "\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arges Needed \n",
    "DATA_IN = '/home/cascade/projects/UrbanHeat/data/interim/CHIRTS_DAILY/HI/' # output from avg temp\n",
    "DATA_OUT = '/home/cascade/projects/UrbanHeat/data/interim/CHIRTS_DAILY/STATS/'\n",
    "dir_path = DATA_IN \n",
    "space_dim = 'ID_HDC_G0'\n",
    "time_dim = 'date'\n",
    "Tthresh = 40.6\n",
    "fn_out = DATA_OUT+'STATS_1DAY406.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "step1 = read_data(dir_path, space_dim = space_dim, time_dim = time_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask data based on Tmax threshold ... we're using 40.6C\n",
    "step2 = tmax_days(step1, Tthresh)\n",
    "print('Tmax masked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this is how to split the df\n",
    "\n",
    "len(np.array_split(step2, 3)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_list = glob.glob(DATA_OUT+'HI406_tmp/*STAT*')\n",
    "fn_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for fn in fn_list:\n",
    "    df_list.append(pd.read_json(fn, orient = 'split'))\n",
    "\n",
    "df_out = pd.concat(df_list)\n",
    "df_out.to_json(fn_out, orient = 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = pd.read_json(DATA_OUT+'HI406_STATS.json', orient = 'split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is the data the same?\n",
    "cpt feb 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.read_json('/home/cascade/projects/UrbanHeat/data/processed/PNAS-DATA-v2/HI406_STATS.json', orient = 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old = pd.read_csv('/home/cascade/projects/UrbanHeat/data/processed/PNAS-DATA-v1/AllDATA-GHS-ERA5-HI406.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old[(old['ID_HDC_G0'] == 3091) & (old['year'] == 1983)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new[(new['ID_HDC_G0'] == 3091) & (new['year'] == 1983)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = 20+2 # - 2\n",
    "n = int(len(step2)/ cpu)  #chunk row size\n",
    "list_df = [step2 [i:i+n] for i in range(0,step2.shape[0],n)]\n",
    "print(len(list_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(step2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write them out\n",
    "for i, df in enumerate(list_df):\n",
    "    df.to_json(DATA_OUT+'tmp/tmp_'+str(i)+'.json', orient = 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir(DATA_OUT+'temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns_list = glob.glob(DATA_OUT+'tmp/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = fns_list[0]\n",
    "df = pd.read_json(fn, orient = 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step3 = tmax_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_stats_run(fn):\n",
    "    \n",
    "    \"\"\" runs max_stats on a fn (.json) and writes on .json)\n",
    "    Args:\n",
    "        fn = file name\n",
    "    \"\"\"\n",
    "    \n",
    "    # open df\n",
    "    df = pd.read_json(fn, orient = 'split')\n",
    "    i = fn.split('temp_')[1].split('.json')[0]\n",
    "    \n",
    "    # make small for testing \n",
    "    df = df.iloc[0:4,:]\n",
    "    \n",
    "    # Calculate stats\n",
    "    step3 = max_stats(df)\n",
    "\n",
    "    # write file\n",
    "    fn_out = DATA_OUT+'temp/'+'STAT_'+str(i)+'.json'\n",
    "    step3.to_json(fn_out, orient = 'split')\n",
    "    print('done', i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in fns_list:\n",
    "    tbd(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = glob.glob(DATA_OUT+'temp/STAT*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Event_Stats_Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '/home/cascade/projects/UrbanHeat/data/interim/CHIRTS_DAILY/STATS/HI406_temp/HI406_2.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test.split('_temp/')[1].split('_')[0]\n",
    "i = test.split(data+'_temp/')[1].split(data+'_')[1]\n",
    "DATA_OUT = test.split(data+'_temp')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob('/home/cascade/projects/UrbanHeat/data/interim/CHIRTS_DAILY/STATS/HI406_temp/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '/home/cascade/projects/UrbanHeat/data/interim/CHIRTS_DAILY/STATS/HI406_temp/HI406_STAT_6.json'\n",
    "test = pd.read_json(fn, orient = 'split')\n",
    "test.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
